{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# breast cancer prediction \n",
    "based on digitized imaged of breast tissue\n",
    "\n",
    "We are Yuval (206587982) and Gabriella (315478404), young computer science students who are passionate about data science and are very willing to learn new things. When we wondered which project we would like to do this semester, we  browsed Kaggle for an open competition. As  both of us never had a project  on Data science and Machine learning and  we are passionate about the medicine world, a competition related to diagnosing cancer was very appealing. The competition's purpose is to predict wether the cancer diagnosis is benign or malignant based on several observations/features.  Those features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass and  they describe characteristics of the cell nuclei present in the image \n",
    "(data from Wisconsin University Data Set).\n",
    "# about breast cancer:\n",
    "Breast cancer is cancer that forms in the cells of the breast. It has been identified as the second largest cause of cancer deaths among women of age 40 and 55. The number of breast cancer diagnosis among women is estimated to be 1.2 million  every year according to studies of the World Health Organization. According to the American cancer society,  in 2001, about 40,200 deaths were caused by breast cancers and there were 192,000 cases of newly diagnosed women. Additional statistics as of 2006 estimated 214,460 new cancer diagnosis and total death at least 41,000 within the US. Early detection and accurate diagnosis have been proved to be crucial to reduce the number of deaths and  increase the survival rate of those diagnosed with breast cancer.\n",
    "\n",
    "# our problem:\n",
    "The machine learning methodology has long been used in medical diagnosis. The Wisconsin Breast Cancer Database (WBCD) dataset released in the early 90's gained a recognition as one of the first milestones of AI and has been widely used in research experiments. The problem is how to diagnose whether or not the patient has breast cancer.\n",
    "This problem is treated as a 2- class (B-benign, M-malignant) classification problem.\n",
    "\n",
    "\n",
    "\n",
    "In the data, there is a list of 10 differents features:\n",
    "-  Radius - mean of distances from center to points on the perimeter.\n",
    "-  Texture - standard deviation of gray-scale value.\n",
    "-  Perimeter \n",
    "-  Area\n",
    "-  Smoothness - Local variation in radius lengths.\n",
    "-  Compactness - (perimeter^2 / area - 1.0) \n",
    "-  Concavity - Severity of concave portions of the contour.\n",
    "-  Concave points - Number of concave portions of the contour.\n",
    "-  Symmetry - Relative difference of two half-planes.\n",
    "-  Fractal dimension - (\"coastline approximation\" - 1)\n",
    "\n",
    "For each feature the dataset include the mean value, standard error and worst value(mean of 3 max values). \n",
    "\n",
    "Class distribution: 357 benign, 212 malignant\n",
    "\n",
    "\n",
    "         \n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQjuiDaPbVi3jtsh-uepYaqpN-iX3SuDAmiIg&usqp=CAU=\" width=900 height=900 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORING THE DATA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "Before well start our journey and analyse the DB first we need to import our ML libraries\n",
    "Sklearn is a open source library for ML and Data science algorithms . It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "Sklearn is simple and efficient tools for predictive data analysis and built on NumPy, SciPy, and matplotlib.\n",
    "\n",
    "Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Seaborn is a library for making statistical graphics . It builds on top of matplotlib and integrates closely with pandas data structures.\n",
    "\n",
    "Pandas and Numpy are a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "built on top of the Python programming language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd  # Import Pandas for data manipulation using dataframes\n",
    "import numpy as np  # Import Numpy for data statistical analysis\n",
    "import matplotlib.pyplot as plt  # Import matplotlib for data visualisation\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "# %matplotlib inline\n",
    "\n",
    "# Import Cancer data drom the Sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Data saves under the file \"BreastCancerDetection.csv\" in this direcory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BreastCancerDetection.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Present the data and visulize it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info method print a concise summary of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see we have 569 lines of data.\n",
    "no missing data.\n",
    "\n",
    "let's look at the dispartion of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_D = len(data['diagnosis'])\n",
    "sum_M = (data['diagnosis'].values=='M').sum()\n",
    "sum_B = sum_D - sum_M\n",
    "frac_B = sum_B/sum_D\n",
    "frac_M = 1 - frac_B\n",
    "labels = 'Benign ('+str(sum_B)+')', 'Malignant ('+str(sum_M)+')'\n",
    "sizes = [frac_B, frac_M]\n",
    "explode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(4, 4))\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.2f%%', shadow=True, startangle=90, textprops={'fontsize': 12})\n",
    "\n",
    "ax1.set_title('Prevalence of diagnosis ('+str(sum_D)+')', fontsize=14)\n",
    "ax1.axis('equal')\n",
    "ax1.legend(fontsize=12, loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 357 benign cases and 212 malignant cases, that mean we have example of both clastifications, we dont have a 50-50 datatset but it will be good enough.\n",
    "\n",
    "the indexes are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the first column is \"ID\" and the last one is \"Unnamed: 32\"\n",
    "\n",
    "Clearly both of them aren't Relevant to analyse the data\n",
    "\n",
    "So we remove them.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove id and Unnamed: 32\n",
    "data=data.iloc[:,1:-1]\n",
    "# Let's print again the remains keys/columns \n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word “normalization” is used informally in statistics, when you normalize data you eliminate the units of measurement for data, enabling you to more easily compare data from different places.\n",
    "let's see if we need to normalize the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.boxplot(data=data.iloc[:,1:11])\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the data is very diverse so we will need to normalized the data before we start working on it\n",
    "\n",
    "let's look a little bit about connections between the diffrent features, we will look on the means values in order to keep a small amound of graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue = 'diagnosis', vars = ['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at first look we can see the data is diffrent between the benign and malignant cases in some of the parameters- look like someting possible\n",
    "\n",
    "let's try to visualize diffrences between benign and malignant in another way- looking at only one feature at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mean= list(data.columns[1:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Those are practicly our best parameters to extricate the rules we want\n",
    "\n",
    "features_mean= ['radius_mean',\n",
    " 'texture_mean',\n",
    " 'perimeter_mean',\n",
    " 'area_mean',\n",
    " 'smoothness_mean',\n",
    " 'compactness_mean',\n",
    " 'concavity_mean',\n",
    " 'concave points_mean',\n",
    " 'symmetry_mean',\n",
    " 'fractal_dimension_mean']\n",
    " \n",
    " \n",
    "matplotlib.pyplot is a state-based interface to matplotlib. It provides a MATLAB-like way of plotting.\n",
    "pyplot is mainly intended for interactive plots and simple cases of programmatic plot generation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i, feature in enumerate(features_mean):\n",
    "    rows = int(len(features_mean)/2)\n",
    "    \n",
    "    plt.subplot(rows, 2, i+1)\n",
    "    \n",
    "    sns.boxplot(x='diagnosis', y=feature, data=data, palette=\"Set1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another  way to look at connections between features is correlation matrix:\n",
    "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.\n",
    "Most correlation matrixes use Pearson’s Product-Moment Correlation (r). It is also common to use Spearman’s Correlation and Kendall’s Tau-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lighter colors mean stronger correlation\n",
    "\n",
    "not suprising to see strong correlation between perimeter and area\n",
    "\n",
    "Lets us explain more :\n",
    "\n",
    "Value of 1 represents perfect correlation between 2 variables. If the first one get value X the second gets the same value . They are perfectly match for one another and it make sense that every variable gets correlation of -1 with himself.\n",
    "\n",
    "Value of -1 represents perfect Negative correlation between 2 variables. If the first one get value X the second gets the opposite . They are perfectly opposites match for one another.\n",
    "\n",
    "Value of 0 represents No  correlation between 2 variables. They are not related at all and there is not connection between them .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check the correlation between the variables \n",
    "plt.figure(figsize=(20,10)) \n",
    "sns.heatmap((data.iloc[:,1:11]).corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.distplot is a Flexibly plot a univariate distribution of observations.\n",
    "This function combines the matplotlib hist function (with automatic calculation of a good default bin size) with the seaborn kdeplot() and rugplot() functions. It can also fit  distributions - which it was we use here\n",
    "\n",
    "Note : \n",
    "\n",
    "kdeplot\n",
    "Show a univariate or bivariate distribution with a kernel density estimate.\n",
    "\n",
    "rugplot\n",
    "Draw small vertical lines to show each observation in a distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 12\n",
    "features_selection = ['radius_mean', 'perimeter_mean', 'area_mean', 'concavity_mean', 'concave points_mean']\n",
    "features_mean= list(data.columns[1:11])\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for i, feature in enumerate(features_mean):\n",
    "    rows = int(len(features_mean)/2)\n",
    "    \n",
    "    plt.subplot(rows, 2, i+1)\n",
    "    \n",
    "    sns.distplot(data[data['diagnosis']=='M'][feature], bins=bins, color='red', label='M');\n",
    "    sns.distplot(data[data['diagnosis']=='B'][feature], bins=bins, color='blue', label='B');\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choosing features\n",
    "after looking and learning about our data and before we get deep onto the models we want to choose the best feature. in order to avoid inrelevants features that may give us mistakes.\n",
    "for that we looked at the correlation matrix and found features that are all correlated, from each group we will chose only 1 feature.\n",
    "To check if removing thos feature give us best score we will use a simple model- random forest classification.\n",
    "let's see it's result before removing anything:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we saw above we need to normalize the data before we can start working with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for exceptions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "data_standard=data\n",
    "\n",
    "#normalize the data\n",
    "#data_standard.iloc[:,1:] = data.iloc[:,1:].apply(zscore)\n",
    "data_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data_standard.drop(['diagnosis'],axis=1) \n",
    "y = data_standard['diagnosis']\n",
    "\n",
    "# split data train 70 % and test 30 %\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#random forest classifier with n_estimators=10 (default)\n",
    "clf_rf = RandomForestClassifier(random_state=43)      \n",
    "clr_rf = clf_rf.fit(x_train,y_train)\n",
    "\n",
    "ac = accuracy_score(y_test,clf_rf.predict(x_test))\n",
    "print('Accuracy is: ',ac)\n",
    "cm = confusion_matrix(y_test,clf_rf.predict(x_test))\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acuracy of this model is 96%\n",
    "let's make it better...\n",
    "\n",
    "There are feature we can guess in advance that will be correlated for example area ,radius and perimeter so lets start with that.\n",
    "radius_mean, perimeter_mean and area_mean are all correlated (0.99 in the matrix) so we will use only area_mean.\n",
    "other features with correlation are Compactness_mean, concavity_mean and concave points_mean so we will chose concavity_mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "till now we look only in the means values so let's check correlation with other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation map\n",
    "f,ax = plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(data_standard.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this matrix we can see correlation between:\n",
    "radius_se, perimeter_se and area_se are correlated so we chose area_se. \n",
    "radius_worst, perimeter_worst and area_worst so we chose area_worst. \n",
    "Compactness_worst, concavity_worst and concave points_worst so we chose concavity_worst.\n",
    "Compactness_se, concavity_se and concave points_se so we chose concavity_se.\n",
    "texture_mean and texture_worst so we chose texture_mean.\n",
    "area_worst and area_mean so we will use area_mean.\n",
    "\n",
    "After chosing this parameters we drop  them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list1 = ['diagnosis','perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\n",
    "x_1 = data_standard.drop(drop_list1,axis = 1 )        \n",
    "x_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = data_standard['diagnosis']\n",
    "\n",
    "# split data train 70 % and test 30 %\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#random forest classifier with n_estimators=10 (default)\n",
    "clf_rf = RandomForestClassifier(random_state=43)      \n",
    "clr_rf = clf_rf.fit(x_train,y_train)\n",
    "\n",
    "ac = accuracy_score(y_test,clf_rf.predict(x_test))\n",
    "print('Accuracy is: ',ac)\n",
    "cm = confusion_matrix(y_test,clf_rf.predict(x_test))\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate feature selection\n",
    "we will use SelectKBest that removes all but the k highest scoring features.\n",
    "we will chose the 5 highest features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "# find best scored 5 features\n",
    "select_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\n",
    "print('Score list:', select_feature.scores_)\n",
    "print('Feature list:', x_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best 5 feature to classify is that area_mean, area_se, texture_mean, concavity_worst and concavity_mean. So lets see what happens if we use only these best scored 5 feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = select_feature.transform(x_train)\n",
    "x_test_2 = select_feature.transform(x_test)\n",
    "#random forest classifier with n_estimators=10 (default)\n",
    "clf_rf_2 = RandomForestClassifier()      \n",
    "clr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\n",
    "ac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\n",
    "print('Accuracy is: ',ac_2)\n",
    "cm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\n",
    "sns.heatmap(cm_2,annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFECV\n",
    "\n",
    "In our last try we guess a number-5 and then chose the best 5 feature/\n",
    "now we will try to use a function that give us also the number of feature that give us best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "clf_rf_3 = RandomForestClassifier() \n",
    "rfecv = RFECV(estimator=clf_rf_3, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\n",
    "rfecv = rfecv.fit(x_train, y_train)\n",
    "\n",
    "print('Optimal number of features :', rfecv.n_features_)\n",
    "print('Best features :', x_train.columns[rfecv.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we have the best feature to chose.\n",
    " Lets look at best accuracy with plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of features VS. cross-validation scores\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score of number of selected features\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_3 = rfecv.transform(x_train)\n",
    "x_test_3 = rfecv.transform(x_test)\n",
    "#random forest classifier with n_estimators=10 (default)\n",
    "clf_rf_3 = RandomForestClassifier()      \n",
    "clr_rf_3 = clf_rf_3.fit(x_train_3,y_train)\n",
    "ac_3 = accuracy_score(y_test,clf_rf_3.predict(x_test_3))\n",
    "print('Accuracy is: ',ac_3)\n",
    "cm_3 = confusion_matrix(y_test,clf_rf_3.predict(x_test_3))\n",
    "sns.heatmap(cm_3,annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to conclude we found out that the best number of features is 14 and using RFECV we found easyily this feature and improve our accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models \n",
    "Since it's our first project ion data science we start exploring diffrents models in order to solve the problem. First we start with few models of sklearn and then after we felt more comfortable we continue to tensorflow and pytorch models.\n",
    "first we start with spliting the data in order to train the model on a training set and then evalueting it with the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize and split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply put, a z-score (also called a standard score) gives you an idea of how far from the meana data point is.\n",
    "But more technically it’s a measure of how many standard deviations below or above the population mean\n",
    "a raw score is.\n",
    "\n",
    "A z-score can be placed on a normal distribution curve.\n",
    "Z-scores range from -3 standard deviations (which would fall to the far left of the normal distribution curve) \n",
    "up to +3 standard deviations (which would fall to the far right of the normal distribution curve).\n",
    "\n",
    "And represents by the formula :\n",
    "z = (x – μ) / σ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a technique for validating the model efficiency by training it on the subset of input data and testing on previously unseen subset of the input data. We can also say that it is a technique to check how a statistical model generalizes to an independent dataset.\n",
    "\n",
    "In machine learning, there is always the need to test the stability of the model. It means based only on the training dataset; we can't fit our model on the training dataset. For this purpose, we reserve a particular sample of the dataset, which was not part of the training dataset. After that, we test our model on that sample before deployment, and this complete process comes under cross-validation. \n",
    "\n",
    "There are some common methods that are used for cross-validation. These methods are given below:\n",
    "\n",
    "Validation Set Approach\n",
    "Leave-P-out cross-validation\n",
    "Leave one out cross-validation\n",
    "K-fold cross-validation\n",
    "Stratified k-fold cross-validation\n",
    "\n",
    "\n",
    "train_test_split: Split arrays or matrices into random train and test subsets\n",
    "We devide our data into 2 groups , Usually the train set is bigger then the test set (75%, 25%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for exceptions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "z = np.abs(zscore(data.iloc[:,1:]))\n",
    "\n",
    "X = data.drop(['diagnosis'],axis=1) # All the data with out the diagnosis col - meaning the data we build the model with\n",
    "y = data['diagnosis'] # The diagnosis col , using that we can avalute our Accuracy percentages\n",
    "\n",
    "#normalize the data\n",
    "X_standard = X.apply(zscore)\n",
    "\n",
    "#split into test and train group\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.35, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we get after spliting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 2 group both with 30 features, the training setis with 426 lines and the test set with 143 examples. this size is define in test_size parameter where we chose 25% of the data for the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we try some models from sklearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning.\n",
    "\n",
    "The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.\n",
    "\n",
    "SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine.\n",
    "\n",
    "Hyperplane: There can be multiple lines/decision boundaries to segregate the classes in n-dimensional space, but we need to find out the best decision boundary that helps to classify the data points. This best boundary is known as the hyperplane of SVM.\n",
    "\n",
    "The dimensions of the hyperplane depend on the features present in the dataset, which means if there are 2 features, then hyperplane will be a straight line. And if there are 3 features, then hyperplane will be a 2-dimension plane.\n",
    "\n",
    "We always create a hyperplane that has a maximum margin, which means the maximum distance between the data points.\n",
    "\n",
    "SVM can be of two types:\n",
    "\n",
    "Linear SVM: Linear SVM is used for linearly separable data, which means if a dataset can be classified into two classes by using a single straight line, then such data is termed as linearly separable data, and classifier is used called as Linear SVM classifier.\n",
    "\n",
    "Non-linear SVM: Non-Linear SVM is used for non-linearly separated data, which means if a dataset cannot be classified by using a straight line, then such data is termed as non-linear data and classifier used is called as Non-linear SVM classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Compute a matrix to evaluate the accuracy of a classification. \n",
    "This is a specific table layout that allows visualization of the performance of an algorithm\n",
    "\n",
    "By definition a confusion matrix C is such that Cij is equal to the number of\n",
    "observations known to be in group i and predicted to be in group j.\n",
    "\n",
    "condition positive (P):\n",
    "the number of real positive cases in the data\n",
    "condition negative (N):\n",
    "the number of real negative cases in the data\n",
    "\n",
    "We have 4 types here :\n",
    "    \n",
    "    true positive (TP):\n",
    "        eqv. with hit\n",
    "    true negative (TN):\n",
    "        eqv. with correct rejection\n",
    "    false positive (FP):\n",
    "        eqv. with false alarm, Type I error\n",
    "    false negative (FN):\n",
    "        eqv. with miss, Type II error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#define model\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "#evaluate model\n",
    "y_predict = svc_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a text report showing the main classification metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pattern recognition, information retrieval and machine learning, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were actually retrieved. Both precision and recall are therefore based on an understanding and measure of relevance.\n",
    "\n",
    "Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and 10 cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12. When a search engine returns 30 pages, only 20 of which were relevant, while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3. So, in this case, precision is \"how valid the search results are\", and recall is \"how complete the results are\".\n",
    "\n",
    "Precision = TP / (TP+FP)\n",
    "\n",
    "Recall = TP / (TP+FN)\n",
    "\n",
    "Accuracy = TP+ TN / (TP + TN + FP+ FN)\n",
    "\n",
    "\n",
    "If the result of the test corresponds with reality, then a correct decision has been made. However, if the result of the test does not correspond with reality, then an error has occurred. There are two situations in which the decision is wrong. The null hypothesis may be true. On the other hand, the alternative hypothesis may be true, whereas we do not reject . Two types of error are distinguished: Type I error and type II error.\n",
    "\n",
    "Type I error\n",
    "\n",
    "The first kind of error is the rejection of a true null hypothesis as the result of a test procedure. This kind of error is called a type I error and is sometimes called an error of the first kind.\n",
    "\n",
    "In terms of the courtroom example, a type I error corresponds to convicting an innocent defendant.\n",
    "\n",
    "Type II error\n",
    "\n",
    "The second kind of error is the failure to reject a false null hypothesis as the result of a test procedure. This sort of error is called a type II error and is also referred to as an error of the second kind.\n",
    "\n",
    "In terms of the courtroom example, a type II error corresponds to acquitting a criminal . This error is the dangerous one , our data is distinguishing between benign and malignant cancer diagnosis, FN meaning our condition was positive (TRUE) while Predicted condition was negative(False ) , that means we said to a sick men that he is healthy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we scored accuracy of 65% . We can do better, lets continue to the next algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV: Exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "param_grid : dict or list of dictionaries\n",
    "Dictionary with parameters names (string) as keys and lists of parameter settings to try as values,\n",
    "or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored.\n",
    "This enables searching over any sequence of parameter settings.\n",
    "\n",
    "Functions :\n",
    "\n",
    "best_estimator_ : Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False.\n",
    "\n",
    "best_score_ :  Score of best_estimator on the left out data.\n",
    "\n",
    "best_params_ : Parameter setting that gave the best results on the hold out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=4)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "cm = confusion_matrix(y_test, grid_predictions)\n",
    "sns.heatmap(cm, annot=True)\n",
    "print(classification_report(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we scored accuracy of 95% . Thats pretty good ,lets continue to the next algorithm \n",
    " \n",
    " We can also see the percision is 94% and recall  95% . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-nearest neighbors algorithm (k-NN)\n",
    "In k-NN classification, the output is a class membership. \n",
    "An object is classified by a plurality vote of its neighbors, \n",
    "with the object being assigned to the class most common among its k nearest neighbors \n",
    "(k is a positive integer, typically small).\n",
    "If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "In k-NN regression, the output is the property value for the object.\n",
    "This value is the average of the values of k nearest neighbors.\n",
    "\n",
    "The function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, normalizing the training data can improve its accuracy dramatically.\n",
    "\n",
    "\n",
    "The best choice of k depends upon the data; generally, larger values of k reduces effect of the noise on the classification, but make boundaries between classes less distinct. A good k can be selected by various heuristic techniques. The special case where the class is predicted to be the class of the closest training sample (i.e. when k = 1) is called the nearest neighbor algorithm.\n",
    "\n",
    "The accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p: type - int, default=2\n",
    "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "\n",
    "n_neighbors: type - int, default=5\n",
    "Number of neighbors to use by default for kneighbors queries.\n",
    "\n",
    "metric:  type - str , default=’minkowski’\n",
    "the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#define model\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_predict = classifier.predict(X_test)\n",
    "\n",
    "#evaluate model\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we scored accuracy of 95% . Thats pretty good ,lets continue to the next algorithm\n",
    "\n",
    "We can also see the percision is 95% and recall 94% ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. \n",
    "k-means clustering minimizes within-cluster variances (squared Euclidean distances)\n",
    "The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. \n",
    "\n",
    "The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing cluster\n",
    "\n",
    "Given a set of observations (x1, x2, ..., xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, ..., Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). \n",
    "\n",
    "Assignment step: Assign each observation to the cluster with the nearest mean: that with the least squared Euclidean distance. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means.)\n",
    "\n",
    "The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may stop the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures.\n",
    "\n",
    "\n",
    "The algorithm has converged when the assignments no longer change. The algorithm does not guarantee to find the optimum.\n",
    "\n",
    "Complexity:\n",
    "\n",
    "    NP-hard in general Euclidean space (of d dimensions) even for two clusters\n",
    "\n",
    "    NP-hard for a general number of clusters k even in the plane\n",
    "\n",
    "    if k and d (the dimension) are fixed, the problem can be exactly solved in time O(n^(kd+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier is a function for classifier implementing the k-nearest neighbors vote.\n",
    "\n",
    "First we get normalize the data again and split it to 2 groups - TRAIN and TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "import numpy as numpyInstance\n",
    "import pandas as pandasInstance\n",
    "import matplotlib.pyplot as matplotlibInstance\n",
    "import seaborn as seabornInstance\n",
    "\n",
    "#looking for exceptions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "z = np.abs(zscore(data.iloc[:,1:]))\n",
    "X_standard = X.apply(zscore)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size = 0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_area_mean = data['area_mean']\n",
    "X_smoothness_mean=data[\"smoothness_mean\"]\n",
    "X_radius_mean= data[\"radius_mean\"]\n",
    "y = data['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pairwise_distances_argmin: \n",
    "Compute minimum distances between one point and a set of points.\n",
    "This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance). The minimal distances are also returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the data is distributed accross the cluster in terms of B-benign, M-malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "plt.scatter(X_area_mean, y, c='gray', s=50 * 2, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_smoothness_mean, y, c='gray', s=50 * 2, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, y, c='gray', s=50 * 2, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_radius_mean, X_area_mean, c='gray', s=50 * 2, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.c_[X_area_mean, y]\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size = 0.25, random_state=5)\n",
    "plt.scatter(X[\"radius_mean\"], X['area_mean'], c=y, cmap='gist_rainbow')\n",
    "plt.xlabel('radius_mean', fontsize=18)\n",
    "plt.ylabel('area_mean', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size = 0.25, random_state=5)\n",
    "plt.scatter(X[\"area_mean\"], X['smoothness_mean'], c=y, cmap='gist_rainbow')\n",
    "plt.xlabel('area_mean', fontsize=18)\n",
    "plt.ylabel('smoothness_mean', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size = 0.25, random_state=5)\n",
    "plt.scatter(X[\"radius_mean\"], X['smoothness_mean'], c=y, cmap='gist_rainbow')\n",
    "plt.xlabel('radius_mean', fontsize=18)\n",
    "plt.ylabel('smoothness_mean', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model classifier and fit it to the data - meaning train it on the model TRAIN group \n",
    "\n",
    "After finishing the fitting stages we predict our data on the TEST group . and avalute it compare to the y_test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knModel = KNeighborsClassifier(n_neighbors=1)\n",
    "knModel.fit(X_train,y_train)\n",
    "predictions = knModel.predict(X_test)\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing 143 Observations We scored:\n",
    "    95% accuracy & precision & recall\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are Calculating predictions for each Value of K from 1 to 50 and Calculating the Average Error Value and Storing it in ERROR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpyInstance.mean : \n",
    "\n",
    "Returns the average of the array elements.\n",
    "Masked entries are ignored. The average is taken over the flattened array by default,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for number in range(1,50):\n",
    "    anOtherModel = KNeighborsClassifier(n_neighbors=number)\n",
    "    anOtherModel.fit(X_train,y_train)\n",
    "    anOtherpredictions = anOtherModel.predict(X_test)\n",
    "    errors.append(numpyInstance.mean(predictions!=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BETA VERSION FOR PERCISION AND RECALL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing(cm):\n",
    "    Eba = cm[1,0]\n",
    "    Eca = cm[2,0]\n",
    "    Eab = cm[0,1]\n",
    "    Ecb = cm[2,1]\n",
    "    Eac = cm[0,2]\n",
    "    Ebc = cm[1,2]\n",
    "    TPa = cm[0,0]\n",
    "    TPb = cm[1,1]\n",
    "    TPc = cm[2,2]\n",
    "    FNa = Eba+Eca\n",
    "    FNb = Eab+Ecb\n",
    "    FNc = Eac+Ebc\n",
    "    FPa = Eab+Eac\n",
    "    FPb = Eba+Ebc\n",
    "    FPc = Eca+Ecb\n",
    "    TNa = TPb+Ebc+Ecb+TPc\n",
    "    TNb = TPa+Eac+Eca+TPc\n",
    "    TNc = TPa+Eab+Eba+TPb\n",
    "    Total = TPa+Eab+Eac+Eba+TPb+Ebc+Eca+Ecb+TPc\n",
    "    accuracy = (TPa+TPb+TPc)/Total\n",
    "    sensitivityA = (TPa)/(TPa+FNa)\n",
    "    sensitivityB = (TPb)/(TPb+FNb)\n",
    "    sensitivityC = (TPc)/(TPc+FNc)\n",
    "    specificityA = (TNa)/(TNa+FPa)\n",
    "    specificityB = (TNb)/(TNb+FPb)\n",
    "    specificityC = (TNc)/(TNc+FPc)\n",
    "    print(\"accuracy: \",accuracy)\n",
    "    print(\"sensitivityA: \",sensitivityA)\n",
    "    print(\"sensitivityB: \",sensitivityB)\n",
    "    print(\"sensitivityC: \",sensitivityC)\n",
    "    print(\"specificityA: \",specificityA)\n",
    "    print(\"specificityB: \",specificityB)\n",
    "    print(\"specificityC: \",specificityC)\n",
    "    \n",
    "    matrisim=[[\"accuracy: \",accuracy],[\"sensitivityA: \",sensitivityA],\n",
    "          [\"sensitivityB: \",sensitivityB],[\"sensitivityC: \",sensitivityC],\n",
    "          [\"specificityA: \",specificityA],[\"specificityB: \",specificityB],\n",
    "          [\"specificityC: \",specificityC]\n",
    "          ]\n",
    "    return matrisim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlibInstance.figure(figsize=(10,6))\n",
    "matplotlibInstance.plot(range(1,50),errors,color='green', linestyle='dashed', marker='o')\n",
    "matplotlibInstance.title('Error Rate vs. K Value')\n",
    "matplotlibInstance.xlabel('K')\n",
    "matplotlibInstance.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Means that we are already at the Best Accuracy so Value of K=1 is already perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.\n",
    "It is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions.\n",
    "\n",
    "\n",
    "In a Decision tree, there are two nodes, which are the Decision Node and Leaf Node. Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the output of those decisions and do not contain any further branches.\n",
    "\n",
    "The decisions or the test are performed on the basis of features of the given dataset.\n",
    "\n",
    "In order to build a tree, we use the CART algorithm, which stands for Classification and Regression Tree algorithm.\n",
    "\n",
    "A decision tree simply asks a question, and based on the answer (Yes/No), it further split the tree into subtrees.\n",
    "\n",
    "Why to use it :\n",
    "\n",
    "Decision Trees usually mimic human thinking ability while making a decision, so it is easy to understand.\n",
    "\n",
    "The logic behind the decision tree can be easily understood because it shows a tree-like structure.\n",
    "\n",
    "\n",
    "How does the Decision Tree algorithm Work?\n",
    "\n",
    "In a decision tree, for predicting the class of the given dataset, the algorithm starts from the root node of the tree. This algorithm compares the values of root attribute with the record (real dataset) attribute and, based on the comparison, follows the branch and jumps to the next node.\n",
    "\n",
    "For the next node, the algorithm again compares the attribute value with the other sub-nodes and move further. It continues the process until it reaches the leaf node of the tree. The complete process can be better understood using the below algorithm:\n",
    "\n",
    "Step-1: Begin the tree with the root node, says S, which contains the complete dataset.\n",
    "\n",
    "Step-2: Find the best attribute in the dataset using Attribute Selection Measure (ASM).\n",
    "\n",
    "Step-3: Divide the S into subsets that contains possible values for the best attributes.\n",
    "\n",
    "Step-4: Generate the decision tree node, which contains the best attribute.\n",
    "\n",
    "Step-5: Recursively make new decision trees using the subsets of the dataset created in step -3. Continue this process until a stage is reached where you cannot further classify the nodes and called the final node as a leaf node.\n",
    "\n",
    "Attribute Selection Measures: \n",
    "While implementing a Decision tree, the main issue arises that how to select the best attribute for the root node and for sub-nodes. So, to solve such problems there is a technique which is called as Attribute selection measure or ASM. By this measurement, we can easily select the best attribute for the nodes of the tree. There are two popular techniques for ASM, which are:\n",
    "\n",
    "1)Information Gain\n",
    "\n",
    "2)Gini Index\n",
    "\n",
    "\n",
    "Information Gain:\n",
    "\n",
    "Information gain is the measurement of changes in entropy after the segmentation of a dataset based on an attribute.\n",
    "It calculates how much information a feature provides us about a class.\n",
    "According to the value of information gain, we split the node and build the decision tree.\n",
    "A decision tree algorithm always tries to maximize the value of information gain, and a node/attribute having the highest information gain is split first. It can be calculated using the below formula:\n",
    "\n",
    "Information Gain= Entropy(S)- [(Weighted Avg) *Entropy(each feature)\n",
    "\n",
    "Entropy: Entropy is a metric to measure the impurity in a given attribute. It specifies randomness in data. Entropy can be calculated as:\n",
    "\n",
    "Entropy(s)= -P(yes)log2 P(yes)- P(no) log2 P(no)\n",
    "\n",
    "S= Total number of samples\n",
    "P(yes)= probability of yes\n",
    "P(no)= probability of no\n",
    "\n",
    "Gini Index:\n",
    "\n",
    "Gini index is a measure of impurity or purity used while creating a decision tree in the CART(Classification and Regression Tree) algorithm.\n",
    "An attribute with the low Gini index should be preferred as compared to the high Gini index.\n",
    "It only creates binary splits, and the CART algorithm uses the Gini index to create binary splits.\n",
    "Gini index can be calculated using the below formula:\n",
    "\n",
    "Gini Index= 1- ∑j Pj^2\n",
    "\n",
    "\n",
    "Advantages of the Decision Tree:\n",
    "\n",
    "1)It is simple to understand as it follows the same process which a human follow while making any decision in real-life.\n",
    "2)It can be very useful for solving decision-related problems.\n",
    "3)It helps to think about all the possible outcomes for a problem.\n",
    "4)There is less requirement of data cleaning compared to other algorithms.\n",
    "\n",
    "Disadvantages of the Decision Tree:\n",
    "\n",
    "1)The decision tree contains lots of layers, which makes it complex.\n",
    "2)It may have an overfitting issue, which can be resolved using the Random Forest algorithm.\n",
    "3)For more class labels, the computational complexity of the decision tree may increase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train,y_train)\n",
    "feature_importances = pd.DataFrame(decision_tree.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_predict_train = decision_tree.predict(X_train)\n",
    "y_predict_train\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_predict_test = decision_tree.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra tree classtifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extremely randomized tree classifier.\n",
    "\n",
    "Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "try_model = ExtraTreeClassifier(random_state=1)\n",
    "\n",
    "# Fit model\n",
    "try_model.fit(X_train, y_train)\n",
    "\n",
    "#check prediction\n",
    "y_predict = try_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can be used for both Classification and Regression problems in ML. It is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model.\n",
    "\n",
    "As the name suggests, \"Random Forest\" is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset.\" Instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output.\n",
    "\n",
    "The greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.\n",
    "\n",
    "Since the random forest combines multiple trees to predict the class of the dataset, it is possible that some decision trees may predict the correct output, while others may not. But together, all the trees predict the correct output. Therefore, below are two assumptions for a better Random forest classifier:\n",
    "\n",
    "There should be some actual values in the feature variable of the dataset so that the classifier can predict accurate results rather than a guessed result.\n",
    "\n",
    "The predictions from each tree must have very low correlations.\n",
    "\n",
    "Why we should use the Random Forest algorithm:\n",
    "\n",
    "It takes less training time as compared to other algorithms.\n",
    "\n",
    "It predicts output with high accuracy, even for the large dataset it runs efficiently.\n",
    "\n",
    "It can also maintain accuracy when a large proportion of data is missing.\n",
    "\n",
    "HOW DOES IT WORKS:\n",
    "\n",
    "Random Forest works in two-phase first is to create the random forest by combining N decision tree, and second is to make predictions for each tree created in the first phase.\n",
    "\n",
    "The Working process can be explained in the below steps :\n",
    "\n",
    "Step-1: Select random K data points from the training set.\n",
    "\n",
    "Step-2: Build the decision trees associated with the selected data points (Subsets).\n",
    "\n",
    "Step-3: Choose the number N for decision trees that you want to build.\n",
    "\n",
    "Step-4: Repeat Step 1 & 2.\n",
    "\n",
    "Step-5: For new data points, find the predictions of each decision tree, and assign the new data points to the category that wins the majority votes.\n",
    "\n",
    "The working of the algorithm can be better understood by the below example:\n",
    "\n",
    "Example: Suppose there is a dataset that contains multiple fruit images. So, this dataset is given to the Random forest classifier. The dataset is divided into subsets and given to each decision tree. During the training phase, each decision tree produces a prediction result, and when a new data point occurs, then based on the majority of results\n",
    "\n",
    "\n",
    "Advantages of Random Forest:\n",
    "Random Forest is capable of performing both Classification and Regression tasks.\n",
    "\n",
    "It is capable of handling large datasets with high dimensionality.\n",
    "\n",
    "It enhances the accuracy of the model and prevents the overfitting issue.\n",
    "\n",
    "Disadvantages of Random Forest:\n",
    "\n",
    "Although random forest can be used for both classification and regression tasks, it is not more suitable for Regression tasks.\n",
    "\n",
    "\n",
    "Implementation Steps are given below:\n",
    "\n",
    "step 1 : Data Pre-processing step\n",
    "\n",
    "step 2 : Fitting the Random forest algorithm to the Training set\n",
    "\n",
    "step 3 : Predicting the test result\n",
    "\n",
    "step 4 : Test accuracy of the result (Creation of Confusion matrix)\n",
    "\n",
    "step 5 : Visualizing the test set result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier(n_estimators=150)\n",
    "RandomForest.fit(X_train, y_train)\n",
    "\n",
    "y_predict_train = RandomForest.predict(X_train)\n",
    "y_predict_train\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_predict_test = RandomForest.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems.\n",
    "\n",
    "It is mainly used in text classification that includes a high-dimensional training dataset.\n",
    "\n",
    "Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.\n",
    "\n",
    "It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.\n",
    "\n",
    "Some popular examples of Naïve Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles.\n",
    "\n",
    "\n",
    "The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as:\n",
    "\n",
    "Naïve: It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\n",
    "\n",
    "Bayes: It is called Bayes because it depends on the principle of Bayes' Theorem.\n",
    "\n",
    "\n",
    "Bayes' theorem is also known as Bayes' Rule or Bayes' law, which is used to determine the probability of a hypothesis with prior knowledge. It depends on the conditional probability.\n",
    "The formula for Bayes' theorem is given as:\n",
    "Naïve Bayes Classifier Algorithm : P(A|B)= P(B|A)P(A)/ P(B)\n",
    "\n",
    "Where,\n",
    "\n",
    "P(A|B) is Posterior probability: Probability of hypothesis A on the observed event B.\n",
    "\n",
    "P(B|A) is Likelihood probability: Probability of the evidence given that the probability of a hypothesis is true.\n",
    "\n",
    "P(A) is Prior Probability: Probability of hypothesis before observing the evidence.\n",
    "\n",
    "P(B) is Marginal Probability: Probability of Evidence.\n",
    "\n",
    "Steps : \n",
    "\n",
    "step 1: Convert the given dataset into frequency tables.\n",
    "step 2: Generate Likelihood table by finding the probabilities of given features.\n",
    "step 3: Now, use Bayes theorem to calculate the posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})\n",
    "y = data['diagnosis']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size=0.2)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "NB_classifier = GaussianNB()\n",
    "NB_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = NB_classifier.predict(X_train)\n",
    "y_predict_train\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_predict_test = NB_classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scored a 93% precision and 92% recall \n",
    "\n",
    "Now lets investigate our data even more by see the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this simple code we can see that in the [13,92,108,109] positions in the y_test we made a false prediction .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=1\n",
    "for i,j in  zip(y_test, y_predict_test):\n",
    "    if i!= j : print(i,j,pos)\n",
    "    pos+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets remind ourselfs about type1 error and type2 error \n",
    "\n",
    "And see we have 2  error type 1 and 2 type 2 :\n",
    "\n",
    "type 1 is FP - The first 2 \n",
    "\n",
    "type 2 is FN - The last 2 \n",
    "\n",
    "\n",
    "lets explore them even more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test.head(14) -> 157 - \n",
    "#y_test.head(93) -> 413\n",
    "#y_test.head(108) -> 263\n",
    "#y_test.head(109) -> 213\n",
    "\n",
    "X_157=data.iloc[157]\n",
    "X_413=data.iloc[413]\n",
    "X_263=data.iloc[263]\n",
    "X_213=data.iloc[213]\n",
    "\n",
    "Exception_area_mean= [X_157['area_mean'] , X_413['area_mean'], X_263['area_mean'], X_213['area_mean'] ]\n",
    "Exception_smoothness_mean= [X_157['smoothness_mean'] , X_413['smoothness_mean'], X_263['smoothness_mean'], X_213['smoothness_mean'] ]\n",
    "Exception_radius_mean= [X_157['radius_mean'] , X_413['radius_mean'], X_263['radius_mean'], X_213['radius_mean'] ]\n",
    "\n",
    "print(Exception_area_mean)\n",
    "print(Exception_smoothness_mean)\n",
    "print(Exception_radius_mean)\n",
    "\n",
    "#while:\n",
    "print(data['area_mean'].mean())\n",
    "print(data['smoothness_mean'].mean())\n",
    "print(data['radius_mean'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.\n",
    "\n",
    "Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n",
    "\n",
    "Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems.\n",
    "\n",
    "In Logistic regression, instead of fitting a regression line, we fit an \"S\" shaped logistic function, which predicts two maximum values (0 or 1).\n",
    "\n",
    "The curve from the logistic function indicates the likelihood of something such as whether the cells are cancerous or not, a mouse is obese or not based on its weight, etc.\n",
    "\n",
    "Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets.\n",
    "\n",
    "Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification.\n",
    "\n",
    "Note: Logistic regression uses the concept of predictive modeling as regression; therefore, it is called logistic regression, but is used to classify samples; Therefore, it falls under the classification algorithm.\n",
    "\n",
    "Logistic Function (Sigmoid Function):\n",
    "\n",
    "    The sigmoid function is a mathematical function used to map the predicted values to probabilities.\n",
    "\n",
    "    It maps any real value into another value within a range of 0 and 1.\n",
    "\n",
    "    The value of the logistic regression must be between 0 and 1, which cannot go beyond this limit, so it forms a curve like the \"S\" form. The S-form curve is called the Sigmoid function or the logistic function.\n",
    "\n",
    "    In logistic regression, we use the concept of the threshold value, which defines the probability of either 0 or 1. Such as values above the threshold value tends to 1, and a value below the threshold values tends to 0.\n",
    "    \n",
    "    Sigmoid function(x)= e^x / (e^x + 1)\n",
    "    \n",
    "Assumptions for Logistic Regression:\n",
    "\n",
    "    The dependent variable must be categorical in nature.\n",
    "    \n",
    "    The independent variable should not have multi-collinearity.\n",
    "\n",
    "Logistic Regression Equation:\n",
    "\n",
    "    The Logistic regression equation can be obtained from the Linear Regression equation. The mathematical steps to get Logistic Regression equations are given below:\n",
    "\n",
    "    We know the equation of the straight line can be written as:\n",
    "    y= b0+b1x1+ ....+ bnxn\n",
    "    \n",
    "    In Logistic Regression y can be between 0 and 1 only, so for this let's divide the above equation by (1-y):\n",
    "    y/ 1-y .\n",
    "    0 for y=0\n",
    "    inf for y=1\n",
    "    \n",
    "    But we need range between -[infinity] to +[infinity], then take logarithm of the equation it will become:\n",
    "    log(y/ (1-y)) = b0+b1x1+ ....+ bnxn\n",
    "    \n",
    "    \n",
    "Type of Logistic Regression:\n",
    "\n",
    "On the basis of the categories, Logistic Regression can be classified into three types:\n",
    "\n",
    "    Binomial: In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc.\n",
    "   \n",
    "    Multinomial: In multinomial Logistic regression, there can be 3 or more possible unordered types of the dependent variable, such as \"cat\", \"dogs\", or \"sheep\"\n",
    "    Ordinal: In ordinal Logistic regression, there can be 3 or more possible ordered types of dependent variables, such as \"low\", \"Medium\", or \"High\".\n",
    "\n",
    "\n",
    "Steps in Logistic Regression: To implement the Logistic Regression using Python, we will use the same steps as we have done in previous topics of Regression. Below are the steps:\n",
    "\n",
    "    Data Pre-processing step\n",
    "    \n",
    "    Fitting Logistic Regression to the Training set\n",
    "    \n",
    "    Predicting the test result\n",
    "    \n",
    "    Test accuracy of the result(Creation of Confusion matrix)\n",
    "    \n",
    "    Visualizing the test set result.\n",
    "    \n",
    "Data Pre-processing step:\n",
    "\n",
    "    In this step, we will pre-process/prepare the data so that we can use it in our code efficiently. It will be the same as we have done in Data pre-processing topic. In logistic regression, we will do feature scaling because we want accurate result of predictions. Here we will only scale the independent variable because dependent variable have only 0 and 1 values. \n",
    "\n",
    "Fitting Logistic Regression to the Training set:\n",
    "\n",
    "    We have well prepared our dataset, and now we will train the dataset using the training set. For providing training or fitting the model to the training set, we will import the LogisticRegression class of the sklearn library.\n",
    "\n",
    "    After importing the class, we will create a classifier object and use it to fit the model to the logistic regression\n",
    "\n",
    "\n",
    "Predicting the Test Result:\n",
    "\n",
    "    Our model is well trained on the training set, so we will now predict the result by using test set data. \n",
    "\n",
    "Test Accuracy of the result:\n",
    "\n",
    "    Now we will create the confusion matrix here to check the accuracy of the classification. To create it, we need to import the confusion_matrix function of the sklearn library. After importing the function, we will call it using a new variable cm. The function takes two parameters, mainly y_true( the actual values) and y_pred (the targeted value return by the classifier). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = classifier.predict(X_train)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_test = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network multi layer perspetron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "try_model = MLPClassifier()\n",
    "\n",
    "# Fit model\n",
    "try_model.fit(X_train, y_train)\n",
    "\n",
    "#check prediction\n",
    "y_predict = try_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bernuli & linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc.\n",
    "\n",
    "Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.\n",
    "\n",
    "The linear regression model provides a sloped straight line representing the relationship between the variables\n",
    "\n",
    "Mathematically, we can represent a linear regression as:\n",
    "\n",
    "    y= a0+a1x+ ε\n",
    "\n",
    "Here,\n",
    "\n",
    "    Y= Dependent Variable (Target Variable)\n",
    "\n",
    "    X= Independent Variable (predictor Variable)\n",
    "\n",
    "    a0= intercept of the line (Gives an additional degree of freedom)\n",
    "\n",
    "    a1 = Linear regression coefficient (scale factor to each input value).\n",
    "\n",
    "    ε = random error\n",
    "\n",
    "The values for x and y variables are training datasets for Linear Regression model representation.\n",
    "\n",
    "Linear regression can be further divided into two types of the algorithm:\n",
    "\n",
    "Simple Linear Regression:\n",
    "\n",
    "    If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.\n",
    "    \n",
    "Multiple Linear regression:\n",
    "\n",
    "    If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression.\n",
    "    \n",
    "A linear line showing the relationship between the dependent and independent variables is called a regression line. A regression line can show two types of relationship:\n",
    "\n",
    "Positive Linear Relationship:\n",
    "\n",
    "    If the dependent variable increases on the Y-axis and independent variable increases on X-axis, then such a relationship is termed as a Positive linear relationship.\n",
    "\n",
    "Negative Linear Relationship:\n",
    "\n",
    "    If the dependent variable decreases on the Y-axis and independent variable increases on the X-axis, then such a relationship is called a negative linear relationship.\n",
    "    \n",
    "    \n",
    "Finding the best fit line:\n",
    "\n",
    "    When working with linear regression, our main goal is to find the best fit line that means the error between predicted values and actual values should be minimized. The best fit line will have the least error.\n",
    "\n",
    "    The different values for weights or the coefficient of lines (a0, a1) gives a different line of regression, so we need to calculate the best values for a0 and a1 to find the best fit line, so to calculate this we use cost function.\n",
    "\n",
    "\n",
    "Cost function-\n",
    "\n",
    "    The different values for weights or coefficient of lines (a0, a1) gives the different line of regression, and the cost function is used to estimate the values of the coefficient for the best fit line.\n",
    "\n",
    "    Cost function optimizes the regression coefficients or weights. It measures how a linear regression model is performing.\n",
    "\n",
    "    We can use the cost function to find the accuracy of the mapping function, which maps the input variable to the output variable. This mapping function is also known as Hypothesis function.\n",
    "\n",
    "    For Linear Regression, we use the Mean Squared Error (MSE) cost function, which is the average of squared error occurred between the predicted values and actual values. It can be written as:\n",
    "\n",
    "    For the above linear equation, MSE can be calculated as:\n",
    "\n",
    "    MSE = 1/N * sigma 1->n (yi - (a1 * xi + a0) )^2\n",
    "\n",
    "\n",
    "    Where,\n",
    "\n",
    "        N=Total number of observation\n",
    "        Yi = Actual value\n",
    "        (a1xi+a0)= Predicted value.\n",
    "\n",
    "\n",
    "Note : \n",
    "\n",
    "Residuals: The distance between the actual value and predicted values is called residual. If the observed points are far from the regression line, then the residual will be high, and so cost function will high. If the scatter points are close to the regression line, then the residual will be small and hence the cost function.\n",
    "\n",
    "Assumptions of Linear Regression:\n",
    "\n",
    "\n",
    "Below are some important assumptions of Linear Regression. These are some formal checks while building a Linear Regression model, which ensures to get the best possible result from the given dataset.\n",
    "\n",
    "Linear relationship between the features and target:\n",
    "\n",
    "Linear regression assumes the linear relationship between the dependent and independent variables.\n",
    "\n",
    "Small or no multicollinearity between the features:\n",
    "\n",
    "    Multicollinearity means high-correlation between the independent variables. Due to multicollinearity, it may difficult to find the true relationship between the predictors and target variables. Or we can say, it is difficult to determine which predictor variable is affecting the target variable and which is not. So, the model assumes either little or no multicollinearity between the features or independent variables.\n",
    "\n",
    "Homoscedasticity Assumption:\n",
    "\n",
    "    Homoscedasticity is a situation when the error term is the same for all the values of independent variables. With homoscedasticity, there should be no clear pattern distribution of data in the scatter plot.\n",
    "\n",
    "Normal distribution of error terms:\n",
    "\n",
    "    Linear regression assumes that the error term should follow the normal distribution pattern. If error terms are not normally distributed, then confidence intervals will become either too wide or too narrow, which may cause difficulties in finding coefficients.\n",
    "    It can be checked using the q-q plot. If the plot shows a straight line without any deviation, which means the error is normally distributed.\n",
    "\n",
    "No autocorrelations:\n",
    "\n",
    "    The linear regression model assumes no autocorrelation in error terms. If there will be any correlation in the error term, then it will drastically reduce the accuracy of the model. Autocorrelation usually occurs if there is a dependency between residual errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Restricted Boltzmann Machine (RBM).\n",
    "\n",
    "A Restricted Boltzmann Machine with binary visible units and binary hiddens. Parameters are estimated using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD) \n",
    "\n",
    "The time complexity of this implementation is O(d ** 2) assuming d ~ n_features ~ n_components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "logistic = linear_model.LogisticRegression(solver='newton-cg', tol=1)\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "rbm_features_classifier = Pipeline(\n",
    "    steps=[('rbm', rbm), ('logistic', logistic)])\n",
    "\n",
    "# Hyper-parameters. These were set by cross-validation,\n",
    "# using a GridSearchCV. Here we are not performing cross-validation to\n",
    "# save time.\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 10\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 100\n",
    "logistic.C = 6000\n",
    "\n",
    "# Fit model\n",
    "rbm_features_classifier.fit(X_train, y_train)\n",
    "\n",
    "#check prediction\n",
    "y_predict = rbm_features_classifier.predict(X_test)\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, y_predict)))\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "#accuracy and f1\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_test, y_predict)\n",
    "f1 = f1_score(y_test, y_predict, average='macro')\n",
    "f'The accuracy is %.2f     The f1_score is %.2f' % (acc,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Decent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent:\n",
    "\n",
    "    Gradient descent is used to minimize the MSE by calculating the gradient of the cost function.\n",
    "    A regression model uses gradient descent to update the coefficients of the line by reducing the cost function.\n",
    "    It is done by a random selection of values of coefficient and then iteratively update the values to reach the minimum cost function\n",
    "\n",
    "\n",
    "Model Performance:\n",
    "\n",
    "The Goodness of fit determines how the line of regression fits the set of observations. The process of finding the best model out of various models is called optimization. It can be achieved by below method:\n",
    "\n",
    "R-squared method:\n",
    "\n",
    "    R-squared is a statistical method that determines the goodness of fit.\n",
    "\n",
    "    It measures the strength of the relationship between the dependent and independent variables on a scale of 0-100%.\n",
    "\n",
    "    The high value of R-square determines the less difference between the predicted values and actual values and hence represents a good model.\n",
    "\n",
    "    It is also called a coefficient of determination, or coefficient of multiple determination for multiple regression.\n",
    "    \n",
    "    R-squared = Explained Variation / Total Variation\n",
    "\n",
    "\n",
    "Our biggest question is, how we train a model to determine the weight parameters which will minimize our error function. Let starts how gradient descent help us to train our model.\n",
    "\n",
    "First, the linear model will begin with a random initial parameter recall when we initialize the model with the linear function. It indeed gave us a random initial parameter.\n",
    "\n",
    "Let's ignore the bias value for now and based on the error associated with this initial parameter A. Our motive is to move in the direction that gives us the smaller error.\n",
    "\n",
    "If we take the gradient of error function the derivatives of the slope of the tangent at the current value that we met, this derivative will take us in the direction of the highest error.\n",
    "\n",
    "In a summarized way, first, we have to compute the derivatives of the loss function and then submission in the current weight value of the line. Whatever the weight maybe, they will give you the gradient value. This gradient value is then subtracted from the current weight A0, to give the new updated weight A1. The new weight should result in a smaller error than the previous one. We will do that iteratively until we obtain the optimal parameter for our line model to fit the data.\n",
    "\n",
    "We are descending with the gradient, however, to ensure optimal results. One should descend in minimal steps. As such, we will multiply the gradient by a minimal number known as the learning rate. The value of the learning rate is empirical. Although a good standard starting value tends to be one over 10 or 1 over 100, the learning rate needs to be sufficiently small since as the line adjusting itself you never wanted to move drastically in one direction as that can cause for unwanted divergent behavior.\n",
    "\n",
    "In this, we will learn to adjust the earning rate based on empirical results, and we will code a gradient descent algorithm later but follow through with our gradient descent example let's refer to a demonstration on excel to visualize the effect of gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD Classifier implements regularised linear models with Stochastic Gradient Descent.\n",
    "\n",
    "Stochastic gradient descent considers only 1 random point while changing weights unlike gradient descent which considers the whole training data. As such stochastic gradient descent is much faster than gradient descent when dealing with large data sets.\n",
    "\n",
    "SGD Classifier VS Logistic Regression?\n",
    "\n",
    "Logistic Regression by default uses Gradient Descent and as such it would be better to use SGD Classifier on larger data sets. One another reason you might want to use SGD Classifier is, logistic regression, in its vanilla sklearn form, won’t work if you can’t hold the dataset in RAM but SGD will still work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "accuracy_selection=[]\n",
    "cvs_selection=[]\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "accuracy_selection.append(accuracy_score(prediction, y_test))\n",
    "cvs_selection.append(np.mean(scores))\n",
    "\n",
    "print(\"SGD Classifier Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\n",
    "print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\n",
    "print(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a combined model using 3 models\n",
    "after we got very good result we thought maybe if we combine 3 models together and try to do an democratic vote between then we may get better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "data =data.iloc[:,1:-1]\n",
    "X = data.drop(['diagnosis'],axis=1)\n",
    "y = data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.get_dummies(y)\n",
    "y = d.drop('M', axis=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are doing again 3 models with good results:svc,neural network and bernouli\n",
    "for each model we take the result in form of - or 1 instead of B or M in prder to make addition and get a \"vote\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define model\n",
    "svc_model = SVC()\n",
    "\n",
    "# Fit model\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "#check prediction\n",
    "y_predict1 = svc_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict1)\n",
    "\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "\n",
    "#accuracy and f1\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_test, y_predict1)\n",
    "f1 = f1_score(y_test, y_predict1, average='macro')\n",
    "f'The accuracy is %.2f     The f1_score is %.2f' % (acc,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron:\n",
    "\n",
    "Perceptron is a single layer neural network, or we can say a neural network is a multi-layer perceptron. Perceptron is a binary classifier, and it is used in supervised learning. A simple model of a biological neuron in an artificial neural network is known as Perceptron.\n",
    "\n",
    "A function that can decide whether or not an input which is represented by a vector of number belongs to some specific class is known as binary classifiers. The binary classifier is a type of linear classifier. A linear classifier is a classification algorithm which makes its predictions based on a linear predictor function combining a set of weight with the feature vector.\n",
    "\n",
    "The perceptron algorithm was designed to categorizing subjects into one of two types, classify visual input and separating groups with a line. Classification is a key part of image processing and machine learning. The perceptron algorithm classifies patterns, i.e., find and classify by many different means using machine learning algorithm, and groups by finding the linear separation between different objects and patterns which are received through numeric or visual input.\n",
    "\n",
    "\n",
    "Perceptron consist of four parts and which are required to understand for the implementation of the perceptron model in PyTorch.\n",
    "\n",
    "Input values or one input layer\n",
    "The input layer of a perceptron is made of artificial input neurons and brings the initial data into the system for further processing.\n",
    "\n",
    "Weights and bias\n",
    "Weight represents the strength or dimension of the connection between units. If the weight from node 1 to node 2 has the greater quantity, then neuron 1 has greater influence over neuron 2. How much influence of the input will have on the output, is determined by weight.\n",
    "Bias is similar to the intercept added in a linear equation. It is an additional parameter which task is to adjust the output along with the weighted sum of the inputs to the neuron.\n",
    "\n",
    "Activation Function\n",
    "A neuron should be activated or not, is determined by an activation function. Activation function calculates a weighted sum and further adding bias with it to give the result.\n",
    "\n",
    "\n",
    "Neural Network is based on the Perceptron, so if we want to know the working of the neural network, learn how perceptron work.\n",
    "\n",
    "The Perceptron works on three simple steps which are as follows:\n",
    "\n",
    "a) In the first step, all the input x are multiplied with their weights denoted as K. This step is essential because the output of this step will be input for the next step.\n",
    "\n",
    "b) Next step is to add all the multiplied value from K1 to Kn. It is known as the weighted sum. This weighted sum will be treated as an input for the next step.\n",
    "\n",
    "c) In the next step, the weighted sum, which is calculated from the previous step, is applied to the correct activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "neuro_model = MLPClassifier()\n",
    "\n",
    "# Fit model\n",
    "neuro_model.fit(X_train, y_train)\n",
    "\n",
    "#check prediction\n",
    "y_predict2 = neuro_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict2)\n",
    "\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "#accuracy and f1\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_test, y_predict2)\n",
    "f1 = f1_score(y_test, y_predict2, average='macro')\n",
    "f'The accuracy is %.2f     The f1_score is %.2f' % (acc,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bernouliRBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference to see above in the # bernuli & linear regression section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "logistic = linear_model.LogisticRegression(solver='newton-cg', tol=1)\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "rbm_features_classifier = Pipeline(\n",
    "    steps=[('rbm', rbm), ('logistic', logistic)])\n",
    "\n",
    "# Hyper-parameters. These were set by cross-validation,\n",
    "# using a GridSearchCV. Here we are not performing cross-validation to\n",
    "# save time.\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 10\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 100\n",
    "logistic.C = 6000\n",
    "\n",
    "# Fit model\n",
    "rbm_features_classifier.fit(X_train, y_train)\n",
    "\n",
    "#check prediction\n",
    "y_predict3 = rbm_features_classifier.predict(X_test)\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, y_predict3)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict3)\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "#accuracy and f1\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_test, y_predict3)\n",
    "f1 = f1_score(y_test, y_predict3, average='macro')\n",
    "f'The accuracy is %.2f     The f1_score is %.2f' % (acc,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=y_predict1+y_predict2+y_predict3\n",
    "\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=y_predict//2\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predict)\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "#accuracy and f1\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_test, y_predict)\n",
    "f1 = f1_score(y_test, y_predict, average='macro')\n",
    "f'The accuracy is %.2f     The f1_score is %.2f' % (acc,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfotunally it didn't work. \n",
    "we got same result as our best model, we think the reason is that the models make mistakes on the same objects so even when we try to make a  vote they make a mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISING PCA AND TENSORFLOW PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation. These new transformed features are called the Principal Components. It is one of the popular tools that is used for exploratory data analysis and predictive modeling. It is a technique to draw strong patterns from the given dataset by reducing the variances.\n",
    "\n",
    "PCA generally tries to find the lower-dimensional surface to project the high-dimensional data.\n",
    "\n",
    "PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality. Some real-world applications of PCA are image processing, movie recommendation system, optimizing the power allocation in various communication channels. It is a feature extraction technique, so it contains the important variables and drops the least important variable.\n",
    "\n",
    "The PCA algorithm is based on some mathematical concepts such as:\n",
    "\n",
    "Variance and Covariance\n",
    "\n",
    "Eigenvalues and Eigen factors\n",
    "\n",
    "Some common terms used in PCA algorithm:\n",
    "\n",
    "Dimensionality: \n",
    "\n",
    "    It is the number of features or variables present in the given dataset. More easily, it is the number of columns present in the dataset.\n",
    "\n",
    "Correlation: \n",
    "\n",
    "    It signifies that how strongly two variables are related to each other. Such as if one changes, the other variable also gets changed. The correlation value ranges from -1 to +1. Here, -1 occurs if variables are inversely proportional to each other, and +1 indicates that variables are directly proportional to each other.\n",
    "    \n",
    "Orthogonal: \n",
    "\n",
    "    It defines that variables are not correlated to each other, and hence the correlation between the pair of variables is zero.\n",
    "    \n",
    "Eigenvectors: \n",
    "\n",
    "    If there is a square matrix M, and a non-zero vector v is given. Then v will be eigenvector if Av is the scalar multiple of v.\n",
    "Covariance Matrix:\n",
    "\n",
    "    A matrix containing the covariance between the pair of variables is called the Covariance Matrix.\n",
    "    \n",
    "    \n",
    "Steps for PCA algorithm : \n",
    "\n",
    "\n",
    "1. Getting the dataset:\n",
    "\n",
    "Firstly, we need to take the input dataset and divide it into two subparts X and Y, where X is the training set, and Y is the validation set.\n",
    "\n",
    "2. Representing data into a structure:\n",
    "\n",
    "Now we will represent our dataset into a structure. Such as we will represent the two-dimensional matrix of independent variable X. Here each row corresponds to the data items, and the column corresponds to the Features. The number of columns is the dimensions of the dataset.\n",
    "\n",
    "3. Standardizing the data:\n",
    "In this step, we will standardize our dataset. Such as in a particular column, the features with high variance are more important compared to the features with lower variance.\n",
    "If the importance of features is independent of the variance of the feature, then we will divide each data item in a column with the standard deviation of the column. Here we will name the matrix as Z.\n",
    "\n",
    "4. Calculating the Covariance of Z:\n",
    "\n",
    "To calculate the covariance of Z, we will take the matrix Z, and will transpose it. After transpose, we will multiply it by Z. The output matrix will be the Covariance matrix of Z.\n",
    "\n",
    "5. Calculating the Eigen Values and Eigen Vectors:\n",
    "\n",
    "Now we need to calculate the eigenvalues and eigenvectors for the resultant covariance matrix Z. Eigenvectors or the covariance matrix are the directions of the axes with high information. And the coefficients of these eigenvectors are defined as the eigenvalues.\n",
    "\n",
    "6. Sorting the Eigen Vectors:\n",
    "In this step, we will take all the eigenvalues and will sort them in decreasing order, which means from largest to smallest. And simultaneously sort the eigenvectors accordingly in matrix P of eigenvalues. The resultant matrix will be named as P*.\n",
    "\n",
    "7. Calculating the new features Or Principal Components:\n",
    "Here we will calculate the new features. To do this, we will multiply the P* matrix to the Z. In the resultant matrix Z*, each observation is the linear combination of original features. Each column of the Z* matrix is independent of each other.\n",
    "\n",
    "\n",
    "8. Remove less or unimportant features from the new dataset:\n",
    "The new feature set has occurred, so we will decide here what to keep and what to remove. It means, we will only keep the relevant or important features in the new dataset, and unimportant features will be removed out.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a popular framework of machine learning and deep learning. It is a free and open-source library which is released on 9 November 2015 and developed by Google Brain Team. It is entirely based on Python programming language and use for numerical computation and data flow, which makes machine learning faster and easier.\n",
    "\n",
    "TensorFlow can train and run the deep neural networks for image recognition, handwritten digit classification, recurrent neural network, word embedding, natural language processing, video detection, and many more. TensorFlow is run on multiple CPUs or GPUs and also mobile operating systems.\n",
    "\n",
    "\n",
    "The word TensorFlow is made by two words, i.e., Tensor and Flow\n",
    "\n",
    "Tensor is a multidimensional array\n",
    "Flow is used to define the flow of data in operation.\n",
    "TensorFlow is used to define the flow of data in operation on a multidimensional array or Tensor.\n",
    "\n",
    "It is mainly used for deep learning or machine learning problems such as Classification, Perception, Understanding, Discovering Prediction, and Creation.\n",
    "\n",
    "Components of TensorFlow : \n",
    "\n",
    "\n",
    "Tensor:\n",
    "\n",
    "The name TensorFlow is derived from its core framework, \"Tensor.\" A tensor is a vector or a matrix of n-dimensional that represents all type of data. All values in a tensor hold similar data type with a known shape. The shape of the data is the dimension of the matrix or an array.\n",
    "\n",
    "A tensor can be generated from the input data or the result of a computation. In TensorFlow, all operations are conducted inside a graph. The group is a set of calculation that takes place successively. Each transaction is called an op node are connected.\n",
    "\n",
    "Graph: \n",
    "\n",
    "TensorFlow makes use of a graph framework. The chart gathers and describes all the computations done during the training.\n",
    "\n",
    "\n",
    "\n",
    "Advantages: \n",
    "\n",
    "    It was fixed to run on multiple CPUs or GPUs and mobile operating systems.\n",
    "    The portability of the graph allows to conserve the computations for current or later use. The graph can be saved because it can be executed in the future.\n",
    "    All the computation in the graph is done by connecting tensors together.\n",
    "    \n",
    "Consider the following expression a= (b+c)*(c+2)\n",
    "\n",
    "We can break the functions into components given below:\n",
    "\n",
    "d=b+c\n",
    "e=c+2\n",
    "a=d*e\n",
    "\n",
    "Now, we can represent these operations graphically\n",
    "\n",
    "Session: \n",
    "\n",
    "A session can execute the operation from the graph. To feed the graph with the value of a tensor, we need to open a session. Inside a session, we must run an operator to create an output.\n",
    "\n",
    "\n",
    "Steps of Creating TensorFlow pipeline\n",
    "\n",
    "    Step 1) Create the data\n",
    "\n",
    "    Step 2: Create the placeholder\n",
    "\n",
    "    Step 3: Define the dataset.\n",
    "\n",
    "    Step 4: Create a pipeline :  load the pipeline where the data is flow\n",
    "\n",
    "    Step 5: Execute the operation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is a single processing unit of any neural network. Frank Rosenblatt first proposed in 1958 is a simple neuron which is used to classify its input into one or two categories. Perceptron is a linear classifier, and is used in supervised learning. It helps to organize the given input data.Our goal is to find a linear decision function measured by the weight vector w and the bias parameter b.\n",
    "\n",
    "Perceptron is mainly used to classify the data into two parts. Therefore, it is also known as Linear Binary Classifier.\n",
    "\n",
    "Perceptron uses the step function that returns +1 if the weighted sum of its input 0 and -1.\n",
    "\n",
    "The activation function is used to map the input between the required value like (0, 1) or (-1, 1).\n",
    "\n",
    "The perceptron consists of 4 parts.\n",
    "\n",
    "Input value or One input layer:\n",
    "\n",
    "    The input layer of the perceptron is made of artificial input neurons and takes the initial data into the system for further processing.\n",
    "Weights and Bias:\n",
    "\n",
    "Weight:\n",
    "\n",
    "    It represents the dimension or strength of the connection between units. If the weight to node 1 to node 2 has a higher quantity, then neuron 1 has a more considerable influence on the neuron.\n",
    "Bias:\n",
    "\n",
    "    It is the same as the intercept added in a linear equation. It is an additional parameter which task is to modify the output along with the weighted sum of the input to the other neuron.\n",
    "Net sum:\n",
    "\n",
    "    It calculates the total sum.\n",
    "Activation Function:\n",
    "\n",
    "    A neuron can be activated or not, is determined by an activation function. The activation function calculates a weighted sum and further adding bias with it to give the result.\n",
    "\n",
    "To understand the perceptron layer, it is necessary to comprehend artificial neural networks (ANNs).The artificial neural network (ANN) is an information processing system, whose mechanism is inspired by the functionality of biological neural circuits. An artificial neural network consists of several processing units that are interconnected.\n",
    "\n",
    "This is the first proposal when the neural model is built. The content of the neuron's local memory contains a vector of weight.\n",
    "\n",
    "The single vector perceptron is calculated by calculating the sum of the input vector multiplied by the corresponding element of the vector, with each increasing the amount of the corresponding component of the vector by weight. The value that is displayed in the output is the input of an activation function.\n",
    "\n",
    "Let us focus on the implementation of a single-layer perceptron for an image classification problem using TensorFlow. The best example of drawing a single-layer perceptron is through the representation of \"logistic regression.\"\n",
    "\n",
    "\n",
    "Now, We have to do the following necessary steps of training logistic regression:\n",
    "\n",
    "The weights are initialized with the random values at the origination of each training.\n",
    "\n",
    "For each element of the training set, the error is calculated with the difference between the desired output and the actual output. The calculated error is used to adjust the weight.\n",
    "\n",
    "The process is repeated until the fault made on the entire training set is less than the specified limit until the maximum number of iterations has been reached.\n",
    "\n",
    "\n",
    "Hidden layer:\n",
    "\n",
    "A hidden layer is an artificial neural network that is a layer in between input layers and output layers. Where the artificial neurons take in a set of weighted inputs and produce an output through an activation function. It is a part of nearly and neural in which engineers simulate the types of activity that go on in the human brain.\n",
    "\n",
    "The hidden neural network is set up in some techniques. In many cases, weighted inputs are randomly assigned. On the other hand, they are fine-tuned and calibrated through a process called backpropagation.\n",
    "\n",
    "The artificial neuron in the hidden layer of perceptron works as a biological neuron in the brain- it takes in its probabilistic input signals, and works on them. And it converts them into an output corresponding to the biological neuron's axon.\n",
    "\n",
    "Layers after the input layer are called hidden because they are directly resolved to the input. The simplest network structure is to have a single neuron in the hidden layer that directly outputs the value.\n",
    "\n",
    "Deep learning can refer to having many hidden layers in our neural network. They are deep because they will have been unimaginably slow to train historically, but may take seconds or minutes to prepare using modern techniques and hardware.\n",
    "\n",
    "A single hidden layer will build a simple network.\n",
    "\n",
    "\n",
    "Multi-layer Perceptron in TensorFlow: \n",
    "\n",
    "\n",
    "Multi-Layer perceptron defines the most complex architecture of artificial neural networks. It is substantially formed from multiple layers of the perceptron. TensorFlow is a very popular deep learning framework released by, and this notebook will guide to build a neural network with this library. If we want to understand what is a Multi-layer perceptron, we have to develop a multi-layer perceptron from scratch using Numpy.\n",
    "\n",
    "\n",
    "MLP networks are used for supervised learning format. A typical learning algorithm for MLP networks is also called back propagation's algorithm.\n",
    "\n",
    "A multilayer perceptron (MLP) is a feed forward artificial neural network that generates a set of outputs from a set of inputs. An MLP is characterized by several layers of input nodes connected as a directed graph between the input nodes connected as a directed graph between the input and output layers. MLP uses backpropagation for training the network. MLP is a deep learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the diagnosis column to numeric format\n",
    "# data['diagnosis'] = data['diagnosis'].factorize()[0]\n",
    "# # Fill all Null values with zero\n",
    "# data = data.fillna(value=0)\n",
    "# # Store the diagnosis column in a target object and then drop it\n",
    "# target = data['diagnosis']\n",
    "# data = data.drop('diagnosis', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # Principal Component Analysis module\n",
    "from sklearn.manifold import TSNE # TSNE module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dataframe into arrays\n",
    "X = data.values\n",
    "\n",
    "# Invoke the PCA method. Since this is a binary classification problem\n",
    "# let's call n_components = 2\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_2d = pca.fit_transform(X)\n",
    "\n",
    "# Invoke the TSNE method\n",
    "# tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=2000)\n",
    "# tsne_results = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the TSNE and PCA visuals side-by-side\n",
    "# plt.figure(figsize = (16,11))\n",
    "# plt.subplot(121)\n",
    "# plt.scatter(pca_2d[:,0],pca_2d[:,1], c = target, \n",
    "#             cmap = \"coolwarm\", edgecolor = \"None\", alpha=0.35)\n",
    "# plt.colorbar()\n",
    "# plt.title('PCA Scatter Plot')\n",
    "# plt.subplot(122)\n",
    "# plt.scatter(tsne_results[:,0],tsne_results[:,1],  c = target, \n",
    "#             cmap = \"coolwarm\", edgecolor = \"None\", alpha=0.35)\n",
    "# plt.colorbar()\n",
    "# plt.title('TSNE Scatter Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "data=data.iloc[:,1:-1]\n",
    "data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})\n",
    "X = data.drop(['diagnosis'],axis=1)\n",
    "y = data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors\n",
    "\n",
    "# z = np.exp(-X1**2-X2**2)*1.5+0.2\n",
    "\n",
    "# colors = [\"red\", \"blue\"]\n",
    "# cmap= matplotlib.colors.ListedColormap(colors)\n",
    "# boundaries=[z.min(), 0.35, 0.7, 1.05, 1.4, z.max()]\n",
    "\n",
    "# m = plt.contourf(X1, X2, z, levels = boundaries, cmap=cmap)\n",
    "\n",
    "# plt.colorbar(m, spacing=\"proportional\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), cmap = ListedColormap((\"red\",\"blue\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras and Tenserflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after working on models from sklearn we felt comfortable enough to start more complicate model and try tensorflow and pytorch\n",
    "westarted with a sequential model from tensorflow \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KERAS :\n",
    "\n",
    "Keras is an open-source high-level Neural Network library, which is written in Python is capable enough to run on Theano, TensorFlow, or CNTK. It was developed by one of the Google engineers, Francois Chollet. It is made user-friendly, extensible, and modular for facilitating faster experimentation with deep neural networks. It not only supports Convolutional Networks and Recurrent Networks individually but also their combination.\n",
    "It has a 2X growth ever since every year it has grown. Big companies like Microsoft, Google, NVIDIA, and Amazon have actively contributed to the development of Keras. \n",
    "\n",
    "\n",
    "What makes Keras special?\n",
    "\n",
    "    Focus on user experience has always been a major part of Keras.\n",
    "    Large adoption in the industry.\n",
    "    It is a multi backend and supports multi-platform, which helps all the encoders come together for coding.\n",
    "    Research community present for Keras works amazingly with the production community.\n",
    "    Easy to grasp all concepts.\n",
    "    It supports fast prototyping.\n",
    "    It seamlessly runs on CPU as well as GPU.\n",
    "    It provides the freedom to design any architecture, which then later is utilized as an API for the project.\n",
    "    It is really very simple to get started with.\n",
    "    Easy production of models actually makes Keras special.\n",
    "    \n",
    "Keras being a model-level library helps in developing deep learning models by offering high-level building blocks. All the low-level computations such as products of Tensor, convolutions, etc. are not handled by Keras itself, rather they depend on a specialized tensor manipulation library that is well optimized to serve as a backend engine. Keras has managed it so perfectly that instead of incorporating one single library of tensor and performing operations related to that particular library, it offers plugging of different backend engines into Keras.\n",
    "\n",
    "Keras consist of three backend engines, which are as follows:\n",
    "\n",
    "\n",
    "TensorFlow:\n",
    "TensorFlow is a Google product, which is one of the most famous deep learning tools widely used in the research area of machine learning and deep neural network. It is built in such a way that it can easily run on multiple CPUs and GPUs as well as on mobile operating systems. \n",
    "\n",
    "Theano:\n",
    "Theano was developed at the University of Montreal, Quebec, Canada, by the MILA group. It is an open-source python library that is widely used for performing mathematical operations on multi-dimensional arrays by incorporating scipy and numpy. It utilizes GPUs for faster computation and efficiently computes the gradients by building symbolic graphs automatically. It has come out to be very suitable for unstable expressions, as it first observes them numerically and then computes them with more stable algorithms.\n",
    "\n",
    "CNTK:\n",
    "Microsoft Cognitive Toolkit is deep learning's open-source framework. It consists of all the basic building blocks, which are required to form a neural network. The models are trained using C++ or Python, but it incorporates C# or Java to load the model for making predictions.\n",
    "\n",
    "\n",
    "Advantages of Keras:\n",
    "\n",
    "    It is very easy to understand and incorporate the faster deployment of network models.\n",
    "    It has huge community support in the market as most of the AI companies are keen on using it.\n",
    "    It supports multi backend, which means you can use any one of them among TensorFlow, CNTK, and Theano with Keras as a backend according to your requirement.\n",
    "    Since it has an easy deployment, it also holds support for cross-platform. Following are the devices on which Keras can be deployed:\n",
    "        iOS with CoreML\n",
    "        Android with TensorFlow Android\n",
    "        Web browser with .js support\n",
    "        Cloud engine\n",
    "        Raspberry pi\n",
    "    It supports Data parallelism, which means Keras can be trained on multiple GPU's at an instance for speeding up the training time and processing a huge amount of data.\n",
    "    Disadvantages of Keras\n",
    "    The only disadvantage is that Keras has its own pre-configured layers, and if you want to create an abstract layer, it won't let you because it cannot handle low-level APIs. It only supports high-level API running on the top of the backend engine (TensorFlow, Theano, and CNTK).\n",
    " \n",
    "The only disadvantage is that Keras has its own pre-configured layers, and if you want to create an abstract layer, it won't let you because it cannot handle low-level APIs. It only supports high-level API running on the top of the backend engine .\n",
    "\n",
    "Keras Models\n",
    "Keras has come up with two types of in-built models; Sequential Model and an advanced Model class with functional API. The Sequential model tends to be one of the simplest models as it constitutes a linear set of layers, whereas the functional API model leads to the creation of an arbitrary network structure.\n",
    "\n",
    "\n",
    "Keras Sequential Model:\n",
    "The layers within the sequential models are sequentially arranged, so it is known as Sequential API. In most of the Artificial Neural Network, the layers are sequentially arranged, such that the data flow in between layers is in a specified sequence until it hit the output layer.\n",
    "The sequential model can be simply created by passing a list of instances of layers to the constructor and .add() method is used to add layers: \n",
    "\n",
    "EX: \n",
    "model = Sequential([  \n",
    "          Dense(32, inpuit_shape=(784,)),  \n",
    "          Activation('relu'),  \n",
    "         Dense(10),   \n",
    "         Activation('softmax'),   \n",
    "])  \n",
    "model = Sequential()  \n",
    "model.add(Dense(32, input_dim=784 ))   \n",
    "model.add(Activation('relu'))   \n",
    "\n",
    "Since the model must be aware of the input size that it is expecting, so the very first layer in the sequential model necessitates particulars about its input shape as the rest of the other layers can automatically speculate the shape. It can be done in the following ways:\n",
    "\n",
    "    The input_shape argument is passed to the foremost layer. It comprises of a tuple shape, i.e., a tuple of integers or None, such that None means that any positive integer might anticipate). It excludes the batch dimension.\n",
    "    \n",
    "    Some of the 2D layers, such as Dense, supports input shape specification through input_dim argument, whereas some of the 3D temporal layers support input_dim and input_length\n",
    "    \n",
    "    The batch_size argument is passed to the layer to define a batch size for the inputs. If batch_size=32 and input_shape=(6,8) is passed to a layer, then, in that case, it is expected for every batch of inputs will have a batch shape (32,6,8).\n",
    "\n",
    "\n",
    "At first the model is compiled for which the compile process is used for constructing the learning procedure afterward the model undergoes the training in the next step. The compilation include three parameter, which are as follows:\n",
    "\n",
    "    An optimizer: As the name suggest, an optimizer can be a string of an existing optimizer like (rmsprop or adagrad), or simply an instance of class optimizer.\n",
    "    \n",
    "    A loss function: A loss function act as an objective that every model tries to minimize for example categorical_crossentropy or mse. It is also known as objective function.\n",
    "    \n",
    "    A list of metrics: A list of metrics refers to a string of identifiers of an existing metric or custom metric function. It is suggested to set to metrics=['accuracy'] for any classification problem.\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "data=data.iloc[:,1:-1]\n",
    "data2=data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data and remove exeptional values\n",
    "from scipy.stats import zscore\n",
    "z0 = data2.apply(zscore)\n",
    "z = np.abs(zscore(data2))\n",
    "z_in = (np.abs(zscore(data2)) < 3)\n",
    "data_clean = data[z_in.all(axis=1)]\n",
    "x = data_clean.iloc[:,1:]\n",
    "y = data_clean.iloc[:,0]\n",
    "x_standard = x.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the \n",
    "d = pd.get_dummies(y)\n",
    "y = d.drop('M', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting the data to test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_standard, y, test_size=0.3, random_state=1)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras encompasses a wide range of predefined layers as well as it permits you to create your own layer. It acts as a major building block while building a Keras model. In Keras, whenever each layer receives an input, it performs some computations that result in transformed information. The output of one layer is fed as input to the other layer.\n",
    "\n",
    "Keras Core layer comprises of a dense layer, which is a dot product plus bias, an activation layer that transfers a function or neuron shape, a dropout layer, which randomly at each training update, sets a fraction of input unit to zero so as to avoid the issue of overfitting, a lambda layer that wraps an arbitrary expression just like an object of a Layer, etc.\n",
    "\n",
    "The Keras convolution layer utilizes filters for the creation of a feature map, runs from 1D to 3D. It includes most of the common invariants, for example, cropping and transposed convolution layer for each dimension. The 2D convolution is used for image recognition as it is inspired by the visual cortex.\n",
    "\n",
    "The downscaling layer, which is mainly known as pooling, runs from 1D to 3D. It also includes the most common variants, such as max and average pooling. The layers that are locally connected act as convolution layer, just the fact that weights remain unshared. The noise layer eradicates the issue of overfitting. The recurrent layer that includes simple, gated, LSTM, etc. are implemented in applications like language processing.\n",
    "\n",
    "Following are the number of common methods that each Keras layer have:\n",
    "\n",
    "    get_weights(): It yields the layer's weights as a numpy arrays list.\n",
    "    \n",
    "    set_weights(weights): It sets the layer's weight with the similar shapes as that of the output of get_weights() from numpy arrays list.\n",
    "    \n",
    "    get_config(): It returns a dictionary that includes the layer's configuration, so as to instantiate from its config through;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "def baseline_model(units1, units2, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, input_shape=(30,), activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dense(units2, activation='relu', activity_regularizer=l2(0.01), kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the training if arriving to good results\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are incorporated for training the model and so it make use of fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units1 = 128\n",
    "units2 = 128\n",
    "dropout = 0.25\n",
    " \n",
    "# Fit the model\n",
    "model = baseline_model(units1, units2, dropout)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=32, \n",
    "                    epochs=50, \n",
    "                    validation_data=(x_val,y_val), \n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_train, y_train)\n",
    "loss, acc = model.evaluate(x_val, y_val)\n",
    "loss, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.array([0 if n <= .5 else 1 for n in y_pred])\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('The accuracy is: %.2f' % acc)\n",
    "print('f1 score : %.2f '% f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking for the best hyperparameters\n",
    "in order to understand what hyperparameters will give us best result we ran 3 loops - a loop for each parameter.\n",
    "this process take a long time- but give us a better understanding of how each parameter inflouence thwe results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   min_delta=0,\n",
    "                   patience=1,\n",
    "                   verbose=1,\n",
    "                   mode='auto')\n",
    "\n",
    " # create model\n",
    "    \n",
    "def baseline_model(units1, units2, dropout):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(units1, input_shape=(30,), activation='relu'))\n",
    "  model.add(Dropout(dropout))\n",
    "  model.add(BatchNormalization(axis=1))\n",
    "  model.add(Dense(units2, activation='relu', activity_regularizer=l2(0.01), kernel_regularizer=l2(0.01)))\n",
    "  model.add(Dropout(dropout))\n",
    "  model.add(BatchNormalization(axis=1))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "  model.summary()\n",
    " \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summ = pd.DataFrame(columns=['unit1','unit2','dropout','val_loss','val_acc'])\n",
    "summary=[]\n",
    "\n",
    "for un1 in np.arange(0, 301, 50):\n",
    "    for un2 in np.arange(0, 301, 50):\n",
    "        for dr in np.arange(0.1, 0.5, 0.1):\n",
    "            print('---------------------------------------------------------')\n",
    "            print('---------------------------------------------------------')\n",
    "            print('---------------------------------------------------------')\n",
    "\n",
    "           # Name the test model\n",
    "        \n",
    "            name = 'unit1=_{}_unit2=_{}_dropout={}'.format(un1,un2,dr)\n",
    "            print(name)\n",
    "\n",
    "           # build the model\n",
    "            model = baseline_model(un1,un2,dr)\n",
    "        \n",
    "           # Fit the model\n",
    "            history = model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "                     epochs=100, batch_size=32, verbose=1, callbacks=[es])\n",
    "        \n",
    "           # Plot the history\n",
    "           #plot_history(history,name,'acc')\n",
    "        \n",
    "           #plot_history(history,name,'loss')\n",
    "       \n",
    "           # Print the summary values\n",
    "            val_loss, val_acc = model.evaluate(x_test,y_test)\n",
    "            \n",
    "            line=pd.DataFrame({'unit1':[un1],'unit2':[un2],'dropout':[dr],'val_loss':[val_loss],'val_acc':[val_acc]})                  \n",
    "            summ = summ.append(line,ignore_index=True)\n",
    "        \n",
    "            summary.append('Summary: val_loss: {}, val_acc: {}'.format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summ is a dataFrame with the accuracy and loss values for any value of unit1,unit2 and dropout\n",
    "let's look at the maximum value of val_acc and the minimum value of val_loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ['val_acc'].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That mean the max value is in line 34 let look at it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ.iloc[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ['val_loss'].idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the minimum of val_loss is in line 35 lets see it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ.iloc[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we found out that we get best result with unit1=unit2=50  and dropout between 0.3 to 0.4\n",
    "lets plot it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.lineplot(x=\"unit1\", y=\"val_loss\", hue=\"dropout\" ,data=summ[summ['unit2']==summ['unit1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = sns.lineplot(x=\"unit1\", y=\"val_acc\", hue=\"dropout\" ,data=summ[summ['unit2']==summ['unit1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we found above we can see in the plot that the best values are around 50 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yuval's Tenserflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "X = data_standard.drop(['diagnosis'],axis=1) \n",
    "y = data_standard['diagnosis']\n",
    "X_standard = X.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standard.describe()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_standard, y, test_size=0.3, random_state=1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "#units = (30+1)/2\n",
    "# result is binary - 1\n",
    "#X_train shape is (6000,11) \n",
    "classifier.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(x_train, y_train, batch_size = 32, epochs = 100)\n",
    "\n",
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "#y_pred = classifier.predict(X_test)\n",
    "\n",
    "#y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns\n",
    "y_data = data[cols[1]] #diagnosis\n",
    "le = LabelEncoder()\n",
    "y_data = np.array(le.fit_transform(y_data))\n",
    "y_data[:5], le.classes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = Variable(torch.from_numpy(x_data.values))\n",
    "y_data = Variable(torch.from_numpy(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(30,16)\n",
    "        self.l2 = torch.nn.Linear(16,4)\n",
    "        self.l3 = torch.nn.Linear(4,1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x)) \n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "y_pred = model(x_data.float())\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.float().size(), y_data.float().size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epochs in range(500):\n",
    "    y_pred = model(x_data.float())\n",
    "    loss = criterion(y_pred, y_data.view(-1,1).float())\n",
    "    print('Epoch',epochs,'Loss:',loss.item(), '- Pred:', y_pred.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#The error function is the function representing the difference between the values \n",
    "#computed by your model and the real values. In the optimization field often they\n",
    "#speak about two phases: a training phase in which the model is set,\n",
    "#and a test phase in which the model tests its behaviour against the real values of output.\n",
    "#In the training phase the error is necessary to improve the model, \n",
    "#while in the test phase the error is useful to check if the model works properly.\n",
    "\n",
    "#The objective function is the function you want to maximize or minimize. \n",
    "#When they call it \"cost function\" (again, it's the objective function)\n",
    "#it's because they want to only minimize it. \n",
    "#I see the cost function and the objective function as the same thing seen from slightly \n",
    "#different perspectives.\n",
    "\n",
    "#The \"criterion\" is usually the rule for stopping the algorithm you're using.\n",
    "#Suppose you want that your model find the minimum of an objective function,\n",
    "#in real experiences it is often hard to find the exact minimum and the algorithm \n",
    "#could continuing to work for a very long time. In that case you could accept to \n",
    "#stop it \"near\" to the optimum with a particular stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.data[0]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.double().forward(x_data.data[25]) > 0.5, y_data[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.double().forward(x_data.data[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.data[25]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.double().forward(x_data.data[55]) > 0.5, y_data[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.double().forward(x_data) > 0.5\n",
    "pred.numpy()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pred.numpy()\n",
    "b = y_data.numpy()\n",
    "pred.numpy().reshape(-1).shape, y_data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "c = confusion_matrix(a,b)\n",
    "sns.heatmap(c, annot=True, xticklabels=le.classes_, yticklabels=le.classes_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis']=np.where(data['diagnosis']=='M',1,0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:, 2:-1].values\n",
    "y=data.iloc[:, 1].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=torch.FloatTensor(X_train)\n",
    "X_test=torch.FloatTensor(X_test)\n",
    "y_train=torch.LongTensor(y_train)\n",
    "y_test=torch.LongTensor(y_test)\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self,input_features=30,hidden1=100,hidden2=100,out_features=2):\n",
    "        super().__init__()\n",
    "        self.f_connected1=nn.Linear(input_features,hidden1)\n",
    "        self.f_connected2=nn.Linear(hidden1,hidden2)\n",
    "        self.out=nn.Linear(hidden2,out_features)\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.f_connected1(x))\n",
    "        x=F.relu(self.f_connected2(x))\n",
    "        x=self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(20)\n",
    "model=ANN_Model()\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1000\n",
    "final_losses=[]\n",
    "for i in range(epochs):\n",
    "    i=i+1\n",
    "    y_pred=model.forward(X_train)\n",
    "    loss=loss_function(y_pred,y_train)\n",
    "    final_losses.append(loss)\n",
    "    if i%10==1:\n",
    "        print(\"Epoch number: {} and the loss : {}\".format(i,loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_losses[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(epochs),final_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_test):\n",
    "        y_pred=model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "        print(y_pred.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,predictions)\n",
    "sns.heatmap(cm, annot=True, xticklabels=le.classes_, yticklabels=le.classes_);\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(y_test,predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Neural Net from First Principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's normalized our feature vector.  We will force the mean of each column to 0, and divide by the maximum\n",
    "X_mean = np.mean(X,axis=1,keepdims=True) #Find the mean of each feature\n",
    "X_max = np.max(X,axis=1,keepdims=True) #Find the maximum of each feature\n",
    "X_normalized = (X-X_mean)/(X_max) #Normalizing our dataset by subtracting the mean and dividing by the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's split our dataset into two segments\n",
    "# 1) Training set to train our neural net\n",
    "# 2) A cross validation set to test the accuracy of our neural net\n",
    "\n",
    "#We'll take the first 380 samples for our training set\n",
    "X_train = X_normalized[:,:380]\n",
    "Y_train = Y[:,:380]\n",
    "\n",
    "#We will take the remaining 189 for our cross-validation set\n",
    "X_cv = X_normalized[:,381:]\n",
    "Y_cv = Y[:,381:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now define our sigmoid function to be used in the output layer of our neural network (L3)\n",
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now define our tanh(x) function to be used in hidden layers of our neural network (L1, L2)\n",
    "#Note that the tanh(x) function allows better centering of data than the sigmoid function.  This is why it will be used in our hidden layers.\n",
    "\n",
    "def tanh(z):\n",
    "    s = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's define our forward propogation function.\n",
    "def forward_prop(X,W1,W2,W3,b1,b2,b3):\n",
    "    \n",
    "    #First layer forward propogation\n",
    "    Z1 = np.dot(W1,X)# where W1 represents our matrix of weights in L1, and X represents our feature matrix of measures\n",
    "    A1 = tanh(Z1 + b1) #where b1 represents our intercept term for our first layer\n",
    "    #Second layer forward propogation\n",
    "    Z2 = np.dot(W2,A1) #where W2 represents our matrix of weights in L2\n",
    "    A2 = tanh(Z2 + b2) # where b2 represents our intercept term for our second layer\n",
    "    #Third layer forward propogation\n",
    "    Z3 = np.dot(W3,A2) #where W3 represents our matrix of weights in L3\n",
    "    #where b3 represents our intercept term for our second layer\n",
    "    A3 = sigmoid(Z3 + b3) #A3 will produce our probability vector\n",
    "    \n",
    "    cache = {    \n",
    "                  \"Z1\": Z1,\n",
    "                  \"A1\": A1,\n",
    "                  \"Z2\": Z2,\n",
    "                  \"A2\": A2,\n",
    "                  \"Z3\": Z3,\n",
    "                  \"A3\": A3\n",
    "            }\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will perform gradient descent for our neural network in the following steps:\n",
    "#1) Start by randomly initializing our weight and intercept parameters\n",
    "#2) Run forward propogation through our neural network\n",
    "#3) Calculate the derivatives of our weights and intercept parameters via back propogation\n",
    "#4) Refine our parameters using derivatives from (3)\n",
    "#5) Reiterate 1 - 4 \n",
    "\n",
    "def gradient_descent(iterations,X,Y,alpha):\n",
    "    \n",
    "    #Randomly initialized our parameters before running the algorithm\n",
    "    W1 = np.random.randn(3,30)*0.01\n",
    "    b1 = np.random.rand(3,1)\n",
    "    W2 = np.random.randn(2,3)*0.01\n",
    "    b2 = np.random.rand(2,1)\n",
    "    W3 = np.random.rand(1,2)*0.01\n",
    "    b3 = np.random.rand(1,1)\n",
    "    dummy,m = X.shape\n",
    "    \n",
    "    caches = [] #we will store our cost at each iteration in this array\n",
    "    count_vector = [] #We will store our iteration count in this array\n",
    "    count = 0\n",
    "    \n",
    "    for i in range (1,iterations):\n",
    "        \n",
    "            count = count + 1\n",
    "            \n",
    "            count_vector.append(count)\n",
    "        \n",
    "            params = forward_prop(X,W1,W2,W3,b1,b2,b3) #forward propogation using our parameters\n",
    "            \n",
    "            #Define our values to be used in back propogation using the dictionary of values created from running forward_prop\n",
    "            Z1 = params['Z1']\n",
    "            Z2 = params['Z2']\n",
    "            Z3 = params['Z3']\n",
    "            A1 = params['A1']\n",
    "            A2 = params['A2']\n",
    "            A3 = params['A3']\n",
    "            \n",
    "            #Define our cost function, append the cost of each iteration to caches\n",
    "            cost = -(1 / m)*np.sum(np.multiply(Y,np.log(A3)) + np.multiply((1-Y),np.log(1-A3)))\n",
    "            caches.append(cost)\n",
    "            \n",
    "            #Back propogation for layer 3\n",
    "            dA3 = -Y/A3 + (1-Y)/(1-A3)\n",
    "            dZ3 = dA3 * sigmoid(Z3)*(1-sigmoid(Z3))\n",
    "            dW3 = (1 / m)*np.dot(dZ3,A2.T)\n",
    "            db3 = (1 / m)*np.sum(dZ3,axis=1,keepdims=True)\n",
    "            \n",
    "            #Back propogation for layer 2\n",
    "            dA2 = np.dot(W3.T,dZ3)\n",
    "            dZ2 = dA2*(1-np.power(tanh(Z2),2))\n",
    "            dW2 = (1 / m)*np.dot(dZ2,A1.T)\n",
    "            db2 = (1 / m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "            \n",
    "            #Back propogation for layer 1\n",
    "            dA1 = np.dot(W2.T,dZ2)\n",
    "            dZ1 = dA1*(1-np.power(tanh(Z1),2))\n",
    "            dW1 = (1 / m)*np.dot(dZ1,X.T)\n",
    "            db1 = (1 / m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "            \n",
    "            #Redefine our weight parameters using the derivatives calculated in back propogation\n",
    "            W1 = W1 - alpha*dW1\n",
    "            W2 = W2 - alpha*dW2\n",
    "            W3 = W3 - alpha*dW3\n",
    "            \n",
    "            #Redefine our weight parameters using the derivatives calculated in back propogation\n",
    "            b1 = b1 - alpha*db1\n",
    "            b2 = b2 - alpha*db2\n",
    "            b3 = b3 - alpha*db3\n",
    "        \n",
    "    return W1,W2,W3,b1,b2,b3,count_vector,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see if our algorithm is working.  We should see a declining learning curve with iteration, which eventually flatterns out\n",
    "#This will help us determine the appropriate number of iterations to run to determine the appropriate parameters\n",
    "#Note: we will use a learning rate of 0.5 for now\n",
    "\n",
    "W1,W2,W3,b1,b2,b3,count,caches = gradient_descent(1000,X_cv,Y_cv,0.5)\n",
    "\n",
    "plt.plot(count,caches,label='Cost')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "\n",
    "plt.title(\"Cost vs. Iteration\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,Y,iterations,alpha,X_train,Y_train):\n",
    "\n",
    "    W1,W2,W3,b1,b2,b3,count,caches = gradient_descent(iterations,X_train,Y_train,alpha)\n",
    "    \n",
    "    Z1 = np.dot(W1,X)\n",
    "    A1 = tanh(Z1 + b1)\n",
    "    Z2 = np.dot(W2,A1)\n",
    "    A2 = tanh(Z2 + b2)\n",
    "    Z3 = np.dot(W3,A2)\n",
    "    A3 = sigmoid(Z3 + b3)\n",
    "    \n",
    "    dummy,m = A3.shape\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        Y_prediction[0, i] = 1 if A3[0, i] > 0.5 else 0\n",
    "        \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see how accurate the predictions made by our neural network are compared to the training set and cross validation set\n",
    "print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(predict(X_train,Y_train,1000,0.5,X_train,Y_train) - Y_train)) * 100))\n",
    "print(\"Cross validation accuracy: {} %\".format(100 - np.mean(np.abs(predict(X_cv,Y_cv,1000,0.5,X_train,Y_train) - Y_cv)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy,m1 = X_train.shape\n",
    "dummy,m2 = X_cv.shape\n",
    "\n",
    "train_predict = predict(X_train,Y_train,1000,0.5,X_train,Y_train)\n",
    "CV_predict = predict(X_cv,Y_cv,1000,0.5,X_train,Y_train)\n",
    "count_true_pos = 0\n",
    "count_train_pos = 0\n",
    "\n",
    "count_true_pos_cv = 0\n",
    "count_cv_pos = 0\n",
    "\n",
    "for i in range (1,m1):\n",
    "    if train_predict[0,i] == 1 and Y_train[0,i] == 1:\n",
    "        count_true_pos = count_true_pos + 1\n",
    "    if Y_train[0,i] == 1:\n",
    "        count_train_pos = count_train_pos + 1\n",
    "        \n",
    "for i in range (1,m2):\n",
    "    if CV_predict[0,i] == 1 and Y_cv[0,i] == 1:\n",
    "        count_true_pos_cv = count_true_pos_cv + 1\n",
    "    if Y_cv[0,i] == 1:\n",
    "        count_cv_pos = count_cv_pos + 1\n",
    "        \n",
    "print(str(count_true_pos) + \" positives predicted on the training set\")\n",
    "print(str(count_train_pos) + \" true positives are in the training set\")\n",
    "print(\"The accuracy of true positives on the training set is: {} %\".format(100-np.abs(100*((count_true_pos - count_train_pos)/count_train_pos))))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(str(count_true_pos_cv) + \" positives predicted on the cross validation set\")\n",
    "print(str(count_cv_pos) + \" true positives are in the cross validation set\")\n",
    "print(\"The accuracy of true positives on the cross validation set is: {} %\".format(100-np.abs(100*((count_true_pos_cv - count_cv_pos)/count_true_pos_cv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_true_neg = 0\n",
    "count_train_neg = 0\n",
    "\n",
    "count_true_neg_cv = 0\n",
    "count_cv_neg = 0\n",
    "\n",
    "for i in range (1,m1):\n",
    "    if train_predict[0,i] == 0 and Y_train[0,i] == 0:\n",
    "        count_true_neg = count_true_neg + 1\n",
    "    if Y_train[0,i] == 0:\n",
    "        count_train_neg = count_train_neg + 1\n",
    "        \n",
    "for i in range (1,m2):\n",
    "    if CV_predict[0,i] == 0 and Y_cv[0,i] == 0:\n",
    "        count_true_neg_cv = count_true_neg_cv + 1\n",
    "    if Y_cv[0,i] == 0:\n",
    "        count_cv_neg = count_cv_neg + 1\n",
    "        \n",
    "print(str(count_true_neg) + \" negatives predicted on the training set\")\n",
    "print(str(count_train_neg) + \" true negatives are in the training set\")\n",
    "print(\"The accuracy of true negatives on the training set is: {} %\".format(100-np.abs(100*((count_true_neg - count_train_neg)/count_train_neg))))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(str(count_true_neg_cv) + \" negatives predicted on the cross validation set\")\n",
    "print(str(count_cv_neg) + \" true negatives are in the cross validation set\")\n",
    "print(\"The accuracy of true negatives on the cross validation set is: {} %\".format(100-np.abs(100*((count_true_neg_cv - count_cv_neg)/count_true_neg_cv))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
