{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Predicting if the cancer diagnosis is benign or malignant based on several observations/features \n",
    "\n",
    "\n",
    "- 30 features are used, examples:\n",
    "        - radius (mean of distances from center to points on the perimeter)\n",
    "        - texture (standard deviation of gray-scale values)\n",
    "        - perimeter\n",
    "        - area\n",
    "        - smoothness (local variation in radius lengths)\n",
    "        - compactness (perimeter^2 / area - 1.0)\n",
    "        - concavity (severity of concave portions of the contour)\n",
    "        - concave points (number of concave portions of the contour)\n",
    "        - symmetry \n",
    "        - fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "- Datasets are linearly separable using all 30 input features\n",
    "- Number of Instances: 569\n",
    "- Class Distribution: 212 Malignant, 357 Benign\n",
    "- Target class:\n",
    "         - Malignant\n",
    "         - Benign\n",
    "         \n",
    "         \n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQjuiDaPbVi3jtsh-uepYaqpN-iX3SuDAmiIg&usqp=CAU=\" width=900 height=900 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd  # Import Pandas for data manipulation using dataframes\n",
    "import numpy as np  # Import Numpy for data statistical analysis\n",
    "import matplotlib.pyplot as plt  # Import matplotlib for data visualisation\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "# %matplotlib inline\n",
    "\n",
    "# Import Cancer data drom the Sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BreastCancerDetection.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove id and Unnamed: 32\n",
    "data=data.iloc[:,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"diagnosis\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data['diagnosis'], label = \"Count\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue = 'diagnosis', vars = ['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'fractal_dimension_mean',] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue = 'diagnosis', vars = [\n",
    "       'area_mean', 'smoothness_mean', ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'area_mean', y = 'smoothness_mean', hue = 'diagnosis', data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the correlation between the variables \n",
    "# Strong correlation between the mean radius and mean perimeter, mean area and mean primeter\n",
    "plt.figure(figsize=(20,10)) \n",
    "sns.heatmap(data.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 12\n",
    "features_selection = ['radius_mean', 'perimeter_mean', 'area_mean', 'concavity_mean', 'concave points_mean']\n",
    "features_mean= list(data.columns[1:11])\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for i, feature in enumerate(features_mean):\n",
    "    rows = int(len(features_mean)/2)\n",
    "    \n",
    "    plt.subplot(rows, 2, i+1)\n",
    "    \n",
    "    sns.distplot(data[data['diagnosis']=='M'][feature], bins=bins, color='red', label='M');\n",
    "    sns.distplot(data[data['diagnosis']=='B'][feature], bins=bins, color='blue', label='B');\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i, feature in enumerate(features_mean):\n",
    "    rows = int(len(features_mean)/2)\n",
    "    \n",
    "    plt.subplot(rows, 2, i+1)\n",
    "    \n",
    "    sns.boxplot(x='diagnosis', y=feature, data=data, palette=\"Set1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for exceptions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "z = np.abs(zscore(data.iloc[:,1:]))\n",
    "\n",
    "X = data.drop(['diagnosis'],axis=1)\n",
    "y = data['diagnosis']\n",
    "\n",
    "X_standard = X.apply(zscore)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = svc_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)\n",
    "y_predict = svc_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "sns.heatmap(cm, annot=True)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=4)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "cm = confusion_matrix(y_test, grid_predictions)\n",
    "sns.heatmap(cm, annot=True)\n",
    "print(classification_report(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_area_mean = data['area_mean']\n",
    "X_smoothness_mean=data[\"smoothness_mean\"]\n",
    "X_radius_mean= data[\"radius_mean\"]\n",
    "y = data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train,y_train)\n",
    "feature_importances = pd.DataFrame(decision_tree.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_predict_train = decision_tree.predict(X_train)\n",
    "y_predict_train\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_predict_test = decision_tree.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier(n_estimators=150)\n",
    "RandomForest.fit(X_train, y_train)\n",
    "\n",
    "y_predict_train = RandomForest.predict(X_train)\n",
    "y_predict_train\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_predict_test = RandomForest.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})\n",
    "y = data['diagnosis']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size=0.2)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "NB_classifier = GaussianNB()\n",
    "NB_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = NB_classifier.predict(X_train)\n",
    "y_predict_train\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_predict_test = NB_classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = classifier.predict(X_train)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_test = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network multi layer perspetron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "try_model = MLPClassifier()\n",
    "\n",
    "# Fit model\n",
    "try_model.fit(X_train, y_train)\n",
    "\n",
    "#check prediction\n",
    "y_predict = try_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bernuli&linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "logistic = linear_model.LogisticRegression(solver='newton-cg', tol=1)\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "rbm_features_classifier = Pipeline(\n",
    "    steps=[('rbm', rbm), ('logistic', logistic)])\n",
    "\n",
    "# Hyper-parameters. These were set by cross-validation,\n",
    "# using a GridSearchCV. Here we are not performing cross-validation to\n",
    "# save time.\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 10\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 100\n",
    "logistic.C = 6000\n",
    "\n",
    "# Fit model\n",
    "rbm_features_classifier.fit(X_train, y_train)\n",
    "\n",
    "#check prediction\n",
    "y_predict = rbm_features_classifier.predict(X_test)\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, y_predict)))\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "#accuracy and f1\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "acc = accuracy_score(y_test, y_predict)\n",
    "f1 = f1_score(y_test, y_predict, average='macro')\n",
    "f'The accuracy is %.2f     The f1_score is %.2f' % (acc,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Decent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "accuracy_selection=[]\n",
    "cvs_selection=[]\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "accuracy_selection.append(accuracy_score(prediction, y_test))\n",
    "cvs_selection.append(np.mean(scores))\n",
    "\n",
    "print(\"SGD Classifier Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\n",
    "print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\n",
    "print(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISING PCA AND TENSERFLOW PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the diagnosis column to numeric format\n",
    "# data['diagnosis'] = data['diagnosis'].factorize()[0]\n",
    "# # Fill all Null values with zero\n",
    "# data = data.fillna(value=0)\n",
    "# # Store the diagnosis column in a target object and then drop it\n",
    "# target = data['diagnosis']\n",
    "# data = data.drop('diagnosis', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # Principal Component Analysis module\n",
    "from sklearn.manifold import TSNE # TSNE module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dataframe into arrays\n",
    "X = data.values\n",
    "\n",
    "# Invoke the PCA method. Since this is a binary classification problem\n",
    "# let's call n_components = 2\n",
    "pca = PCA(n_components=2)\n",
    "pca_2d = pca.fit_transform(X)\n",
    "\n",
    "# Invoke the TSNE method\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=2000)\n",
    "tsne_results = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the TSNE and PCA visuals side-by-side\n",
    "plt.figure(figsize = (16,11))\n",
    "plt.subplot(121)\n",
    "plt.scatter(pca_2d[:,0],pca_2d[:,1], c = target, \n",
    "            cmap = \"coolwarm\", edgecolor = \"None\", alpha=0.35)\n",
    "plt.colorbar()\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.subplot(122)\n",
    "plt.scatter(tsne_results[:,0],tsne_results[:,1],  c = target, \n",
    "            cmap = \"coolwarm\", edgecolor = \"None\", alpha=0.35)\n",
    "plt.colorbar()\n",
    "plt.title('TSNE Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors\n",
    "\n",
    "# z = np.exp(-X1**2-X2**2)*1.5+0.2\n",
    "\n",
    "# colors = [\"red\", \"blue\"]\n",
    "# cmap= matplotlib.colors.ListedColormap(colors)\n",
    "# boundaries=[z.min(), 0.35, 0.7, 1.05, 1.4, z.max()]\n",
    "\n",
    "# m = plt.contourf(X1, X2, z, levels = boundaries, cmap=cmap)\n",
    "\n",
    "# plt.colorbar(m, spacing=\"proportional\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), cmap = ListedColormap((\"red\",\"blue\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras and Tenserflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standard = X.apply(zscore)\n",
    "x_standard.describe()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_standard, y, test_size=0.3, random_state=1)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "def baseline_model(units1, units2, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, input_shape=(30,), activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dense(units2, activation='relu', activity_regularizer=l2(0.01), kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the training if arriving to good results\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units1 = 128\n",
    "units2 = 128\n",
    "dropout = 0.25\n",
    " \n",
    "# Fit the model\n",
    "model = baseline_model(units1, units2, dropout)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=32, \n",
    "                    epochs=50, \n",
    "                    validation_data=(x_val,y_val), \n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_train, y_train)\n",
    "loss, acc = model.evaluate(x_val, y_val)\n",
    "loss, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.array([0 if n <= .5 else 1 for n in y_pred])\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('The accuracy is: %.2f' % acc)\n",
    "print('f1 score : %.2f '% f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yuval's Tenserflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standard = X.apply(zscore)\n",
    "x_standard.describe()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_standard, y, test_size=0.3, random_state=1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "#units = (30+1)/2\n",
    "# result is binary - 1\n",
    "#X_train shape is (6000,11) \n",
    "classifier.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(x_train, y_train, batch_size = 32, epochs = 100)\n",
    "\n",
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "#y_pred = classifier.predict(X_test)\n",
    "\n",
    "#y_pred = (y_pred > 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop - Get best paramiters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #def baseline_model(units1, units2, dropout):\n",
    "\n",
    "# best_i=1\n",
    "# best_j=1\n",
    "# best_d=1\n",
    "# best_loss=1.0\n",
    "# best_acc=0.0\n",
    "\n",
    "# best_batch_size=1\n",
    "# best_epochs=1\n",
    "\n",
    "# for i in range(1,128):\n",
    "#     for j in range(1,128):\n",
    "#         for d in range(1,100,5):\n",
    "#             model= baseline_model(i, j, d/100)\n",
    "#             history = model.fit(x_train, y_train, \n",
    "#                     batch_size=32, \n",
    "#                     epochs=50, \n",
    "#                     validation_data=(x_val,y_val), \n",
    "#                     callbacks=[es])\n",
    "            \n",
    "# #            \n",
    "#             loss, acc = model.evaluate(x_test, y_test)\n",
    "#             print(\"the best yet are:\")\n",
    "#             print(\"best i %d the best j is %d the best d is %d \" %(best_i,best_j,best_d) )\n",
    "#             print('The best accuracy is: %.2f loss is  %.2f ' %(acc,loss)) \n",
    "            \n",
    "#             if(best_loss>loss and best_acc<acc):\n",
    "#                 best_loss=loss \n",
    "#                 best_acc=acc\n",
    "#                 best_i=i\n",
    "#                 best_j=j\n",
    "#                 best_d=d\n",
    "#                 print(\"-----------------------------------------------new score\")\n",
    "#                 print('The accuracy is: %.2f loss is  %.2f ' %(acc,loss)) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(i,j,d)\n",
    "# print(\"best results\", \n",
    "# best_i,\n",
    "# best_j,\n",
    "# best_d,\n",
    "# best_loss,\n",
    "# best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing exceptional data and checking again the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutting first column\n",
    "data2=data.iloc[:,1:]\n",
    "chart = sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(data2))\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z0 = data2.apply(zscore)\n",
    "z = np.abs(zscore(data2))\n",
    "z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0]\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing exeptional points\n",
    "z_in = (np.abs(zscore(data2)) < 3)\n",
    "data_clean = data2[z_in.all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns\n",
    "y_data = data[cols[1]] #diagnosis\n",
    "le = LabelEncoder()\n",
    "y_data = np.array(le.fit_transform(y_data))\n",
    "y_data[:5], le.classes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = Variable(torch.from_numpy(x_data.values))\n",
    "y_data = Variable(torch.from_numpy(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(30,16)\n",
    "        self.l2 = torch.nn.Linear(16,4)\n",
    "        self.l3 = torch.nn.Linear(4,1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x)) \n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "y_pred = model(x_data.float())\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.float().size(), y_data.float().size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epochs in range(500):\n",
    "    y_pred = model(x_data.float())\n",
    "    loss = criterion(y_pred, y_data.view(-1,1).float())\n",
    "    print('Epoch',epochs,'Loss:',loss.item(), '- Pred:', y_pred.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#The error function is the function representing the difference between the values \n",
    "#computed by your model and the real values. In the optimization field often they\n",
    "#speak about two phases: a training phase in which the model is set,\n",
    "#and a test phase in which the model tests its behaviour against the real values of output.\n",
    "#In the training phase the error is necessary to improve the model, \n",
    "#while in the test phase the error is useful to check if the model works properly.\n",
    "\n",
    "#The objective function is the function you want to maximize or minimize. \n",
    "#When they call it \"cost function\" (again, it's the objective function)\n",
    "#it's because they want to only minimize it. \n",
    "#I see the cost function and the objective function as the same thing seen from slightly \n",
    "#different perspectives.\n",
    "\n",
    "#The \"criterion\" is usually the rule for stopping the algorithm you're using.\n",
    "#Suppose you want that your model find the minimum of an objective function,\n",
    "#in real experiences it is often hard to find the exact minimum and the algorithm \n",
    "#could continuing to work for a very long time. In that case you could accept to \n",
    "#stop it \"near\" to the optimum with a particular stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.data[0]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.double().forward(x_data.data[25]) > 0.5, y_data[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.double().forward(x_data.data[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.data[25]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.double().forward(x_data.data[55]) > 0.5, y_data[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.double().forward(x_data) > 0.5\n",
    "pred.numpy()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pred.numpy()\n",
    "b = y_data.numpy()\n",
    "pred.numpy().reshape(-1).shape, y_data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "c = confusion_matrix(a,b)\n",
    "sns.heatmap(c, annot=True, xticklabels=le.classes_, yticklabels=le.classes_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis']=np.where(data['diagnosis']=='M',1,0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:, 2:-1].values\n",
    "y=data.iloc[:, 1].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=torch.FloatTensor(X_train)\n",
    "X_test=torch.FloatTensor(X_test)\n",
    "y_train=torch.LongTensor(y_train)\n",
    "y_test=torch.LongTensor(y_test)\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self,input_features=30,hidden1=100,hidden2=100,out_features=2):\n",
    "        super().__init__()\n",
    "        self.f_connected1=nn.Linear(input_features,hidden1)\n",
    "        self.f_connected2=nn.Linear(hidden1,hidden2)\n",
    "        self.out=nn.Linear(hidden2,out_features)\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.f_connected1(x))\n",
    "        x=F.relu(self.f_connected2(x))\n",
    "        x=self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(20)\n",
    "model=ANN_Model()\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1000\n",
    "final_losses=[]\n",
    "for i in range(epochs):\n",
    "    i=i+1\n",
    "    y_pred=model.forward(X_train)\n",
    "    loss=loss_function(y_pred,y_train)\n",
    "    final_losses.append(loss)\n",
    "    if i%10==1:\n",
    "        print(\"Epoch number: {} and the loss : {}\".format(i,loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_losses[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(epochs),final_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_test):\n",
    "        y_pred=model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "        print(y_pred.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,predictions)\n",
    "sns.heatmap(cm, annot=True, xticklabels=le.classes_, yticklabels=le.classes_);\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(y_test,predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('soldgame': conda)",
   "language": "python",
   "name": "python37764bitsoldgameconda7262750cf6184ac8bf201f1ff27a4c4f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
