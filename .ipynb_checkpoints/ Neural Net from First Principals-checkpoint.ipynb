{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "dataset = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "dataset.shape #Let's take a look at the dimensions of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'M':1,'B':0}\n",
    "dataset = dataset.replace(d) #replace M/B with 0 or 1 for the neural net classification\n",
    "dataset = dataset.drop(['Unnamed: 32'],axis=1) #remove column 32 - unknown purpose\n",
    "dataset = dataset.drop(['id'],axis=1) #remove the id - not needed for the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_temp = dataset.drop(['diagnosis'],axis=1) #make a temporary dataset with only our feature vectors\n",
    "X = np.array(dataset_temp).T #create our Numpy array of feature vectors to be used in our neural net\n",
    "Y = np.array(dataset['diagnosis']).T #create our Numpy array of diagnosis to be used in our neural net\n",
    "Y = Y.reshape(1,569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's normalized our feature vector.  We will force the mean of each column to 0, and divide by the maximum\n",
    "X_mean = np.mean(X,axis=1,keepdims=True) #Find the mean of each feature\n",
    "X_max = np.max(X,axis=1,keepdims=True) #Find the maximum of each feature\n",
    "X_normalized = (X-X_mean)/(X_max) #Normalizing our dataset by subtracting the mean and dividing by the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's split our dataset into two segments\n",
    "# 1) Training set to train our neural net\n",
    "# 2) A cross validation set to test the accuracy of our neural net\n",
    "\n",
    "#We'll take the first 380 samples for our training set\n",
    "X_train = X_normalized[:,:380]\n",
    "Y_train = Y[:,:380]\n",
    "\n",
    "#We will take the remaining 189 for our cross-validation set\n",
    "X_cv = X_normalized[:,381:]\n",
    "Y_cv = Y[:,381:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now define our sigmoid function to be used in the output layer of our neural network (L3)\n",
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now define our tanh(x) function to be used in hidden layers of our neural network (L1, L2)\n",
    "#Note that the tanh(x) function allows better centering of data than the sigmoid function.  This is why it will be used in our hidden layers.\n",
    "\n",
    "def tanh(z):\n",
    "    s = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's define our forward propogation function.\n",
    "def forward_prop(X,W1,W2,W3,b1,b2,b3):\n",
    "    \n",
    "    #First layer forward propogation\n",
    "    Z1 = np.dot(W1,X)# where W1 represents our matrix of weights in L1, and X represents our feature matrix of measures\n",
    "    A1 = tanh(Z1 + b1) #where b1 represents our intercept term for our first layer\n",
    "    #Second layer forward propogation\n",
    "    Z2 = np.dot(W2,A1) #where W2 represents our matrix of weights in L2\n",
    "    A2 = tanh(Z2 + b2) # where b2 represents our intercept term for our second layer\n",
    "    #Third layer forward propogation\n",
    "    Z3 = np.dot(W3,A2) #where W3 represents our matrix of weights in L3\n",
    "    #where b3 represents our intercept term for our second layer\n",
    "    A3 = sigmoid(Z3 + b3) #A3 will produce our probability vector\n",
    "    \n",
    "    cache = {    \n",
    "                  \"Z1\": Z1,\n",
    "                  \"A1\": A1,\n",
    "                  \"Z2\": Z2,\n",
    "                  \"A2\": A2,\n",
    "                  \"Z3\": Z3,\n",
    "                  \"A3\": A3\n",
    "            }\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will perform gradient descent for our neural network in the following steps:\n",
    "#1) Start by randomly initializing our weight and intercept parameters\n",
    "#2) Run forward propogation through our neural network\n",
    "#3) Calculate the derivatives of our weights and intercept parameters via back propogation\n",
    "#4) Refine our parameters using derivatives from (3)\n",
    "#5) Reiterate 1 - 4 \n",
    "\n",
    "def gradient_descent(iterations,X,Y,alpha):\n",
    "    \n",
    "    #Randomly initialized our parameters before running the algorithm\n",
    "    W1 = np.random.randn(3,30)*0.01\n",
    "    b1 = np.random.rand(3,1)\n",
    "    W2 = np.random.randn(2,3)*0.01\n",
    "    b2 = np.random.rand(2,1)\n",
    "    W3 = np.random.rand(1,2)*0.01\n",
    "    b3 = np.random.rand(1,1)\n",
    "    dummy,m = X.shape\n",
    "    \n",
    "    caches = [] #we will store our cost at each iteration in this array\n",
    "    count_vector = [] #We will store our iteration count in this array\n",
    "    count = 0\n",
    "    \n",
    "    for i in range (1,iterations):\n",
    "        \n",
    "            count = count + 1\n",
    "            \n",
    "            count_vector.append(count)\n",
    "        \n",
    "            params = forward_prop(X,W1,W2,W3,b1,b2,b3) #forward propogation using our parameters\n",
    "            \n",
    "            #Define our values to be used in back propogation using the dictionary of values created from running forward_prop\n",
    "            Z1 = params['Z1']\n",
    "            Z2 = params['Z2']\n",
    "            Z3 = params['Z3']\n",
    "            A1 = params['A1']\n",
    "            A2 = params['A2']\n",
    "            A3 = params['A3']\n",
    "            \n",
    "            #Define our cost function, append the cost of each iteration to caches\n",
    "            cost = -(1 / m)*np.sum(np.multiply(Y,np.log(A3)) + np.multiply((1-Y),np.log(1-A3)))\n",
    "            caches.append(cost)\n",
    "            \n",
    "            #Back propogation for layer 3\n",
    "            dA3 = -Y/A3 + (1-Y)/(1-A3)\n",
    "            dZ3 = dA3 * sigmoid(Z3)*(1-sigmoid(Z3))\n",
    "            dW3 = (1 / m)*np.dot(dZ3,A2.T)\n",
    "            db3 = (1 / m)*np.sum(dZ3,axis=1,keepdims=True)\n",
    "            \n",
    "            #Back propogation for layer 2\n",
    "            dA2 = np.dot(W3.T,dZ3)\n",
    "            dZ2 = dA2*(1-np.power(tanh(Z2),2))\n",
    "            dW2 = (1 / m)*np.dot(dZ2,A1.T)\n",
    "            db2 = (1 / m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "            \n",
    "            #Back propogation for layer 1\n",
    "            dA1 = np.dot(W2.T,dZ2)\n",
    "            dZ1 = dA1*(1-np.power(tanh(Z1),2))\n",
    "            dW1 = (1 / m)*np.dot(dZ1,X.T)\n",
    "            db1 = (1 / m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "            \n",
    "            #Redefine our weight parameters using the derivatives calculated in back propogation\n",
    "            W1 = W1 - alpha*dW1\n",
    "            W2 = W2 - alpha*dW2\n",
    "            W3 = W3 - alpha*dW3\n",
    "            \n",
    "            #Redefine our weight parameters using the derivatives calculated in back propogation\n",
    "            b1 = b1 - alpha*db1\n",
    "            b2 = b2 - alpha*db2\n",
    "            b3 = b3 - alpha*db3\n",
    "        \n",
    "    return W1,W2,W3,b1,b2,b3,count_vector,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets see if our algorithm is working.  We should see a declining learning curve with iteration, which eventually flatterns out\n",
    "#This will help us determine the appropriate number of iterations to run to determine the appropriate parameters\n",
    "#Note: we will use a learning rate of 0.5 for now\n",
    "\n",
    "W1,W2,W3,b1,b2,b3,count,caches = gradient_descent(1000,X_cv,Y_cv,0.5)\n",
    "\n",
    "plt.plot(count,caches,label='Cost')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "\n",
    "plt.title(\"Cost vs. Iteration\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7224863415857364,\n",
       " 0.6510908469371014,\n",
       " 0.6078464858720865,\n",
       " 0.5806307136699339,\n",
       " 0.5634991321800644,\n",
       " 0.5528864199367188,\n",
       " 0.5464530278981559,\n",
       " 0.5426367896629996,\n",
       " 0.5404142769618747,\n",
       " 0.5391363272954907,\n",
       " 0.5384048534047786,\n",
       " 0.5379829854552256,\n",
       " 0.5377332317745177,\n",
       " 0.5375772994563184,\n",
       " 0.5374712102896024,\n",
       " 0.5373904570385094,\n",
       " 0.5373213976725827,\n",
       " 0.5372563708854339,\n",
       " 0.5371909664128381,\n",
       " 0.5371225161415735,\n",
       " 0.5370492658322298,\n",
       " 0.5369699217705348,\n",
       " 0.5368834020269008,\n",
       " 0.5367886984629732,\n",
       " 0.5366847981434784,\n",
       " 0.5365706361895068,\n",
       " 0.5364450648507971,\n",
       " 0.5363068304658584,\n",
       " 0.5361545536735023,\n",
       " 0.5359867101926468,\n",
       " 0.535801610490216,\n",
       " 0.5355973771360567,\n",
       " 0.5353719188264109,\n",
       " 0.5351229000625641,\n",
       " 0.5348477053582815,\n",
       " 0.5345433966450501,\n",
       " 0.5342066622554749,\n",
       " 0.5338337554882672,\n",
       " 0.5334204202809913,\n",
       " 0.5329618009206791,\n",
       " 0.5324523319836831,\n",
       " 0.5318856037858076,\n",
       " 0.5312541975085444,\n",
       " 0.5305494828113332,\n",
       " 0.5297613691083138,\n",
       " 0.528877999754687,\n",
       " 0.527885376148078,\n",
       " 0.5267668962459469,\n",
       " 0.5255027893600077,\n",
       " 0.5240694265975765,\n",
       " 0.5224384845294924,\n",
       " 0.5205759395703713,\n",
       " 0.5184408738727184,\n",
       " 0.5159840831021233,\n",
       " 0.5131464968204417,\n",
       " 0.5098574603313252,\n",
       " 0.5060329928385139,\n",
       " 0.5015742439608527,\n",
       " 0.49636653376252066,\n",
       " 0.49027959029721074,\n",
       " 0.4831698835843872,\n",
       " 0.47488624080780395,\n",
       " 0.4652800785112677,\n",
       " 0.45422136002967367,\n",
       " 0.4416204528145247,\n",
       " 0.42745415093941747,\n",
       " 0.41179134100517656,\n",
       " 0.39481099285347215,\n",
       " 0.37680412087168924,\n",
       " 0.3581540265266622,\n",
       " 0.33929585370203263,\n",
       " 0.3206645856299048,\n",
       " 0.30264541653279453,\n",
       " 0.28553887478812184,\n",
       " 0.2695463739491207,\n",
       " 0.2547743142734173,\n",
       " 0.2412502132710406,\n",
       " 0.22894359180257973,\n",
       " 0.21778617343283116,\n",
       " 0.2076885063004672,\n",
       " 0.19855217590967236,\n",
       " 0.1902779785224522,\n",
       " 0.18277090946201655,\n",
       " 0.17594287336239606,\n",
       " 0.16971388166922496,\n",
       " 0.16401230996409222,\n",
       " 0.15877461147616906,\n",
       " 0.15394474577538106,\n",
       " 0.14947348376608333,\n",
       " 0.14531768441370785,\n",
       " 0.14143959647366908,\n",
       " 0.13780621240417226,\n",
       " 0.134388686041901,\n",
       " 0.13116181661864612,\n",
       " 0.12810359673668142,\n",
       " 0.12519481937457932,\n",
       " 0.12241873785970774,\n",
       " 0.11976077241390666,\n",
       " 0.11720825699722859,\n",
       " 0.11475022053380934,\n",
       " 0.11237719708640742,\n",
       " 0.11008106008731147,\n",
       " 0.1078548762990526,\n",
       " 0.10569277575127396,\n",
       " 0.10358983447095876,\n",
       " 0.10154196738557014,\n",
       " 0.09954582932565831,\n",
       " 0.09759872257628383,\n",
       " 0.0956985099139255,\n",
       " 0.09384353250437015,\n",
       " 0.09203253241399598,\n",
       " 0.09026457978980751,\n",
       " 0.08853900498367522,\n",
       " 0.08685533602931722,\n",
       " 0.08521324192815012,\n",
       " 0.08361248216965106,\n",
       " 0.08205286281606905,\n",
       " 0.08053419933702129,\n",
       " 0.0790562862058296,\n",
       " 0.07761887308582528,\n",
       " 0.07622164725911051,\n",
       " 0.07486422179708921,\n",
       " 0.0735461288519515,\n",
       " 0.072266817367095,\n",
       " 0.0710256544636356,\n",
       " 0.06982192975735513,\n",
       " 0.06865486189047462,\n",
       " 0.06752360661857966,\n",
       " 0.06642726586718758,\n",
       " 0.06536489725730164,\n",
       " 0.06433552368810597,\n",
       " 0.06333814265217091,\n",
       " 0.0623717350399829,\n",
       " 0.06143527326347352,\n",
       " 0.06052772859091202,\n",
       " 0.05964807763748341,\n",
       " 0.05879530799731376,\n",
       " 0.0579684230343951,\n",
       " 0.05716644587290287,\n",
       " 0.05638842264305917,\n",
       " 0.05563342504827823,\n",
       " 0.05490055232408016,\n",
       " 0.05418893266030631,\n",
       " 0.053497724156496125,\n",
       " 0.05282611537671495,\n",
       " 0.05217332556532525,\n",
       " 0.05153860457969498,\n",
       " 0.05092123259003958,\n",
       " 0.050320519590789016,\n",
       " 0.04973580476226863,\n",
       " 0.04916645571621639,\n",
       " 0.04861186765381158,\n",
       " 0.048071462460504526,\n",
       " 0.04754468775802341,\n",
       " 0.04703101593048305,\n",
       " 0.046529943138510076,\n",
       " 0.04604098833269613,\n",
       " 0.04556369227545929,\n",
       " 0.04509761657849572,\n",
       " 0.04464234276140135,\n",
       " 0.04419747133570004,\n",
       " 0.04376262091739575,\n",
       " 0.04333742737024307,\n",
       " 0.04292154298117265,\n",
       " 0.04251463566869179,\n",
       " 0.042116388224584515,\n",
       " 0.04172649758883946,\n",
       " 0.04134467415742312,\n",
       " 0.04097064112227443,\n",
       " 0.040604133842713275,\n",
       " 0.04024489924732088,\n",
       " 0.03989269526525263,\n",
       " 0.03954729028587993,\n",
       " 0.039208462645618634,\n",
       " 0.03887600014078262,\n",
       " 0.038549699565299064,\n",
       " 0.03822936627213258,\n",
       " 0.037914813757284865,\n",
       " 0.03760586326526497,\n",
       " 0.03730234341495756,\n",
       " 0.03700408984485422,\n",
       " 0.036710944876651684,\n",
       " 0.03642275719626286,\n",
       " 0.03613938155132713,\n",
       " 0.03586067846435032,\n",
       " 0.03558651396064443,\n",
       " 0.035316759310279375,\n",
       " 0.0350512907832986,\n",
       " 0.03478998941748875,\n",
       " 0.03453274079803105,\n",
       " 0.03427943484839776,\n",
       " 0.03402996563189124,\n",
       " 0.03378423116325569,\n",
       " 0.03354213322982263,\n",
       " 0.03330357722168082,\n",
       " 0.03306847197038929,\n",
       " 0.03283672959577806,\n",
       " 0.032608265360407104,\n",
       " 0.03238299753127688,\n",
       " 0.032160847248406677,\n",
       " 0.03194173839991775,\n",
       " 0.03172559750327859,\n",
       " 0.031512353592387625,\n",
       " 0.03130193811018749,\n",
       " 0.031094284806520725,\n",
       " 0.030889329640952964,\n",
       " 0.030687010690304672,\n",
       " 0.030487268060645997,\n",
       " 0.03029004380352327,\n",
       " 0.030095281836197203,\n",
       " 0.029902927865685908,\n",
       " 0.02971292931641545,\n",
       " 0.029525235261292693,\n",
       " 0.029339796356023743,\n",
       " 0.029156564776511806,\n",
       " 0.02897549415917647,\n",
       " 0.028796539544044857,\n",
       " 0.028619657320473366,\n",
       " 0.028444805175365878,\n",
       " 0.028271942043761482,\n",
       " 0.028101028061671602,\n",
       " 0.027932024521052652,\n",
       " 0.02776489382680624,\n",
       " 0.027599599455705018,\n",
       " 0.027436105917147145,\n",
       " 0.027274378715647887,\n",
       " 0.0271143843149815,\n",
       " 0.026956090103890988,\n",
       " 0.026799464363288065,\n",
       " 0.02664447623486926,\n",
       " 0.026491095691078378,\n",
       " 0.0263392935063492,\n",
       " 0.026189041229565092,\n",
       " 0.026040311157677045,\n",
       " 0.025893076310422754,\n",
       " 0.02574731040609418,\n",
       " 0.025602987838302515,\n",
       " 0.025460083653692962,\n",
       " 0.025318573530563736,\n",
       " 0.025178433758346403,\n",
       " 0.025039641217906573,\n",
       " 0.02490217336262663,\n",
       " 0.024766008200233537,\n",
       " 0.024631124275337122,\n",
       " 0.024497500652646133,\n",
       " 0.024365116900830507,\n",
       " 0.02423395307700033,\n",
       " 0.024103989711773716,\n",
       " 0.02397520779490659,\n",
       " 0.023847588761459215,\n",
       " 0.02372111447847569,\n",
       " 0.023595767232153408,\n",
       " 0.023471529715480846,\n",
       " 0.023348385016323636,\n",
       " 0.02322631660593861,\n",
       " 0.023105308327898155,\n",
       " 0.022985344387406743,\n",
       " 0.02286640934099312,\n",
       " 0.022748488086562258,\n",
       " 0.022631565853791835,\n",
       " 0.022515628194859107,\n",
       " 0.022400660975484137,\n",
       " 0.022286650366276826,\n",
       " 0.02217358283437484,\n",
       " 0.022061445135360985,\n",
       " 0.021950224305448695,\n",
       " 0.021839907653924816,\n",
       " 0.021730482755839622,\n",
       " 0.02162193744493416,\n",
       " 0.021514259806795787,\n",
       " 0.021407438172232927,\n",
       " 0.02130146111086062,\n",
       " 0.021196317424888745,\n",
       " 0.021091996143105345,\n",
       " 0.02098848651504743,\n",
       " 0.020885778005352497,\n",
       " 0.02078386028828382,\n",
       " 0.02068272324242309,\n",
       " 0.02058235694552449,\n",
       " 0.020482751669523958,\n",
       " 0.02038389787569817,\n",
       " 0.020285786209967953,\n",
       " 0.020188407498340836,\n",
       " 0.020091752742487635,\n",
       " 0.019995813115448655,\n",
       " 0.01990057995746481,\n",
       " 0.019806044771929175,\n",
       " 0.019712199221454934,\n",
       " 0.019619035124055804,\n",
       " 0.019526544449434854,\n",
       " 0.019434719315378133,\n",
       " 0.01934355198424975,\n",
       " 0.019253034859584562,\n",
       " 0.01916316048277579,\n",
       " 0.019073921529853825,\n",
       " 0.018985310808353695,\n",
       " 0.018897321254268087,\n",
       " 0.01880994592908294,\n",
       " 0.018723178016893507,\n",
       " 0.01863701082159762,\n",
       " 0.018551437764164205,\n",
       " 0.018466452379974442,\n",
       " 0.018382048316233232,\n",
       " 0.018298219329448983,\n",
       " 0.018214959282979355,\n",
       " 0.018132262144641073,\n",
       " 0.018050121984381745,\n",
       " 0.017968532972011866,\n",
       " 0.017887489374995166,\n",
       " 0.017806985556295493,\n",
       " 0.017727015972278576,\n",
       " 0.01764757517066702,\n",
       " 0.01756865778854699,\n",
       " 0.017490258550424964,\n",
       " 0.017412372266333246,\n",
       " 0.0173349938299827,\n",
       " 0.017258118216961348,\n",
       " 0.01718174048297752,\n",
       " 0.01710585576214643,\n",
       " 0.01703045926531868,\n",
       " 0.016955546278449696,\n",
       " 0.016881112161008953,\n",
       " 0.01680715234442777,\n",
       " 0.016733662330584653,\n",
       " 0.016660637690327197,\n",
       " 0.01658807406202951,\n",
       " 0.016515967150184198,\n",
       " 0.016444312724027896,\n",
       " 0.016373106616199597,\n",
       " 0.016302344721430776,\n",
       " 0.016232022995266535,\n",
       " 0.016162137452816843,\n",
       " 0.016092684167537178,\n",
       " 0.016023659270037842,\n",
       " 0.015955058946920984,\n",
       " 0.015886879439644903,\n",
       " 0.015819117043414665,\n",
       " 0.0157517681060985,\n",
       " 0.015684829027169323,\n",
       " 0.015618296256670683,\n",
       " 0.015552166294206516,\n",
       " 0.01548643568795416,\n",
       " 0.015421101033699994,\n",
       " 0.01535615897389721,\n",
       " 0.015291606196744974,\n",
       " 0.015227439435288767,\n",
       " 0.01516365546654109,\n",
       " 0.015100251110622078,\n",
       " 0.01503722322991979,\n",
       " 0.014974568728269282,\n",
       " 0.01491228455015042,\n",
       " 0.014850367679903609,\n",
       " 0.01478881514096332,\n",
       " 0.014727623995108753,\n",
       " 0.014666791341731357,\n",
       " 0.014606314317118778,\n",
       " 0.01454619009375476,\n",
       " 0.01448641587963473,\n",
       " 0.014426988917596705,\n",
       " 0.01436790648466701,\n",
       " 0.014309165891420561,\n",
       " 0.014250764481355455,\n",
       " 0.014192699630281313,\n",
       " 0.01413496874572127,\n",
       " 0.014077569266327163,\n",
       " 0.014020498661307605,\n",
       " 0.013963754429868741,\n",
       " 0.013907334100667412,\n",
       " 0.013851235231276066,\n",
       " 0.013795455407659848,\n",
       " 0.013739992243664852,\n",
       " 0.013684843380517783,\n",
       " 0.013630006486336487,\n",
       " 0.013575479255651259,\n",
       " 0.013521259408936614,\n",
       " 0.013467344692153273,\n",
       " 0.013413732876300112,\n",
       " 0.013360421756976002,\n",
       " 0.013307409153950973,\n",
       " 0.013254692910746954,\n",
       " 0.013202270894227435,\n",
       " 0.013150140994196124,\n",
       " 0.01309830112300426,\n",
       " 0.01304674921516649,\n",
       " 0.012995483226984954,\n",
       " 0.012944501136181574,\n",
       " 0.012893800941538315,\n",
       " 0.012843380662545012,\n",
       " 0.012793238339055013,\n",
       " 0.01274337203094803,\n",
       " 0.012693779817800389,\n",
       " 0.012644459798562213,\n",
       " 0.012595410091241611,\n",
       " 0.012546628832595594,\n",
       " 0.0124981141778276,\n",
       " 0.012449864300291427,\n",
       " 0.012401877391201552,\n",
       " 0.012354151659349565,\n",
       " 0.01230668533082661,\n",
       " 0.012259476648751764,\n",
       " 0.01221252387300612,\n",
       " 0.012165825279972533,\n",
       " 0.012119379162280857,\n",
       " 0.0120731838285585,\n",
       " 0.01202723760318632,\n",
       " 0.011981538826059525,\n",
       " 0.011936085852353775,\n",
       " 0.011890877052295953,\n",
       " 0.011845910810939927,\n",
       " 0.01180118552794689,\n",
       " 0.011756699617370234,\n",
       " 0.011712451507445052,\n",
       " 0.011668439640381832,\n",
       " 0.011624662472164538,\n",
       " 0.011581118472352776,\n",
       " 0.01153780612388814,\n",
       " 0.011494723922904465,\n",
       " 0.011451870378541977,\n",
       " 0.011409244012765296,\n",
       " 0.011366843360185083,\n",
       " 0.011324666967883446,\n",
       " 0.011282713395242743,\n",
       " 0.011240981213777972,\n",
       " 0.011199469006972482,\n",
       " 0.011158175370117037,\n",
       " 0.01111709891015219,\n",
       " 0.011076238245513582,\n",
       " 0.011035592005980614,\n",
       " 0.010995158832528017,\n",
       " 0.010954937377180294,\n",
       " 0.01091492630286925,\n",
       " 0.010875124283294084,\n",
       " 0.010835530002784524,\n",
       " 0.01079614215616641,\n",
       " 0.010756959448630015,\n",
       " 0.010717980595600949,\n",
       " 0.010679204322613576,\n",
       " 0.010640629365186813,\n",
       " 0.010602254468702374,\n",
       " 0.010564078388285427,\n",
       " 0.01052609988868741,\n",
       " 0.010488317744171173,\n",
       " 0.010450730738398289,\n",
       " 0.010413337664318506,\n",
       " 0.010376137324061242,\n",
       " 0.01033912852882915,\n",
       " 0.010302310098793788,\n",
       " 0.010265680862992963,\n",
       " 0.010229239659230283,\n",
       " 0.010192985333976364,\n",
       " 0.010156916742271932,\n",
       " 0.010121032747632699,\n",
       " 0.010085332221955953,\n",
       " 0.01004981404542888,\n",
       " 0.010014477106438501,\n",
       " 0.009979320301483217,\n",
       " 0.009944342535086036,\n",
       " 0.009909542719709178,\n",
       " 0.009874919775670404,\n",
       " 0.009840472631060549,\n",
       " 0.009806200221662842,\n",
       " 0.009772101490873247,\n",
       " 0.009738175389622565,\n",
       " 0.00970442087629954,\n",
       " 0.009670836916675528,\n",
       " 0.009637422483830371,\n",
       " 0.009604176558079425,\n",
       " 0.009571098126902032,\n",
       " 0.009538186184870968,\n",
       " 0.009505439733583207,\n",
       " 0.009472857781591787,\n",
       " 0.009440439344338804,\n",
       " 0.00940818344408949,\n",
       " 0.009376089109867349,\n",
       " 0.009344155377390384,\n",
       " 0.009312381289008295,\n",
       " 0.009280765893640726,\n",
       " 0.009249308246716471,\n",
       " 0.009218007410113565,\n",
       " 0.00918686245210047,\n",
       " 0.009155872447277986,\n",
       " 0.00912503647652222,\n",
       " 0.009094353626928278,\n",
       " 0.009063822991754976,\n",
       " 0.009033443670370216,\n",
       " 0.00900321476819729,\n",
       " 0.008973135396661976,\n",
       " 0.0089432046731403,\n",
       " 0.008913421720907255,\n",
       " 0.008883785669086099,\n",
       " 0.00885429565259846,\n",
       " 0.008824950812115082,\n",
       " 0.008795750294007431,\n",
       " 0.008766693250299815,\n",
       " 0.008737778838622192,\n",
       " 0.008709006222163763,\n",
       " 0.008680374569627086,\n",
       " 0.008651883055182785,\n",
       " 0.008623530858425004,\n",
       " 0.008595317164327404,\n",
       " 0.008567241163199596,\n",
       " 0.008539302050644479,\n",
       " 0.008511499027515754,\n",
       " 0.00848383129987628,\n",
       " 0.008456298078956878,\n",
       " 0.008428898581115559,\n",
       " 0.008401632027797416,\n",
       " 0.00837449764549495,\n",
       " 0.008347494665708919,\n",
       " 0.008320622324909599,\n",
       " 0.008293879864498614,\n",
       " 0.00826726653077119,\n",
       " 0.008240781574878824,\n",
       " 0.008214424252792457,\n",
       " 0.008188193825266104,\n",
       " 0.008162089557800782,\n",
       " 0.00813611072060903,\n",
       " 0.008110256588579686,\n",
       " 0.008084526441243176,\n",
       " 0.008058919562737141,\n",
       " 0.008033435241772467,\n",
       " 0.008008072771599685,\n",
       " 0.007982831449975745,\n",
       " 0.007957710579131182,\n",
       " 0.00793270946573758,\n",
       " 0.00790782742087545,\n",
       " 0.007883063760002387,\n",
       " 0.007858417802921668,\n",
       " 0.007833888873751017,\n",
       " 0.007809476300891843,\n",
       " 0.007785179416998758,\n",
       " 0.0077609975589493395,\n",
       " 0.007736930067814318,\n",
       " 0.007712976288827915,\n",
       " 0.007689135571358626,\n",
       " 0.007665407268880232,\n",
       " 0.007641790738943,\n",
       " 0.007618285343145384,\n",
       " 0.007594890447105752,\n",
       " 0.007571605420434607,\n",
       " 0.00754842963670687,\n",
       " 0.007525362473434602,\n",
       " 0.007502403312039833,\n",
       " 0.007479551537827779,\n",
       " 0.007456806539960173,\n",
       " 0.007434167711428966,\n",
       " 0.007411634449030156,\n",
       " 0.007389206153337902,\n",
       " 0.007366882228678925,\n",
       " 0.007344662083107018,\n",
       " 0.007322545128377872,\n",
       " 0.0073005307799241005,\n",
       " 0.007278618456830427,\n",
       " 0.007256807581809181,\n",
       " 0.007235097581175937,\n",
       " 0.007213487884825392,\n",
       " 0.007191977926207381,\n",
       " 0.007170567142303189,\n",
       " 0.007149254973602024,\n",
       " 0.007128040864077648,\n",
       " 0.007106924261165206,\n",
       " 0.007085904615738335,\n",
       " 0.007064981382086316,\n",
       " 0.007044154017891536,\n",
       " 0.007023421984207042,\n",
       " 0.007002784745434294,\n",
       " 0.00698224176930116,\n",
       " 0.006961792526840015,\n",
       " 0.006941436492365983,\n",
       " 0.00692117314345544,\n",
       " 0.006901001960924617,\n",
       " 0.00688092242880839,\n",
       " 0.006860934034339208,\n",
       " 0.006841036267926225,\n",
       " 0.006821228623134564,\n",
       " 0.006801510596664706,\n",
       " 0.0067818816883321136,\n",
       " 0.006762341401046888,\n",
       " 0.006742889240793699,\n",
       " 0.006723524716611826,\n",
       " 0.006704247340575244,\n",
       " 0.006685056627773024,\n",
       " 0.006665952096289727,\n",
       " 0.0066469332671860535,\n",
       " 0.006627999664479538,\n",
       " 0.006609150815125468,\n",
       " 0.006590386248997853,\n",
       " 0.006571705498870575,\n",
       " 0.006553108100398717,\n",
       " 0.006534593592099922,\n",
       " 0.0065161615153359834,\n",
       " 0.006497811414294421,\n",
       " 0.006479542835970429,\n",
       " 0.006461355330148676,\n",
       " 0.006443248449385384,\n",
       " 0.006425221748990548,\n",
       " 0.0064072747870102115,\n",
       " 0.006389407124208841,\n",
       " 0.006371618324051898,\n",
       " 0.0063539079526885285,\n",
       " 0.006336275578934288,\n",
       " 0.006318720774254041,\n",
       " 0.006301243112744963,\n",
       " 0.006283842171119718,\n",
       " 0.006266517528689596,\n",
       " 0.0062492687673479375,\n",
       " 0.006232095471553571,\n",
       " 0.006214997228314341,\n",
       " 0.00619797362717083,\n",
       " 0.006181024260180157,\n",
       " 0.006164148721899795,\n",
       " 0.006147346609371635,\n",
       " 0.0061306175221060985,\n",
       " 0.006113961062066271,\n",
       " 0.006097376833652282,\n",
       " 0.006080864443685708,\n",
       " 0.0060644235013940515,\n",
       " 0.006048053618395411,\n",
       " 0.006031754408683159,\n",
       " 0.006015525488610783,\n",
       " 0.00599936647687676,\n",
       " 0.005983276994509652,\n",
       " 0.005967256664853122,\n",
       " 0.005951305113551198,\n",
       " 0.005935421968533568,\n",
       " 0.0059196068600009585,\n",
       " 0.00590385942041064,\n",
       " 0.005888179284461993,\n",
       " 0.005872566089082249,\n",
       " 0.005857019473412143,\n",
       " 0.005841539078791879,\n",
       " 0.00582612454874706,\n",
       " 0.005810775528974664,\n",
       " 0.0057954916673292655,\n",
       " 0.005780272613809192,\n",
       " 0.005765118020542847,\n",
       " 0.0057500275417751035,\n",
       " 0.005735000833853797,\n",
       " 0.005720037555216286,\n",
       " 0.005705137366376104,\n",
       " 0.005690299929909688,\n",
       " 0.0056755249104431926,\n",
       " 0.0056608119746394394,\n",
       " 0.005646160791184877,\n",
       " 0.005631571030776615,\n",
       " 0.005617042366109665,\n",
       " 0.005602574471864116,\n",
       " 0.005588167024692441,\n",
       " 0.005573819703206921,\n",
       " 0.00555953218796714,\n",
       " 0.005545304161467493,\n",
       " 0.005531135308124863,\n",
       " 0.005517025314266295,\n",
       " 0.005502973868116852,\n",
       " 0.005488980659787398,\n",
       " 0.005475045381262604,\n",
       " 0.00546116772638897,\n",
       " 0.005447347390862885,\n",
       " 0.005433584072218845,\n",
       " 0.005419877469817666,\n",
       " 0.005406227284834831,\n",
       " 0.005392633220248855,\n",
       " 0.005379094980829823,\n",
       " 0.005365612273127837,\n",
       " 0.005352184805461719,\n",
       " 0.005338812287907653,\n",
       " 0.005325494432287968,\n",
       " 0.005312230952159938,\n",
       " 0.005299021562804727,\n",
       " 0.005285865981216341,\n",
       " 0.005272763926090646,\n",
       " 0.005259715117814528,\n",
       " 0.005246719278455054,\n",
       " 0.0052337761317486926,\n",
       " 0.005220885403090693,\n",
       " 0.0052080468195244275,\n",
       " 0.005195260109730849,\n",
       " 0.005182525004018014,\n",
       " 0.005169841234310709,\n",
       " 0.005157208534140037,\n",
       " 0.005144626638633199,\n",
       " 0.005132095284503214,\n",
       " 0.005119614210038858,\n",
       " 0.0051071831550944915,\n",
       " 0.005094801861080092,\n",
       " 0.005082470070951267,\n",
       " 0.005070187529199413,\n",
       " 0.005057953981841796,\n",
       " 0.005045769176411872,\n",
       " 0.005033632861949543,\n",
       " 0.0050215447889914895,\n",
       " 0.005009504709561663,\n",
       " 0.004997512377161675,\n",
       " 0.004985567546761401,\n",
       " 0.004973669974789573,\n",
       " 0.004961819419124425,\n",
       " 0.004950015639084418,\n",
       " 0.004938258395419018,\n",
       " 0.004926547450299565,\n",
       " 0.004914882567310122,\n",
       " 0.0049032635114384755,\n",
       " 0.00489169004906713,\n",
       " 0.004880161947964385,\n",
       " 0.004868678977275473,\n",
       " 0.004857240907513733,\n",
       " 0.004845847510551857,\n",
       " 0.004834498559613199,\n",
       " 0.004823193829263156,\n",
       " 0.00481193309540049,\n",
       " 0.004800716135248901,\n",
       " 0.004789542727348497,\n",
       " 0.004778412651547377,\n",
       " 0.004767325688993279,\n",
       " 0.0047562816221252075,\n",
       " 0.004745280234665285,\n",
       " 0.004734321311610404,\n",
       " 0.004723404639224205,\n",
       " 0.004712530005028873,\n",
       " 0.0047016971977971635,\n",
       " 0.004690906007544337,\n",
       " 0.004680156225520283,\n",
       " 0.004669447644201571,\n",
       " 0.0046587800572836366,\n",
       " 0.004648153259672984,\n",
       " 0.004637567047479441,\n",
       " 0.0046270212180084674,\n",
       " 0.004616515569753518,\n",
       " 0.00460604990238846,\n",
       " 0.0045956240167600165,\n",
       " 0.004585237714880267,\n",
       " 0.0045748907999192305,\n",
       " 0.004564583076197448,\n",
       " 0.004554314349178642,\n",
       " 0.004544084425462412,\n",
       " 0.004533893112776987,\n",
       " 0.004523740219972049,\n",
       " 0.004513625557011481,\n",
       " 0.0045035489349663825,\n",
       " 0.004493510166007925,\n",
       " 0.004483509063400343,\n",
       " 0.0044735454414939774,\n",
       " 0.004463619115718331,\n",
       " 0.004453729902575225,\n",
       " 0.004443877619631898,\n",
       " 0.00443406208551428,\n",
       " 0.00442428311990019,\n",
       " 0.00441454054351268,\n",
       " 0.0044048341781133355,\n",
       " 0.004395163846495686,\n",
       " 0.004385529372478622,\n",
       " 0.004375930580899862,\n",
       " 0.004366367297609476,\n",
       " 0.0043568393494634395,\n",
       " 0.004347346564317222,\n",
       " 0.004337888771019435,\n",
       " 0.004328465799405525,\n",
       " 0.0043190774802914535,\n",
       " 0.004309723645467529,\n",
       " 0.0043004041276921675,\n",
       " 0.0042911187606857314,\n",
       " 0.004281867379124457,\n",
       " 0.004272649818634342,\n",
       " 0.004263465915785149,\n",
       " 0.004254315508084372,\n",
       " 0.0042451984339713245,\n",
       " 0.004236114532811183,\n",
       " 0.004227063644889149,\n",
       " 0.004218045611404598,\n",
       " 0.004209060274465273,\n",
       " 0.004200107477081535,\n",
       " 0.004191187063160648,\n",
       " 0.004182298877501061,\n",
       " 0.004173442765786823,\n",
       " 0.0041646185745818935,\n",
       " 0.004155826151324655,\n",
       " 0.004147065344322312,\n",
       " 0.0041383360027454205,\n",
       " 0.004129637976622418,\n",
       " 0.004120971116834209,\n",
       " 0.0041123352751087725,\n",
       " 0.00410373030401578,\n",
       " 0.004095156056961316,\n",
       " 0.004086612388182555,\n",
       " 0.004078099152742561,\n",
       " 0.004069616206525005,\n",
       " 0.004061163406229089,\n",
       " 0.004052740609364257,\n",
       " 0.004044347674245203,\n",
       " 0.0040359844599867495,\n",
       " 0.004027650826498812,\n",
       " 0.004019346634481388,\n",
       " 0.0040110717454195375,\n",
       " 0.004002826021578501,\n",
       " 0.003994609325998767,\n",
       " 0.0039864215224911966,\n",
       " 0.00397826247563213,\n",
       " 0.003970132050758631,\n",
       " 0.0039620301139637005,\n",
       " 0.0039539565320914905,\n",
       " 0.0039459111727325885,\n",
       " 0.003937893904219368,\n",
       " 0.003929904595621301,\n",
       " 0.003921943116740325,\n",
       " 0.003914009338106275,\n",
       " 0.0039061031309722874,\n",
       " 0.0038982243673103063,\n",
       " 0.0038903729198065343,\n",
       " 0.003882548661856981,\n",
       " 0.0038747514675630157,\n",
       " 0.003866981211726938,\n",
       " 0.0038592377698476454,\n",
       " 0.0038515210181162044,\n",
       " 0.003843830833411553,\n",
       " 0.0038361670932961973,\n",
       " 0.003828529676011986,\n",
       " 0.0038209184604757703,\n",
       " 0.0038133333262752836,\n",
       " 0.0038057741536649146,\n",
       " 0.003798240823561558,\n",
       " 0.003790733217540456,\n",
       " 0.00378325121783114,\n",
       " 0.0037757947073133284,\n",
       " 0.0037683635695128794,\n",
       " 0.00376095768859776,\n",
       " 0.0037535769493740813,\n",
       " 0.003746221237282092,\n",
       " 0.003738890438392285,\n",
       " 0.0037315844394014155,\n",
       " 0.0037243031276286516,\n",
       " 0.0037170463910117055,\n",
       " 0.003709814118103028,\n",
       " 0.003702606198065906,\n",
       " 0.0036954225206707603,\n",
       " 0.0036882629762913363,\n",
       " 0.0036811274559010046,\n",
       " 0.003674015851068982,\n",
       " 0.0036669280539567382,\n",
       " 0.0036598639573142467,\n",
       " 0.003652823454476384,\n",
       " 0.0036458064393593043,\n",
       " 0.0036388128064568466,\n",
       " 0.0036318424508370063,\n",
       " 0.003624895268138319,\n",
       " 0.0036179711545663937,\n",
       " 0.0036110700068904154,\n",
       " 0.0036041917224396095,\n",
       " 0.0035973361990999154,\n",
       " 0.003590503335310399,\n",
       " 0.003583693030059978,\n",
       " 0.0035769051828840053,\n",
       " 0.00357013969386085,\n",
       " 0.0035633964636086306,\n",
       " 0.0035566753932818823,\n",
       " 0.0035499763845682313,\n",
       " 0.003543299339685158,\n",
       " 0.003536644161376737,\n",
       " 0.0035300107529104056,\n",
       " 0.003523399018073762,\n",
       " 0.0035168088611713783,\n",
       " 0.0035102401870215926,\n",
       " 0.0035036929009534556,\n",
       " 0.003497166908803551,\n",
       " 0.0034906621169128516,\n",
       " 0.003484178432123734,\n",
       " 0.0034777157617768596,\n",
       " 0.003471274013708118,\n",
       " 0.003464853096245649,\n",
       " 0.00345845291820682,\n",
       " 0.003452073388895228,\n",
       " 0.003445714418097774,\n",
       " 0.0034393759160817137,\n",
       " 0.003433057793591697,\n",
       " 0.0034267599618469143,\n",
       " 0.003420482332538173,\n",
       " 0.003414224817825064,\n",
       " 0.003407987330333088,\n",
       " 0.0034017697831508543,\n",
       " 0.003395572089827267,\n",
       " 0.003389394164368693,\n",
       " 0.0033832359212362523,\n",
       " 0.0033770972753430577,\n",
       " 0.0033709781420513943,\n",
       " 0.003364878437170129,\n",
       " 0.0033587980769519163,\n",
       " 0.003352736978090563,\n",
       " 0.003346695057718351,\n",
       " 0.0033406722334033737,\n",
       " 0.0033346684231469435,\n",
       " 0.0033286835453809603,\n",
       " 0.0033227175189653032,\n",
       " 0.0033167702631852697,\n",
       " 0.003310841697749051,\n",
       " 0.003304931742785089,\n",
       " 0.0032990403188396557,\n",
       " 0.0032931673468742876,\n",
       " 0.003287312748263309,\n",
       " 0.003281476444791334,\n",
       " 0.0032756583586508303,\n",
       " 0.0032698584124396606,\n",
       " 0.0032640765291586725,\n",
       " 0.003258312632209259,\n",
       " 0.0032525666453909544,\n",
       " 0.003246838492899121,\n",
       " 0.0032411280993224836,\n",
       " 0.003235435389640855,\n",
       " 0.0032297602892227746,\n",
       " 0.0032241027238232053,\n",
       " 0.003218462619581173,\n",
       " 0.0032128399030175664,\n",
       " 0.0032072345010327906,\n",
       " 0.0032016463409045193,\n",
       " 0.003196075350285504,\n",
       " 0.0031905214572012595,\n",
       " 0.003184984590047934,\n",
       " 0.003179464677590029,\n",
       " 0.003173961648958267,\n",
       " 0.0031684754336474257,\n",
       " 0.0031630059615141057,\n",
       " 0.003157553162774689,\n",
       " 0.0031521169680031225,\n",
       " 0.0031466973081288674,\n",
       " 0.0031412941144347353,\n",
       " 0.003135907318554867,\n",
       " 0.0031305368524725896,\n",
       " 0.0031251826485184098,\n",
       " 0.0031198446393679315,\n",
       " 0.003114522758039833,\n",
       " 0.00310921693789387,\n",
       " 0.0031039271126287985,\n",
       " 0.003098653216280458,\n",
       " 0.003093395183219755,\n",
       " 0.0030881529481506667,\n",
       " 0.003082926446108337,\n",
       " 0.003077715612457117,\n",
       " 0.0030725203828885724,\n",
       " 0.0030673406934196845,\n",
       " 0.003062176480390814,\n",
       " 0.0030570276804639087,\n",
       " 0.0030518942306205703,\n",
       " 0.0030467760681601786,\n",
       " 0.0030416731306980816,\n",
       " 0.003036585356163703,\n",
       " 0.003031512682798719,\n",
       " 0.0030264550491552523,\n",
       " 0.0030214123940940623,\n",
       " 0.003016384656782715,\n",
       " 0.0030113717766938435,\n",
       " 0.0030063736936033225,\n",
       " 0.003001390347588546,\n",
       " 0.0029964216790266843,\n",
       " 0.0029914676285928586,\n",
       " 0.002986528137258529,\n",
       " 0.002981603146289677,\n",
       " 0.002976692597245185,\n",
       " 0.002971796431975035,\n",
       " 0.002966914592618736,\n",
       " 0.0029620470216035638,\n",
       " 0.0029571936616429374,\n",
       " 0.0029523544557347675,\n",
       " 0.002947529347159776,\n",
       " 0.0029427182794799145,\n",
       " 0.002937921196536724,\n",
       " 0.002933138042449693,\n",
       " 0.002928368761614696,\n",
       " 0.0029236132987024,\n",
       " 0.0029188715986566576,\n",
       " 0.0029141436066929525,\n",
       " 0.0029094292682968546,\n",
       " 0.0029047285292224355,\n",
       " 0.002900041335490758,\n",
       " 0.0028953676333883255,\n",
       " 0.00289070736946557,\n",
       " 0.0028860604905353603,\n",
       " 0.002881426943671456,\n",
       " 0.0028768066762070495,\n",
       " 0.002872199635733282,\n",
       " 0.0028676057700977387,\n",
       " 0.0028630250274030425,\n",
       " 0.0028584573560053598,\n",
       " 0.002853902704512935,\n",
       " 0.0028493610217847263,\n",
       " 0.002844832256928891,\n",
       " 0.0028403163593014456,\n",
       " 0.0028358132785047856,\n",
       " 0.0028313229643863918,\n",
       " 0.002826845367037294,\n",
       " 0.002822380436790828,\n",
       " 0.002817928124221169,\n",
       " 0.0028134883801420045,\n",
       " 0.0028090611556051627,\n",
       " 0.0028046464018992986,\n",
       " 0.002800244070548533,\n",
       " 0.0027958541133111,\n",
       " 0.0027914764821780646,\n",
       " 0.0027871111293719915,\n",
       " 0.0027827580073456217,\n",
       " 0.0027784170687806193,\n",
       " 0.0027740882665862306,\n",
       " 0.0027697715538980592]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,Y,iterations,alpha,X_train,Y_train):\n",
    "\n",
    "    W1,W2,W3,b1,b2,b3,count,caches = gradient_descent(iterations,X_train,Y_train,alpha)\n",
    "    \n",
    "    Z1 = np.dot(W1,X)\n",
    "    A1 = tanh(Z1 + b1)\n",
    "    Z2 = np.dot(W2,A1)\n",
    "    A2 = tanh(Z2 + b2)\n",
    "    Z3 = np.dot(W3,A2)\n",
    "    A3 = sigmoid(Z3 + b3)\n",
    "    \n",
    "    dummy,m = A3.shape\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        Y_prediction[0, i] = 1 if A3[0, i] > 0.5 else 0\n",
    "        \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 98.6842105263158 %\n",
      "Cross validation accuracy: 96.80851063829788 %\n"
     ]
    }
   ],
   "source": [
    "#Lets see how accurate the predictions made by our neural network are compared to the training set and cross validation set\n",
    "print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(predict(X_train,Y_train,1000,0.5,X_train,Y_train) - Y_train)) * 100))\n",
    "print(\"Cross validation accuracy: {} %\".format(100 - np.mean(np.abs(predict(X_cv,Y_cv,1000,0.5,X_train,Y_train) - Y_cv)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 positives predicted on the training set\n",
      "168 true positives are in the training set\n",
      "The accuracy of true positives on the training set is: 98.80952380952381 %\n",
      "----------------------------------------------------------------\n",
      "43 positives predicted on the cross validation set\n",
      "43 true positives are in the cross validation set\n",
      "The accuracy of true positives on the cross validation set is: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "dummy,m1 = X_train.shape\n",
    "dummy,m2 = X_cv.shape\n",
    "\n",
    "train_predict = predict(X_train,Y_train,1000,0.5,X_train,Y_train)\n",
    "CV_predict = predict(X_cv,Y_cv,1000,0.5,X_train,Y_train)\n",
    "count_true_pos = 0\n",
    "count_train_pos = 0\n",
    "\n",
    "count_true_pos_cv = 0\n",
    "count_cv_pos = 0\n",
    "\n",
    "for i in range (1,m1):\n",
    "    if train_predict[0,i] == 1 and Y_train[0,i] == 1:\n",
    "        count_true_pos = count_true_pos + 1\n",
    "    if Y_train[0,i] == 1:\n",
    "        count_train_pos = count_train_pos + 1\n",
    "        \n",
    "for i in range (1,m2):\n",
    "    if CV_predict[0,i] == 1 and Y_cv[0,i] == 1:\n",
    "        count_true_pos_cv = count_true_pos_cv + 1\n",
    "    if Y_cv[0,i] == 1:\n",
    "        count_cv_pos = count_cv_pos + 1\n",
    "        \n",
    "print(str(count_true_pos) + \" positives predicted on the training set\")\n",
    "print(str(count_train_pos) + \" true positives are in the training set\")\n",
    "print(\"The accuracy of true positives on the training set is: {} %\".format(100-np.abs(100*((count_true_pos - count_train_pos)/count_train_pos))))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(str(count_true_pos_cv) + \" positives predicted on the cross validation set\")\n",
    "print(str(count_cv_pos) + \" true positives are in the cross validation set\")\n",
    "print(\"The accuracy of true positives on the cross validation set is: {} %\".format(100-np.abs(100*((count_true_pos_cv - count_cv_pos)/count_true_pos_cv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 negatives predicted on the training set\n",
      "211 true negatives are in the training set\n",
      "The accuracy of true negatives on the training set is: 99.5260663507109 %\n",
      "----------------------------------------------------------------\n",
      "138 negatives predicted on the cross validation set\n",
      "144 true negatives are in the cross validation set\n",
      "The accuracy of true negatives on the cross validation set is: 95.65217391304348 %\n"
     ]
    }
   ],
   "source": [
    "count_true_neg = 0\n",
    "count_train_neg = 0\n",
    "\n",
    "count_true_neg_cv = 0\n",
    "count_cv_neg = 0\n",
    "\n",
    "for i in range (1,m1):\n",
    "    if train_predict[0,i] == 0 and Y_train[0,i] == 0:\n",
    "        count_true_neg = count_true_neg + 1\n",
    "    if Y_train[0,i] == 0:\n",
    "        count_train_neg = count_train_neg + 1\n",
    "        \n",
    "for i in range (1,m2):\n",
    "    if CV_predict[0,i] == 0 and Y_cv[0,i] == 0:\n",
    "        count_true_neg_cv = count_true_neg_cv + 1\n",
    "    if Y_cv[0,i] == 0:\n",
    "        count_cv_neg = count_cv_neg + 1\n",
    "        \n",
    "print(str(count_true_neg) + \" negatives predicted on the training set\")\n",
    "print(str(count_train_neg) + \" true negatives are in the training set\")\n",
    "print(\"The accuracy of true negatives on the training set is: {} %\".format(100-np.abs(100*((count_true_neg - count_train_neg)/count_train_neg))))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(str(count_true_neg_cv) + \" negatives predicted on the cross validation set\")\n",
    "print(str(count_cv_neg) + \" true negatives are in the cross validation set\")\n",
    "print(\"The accuracy of true negatives on the cross validation set is: {} %\".format(100-np.abs(100*((count_true_neg_cv - count_cv_neg)/count_true_neg_cv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('soldgame': conda)",
   "language": "python",
   "name": "python37764bitsoldgameconda7262750cf6184ac8bf201f1ff27a4c4f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
