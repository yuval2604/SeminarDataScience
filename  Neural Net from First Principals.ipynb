{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "dataset = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "dataset.shape #Let's take a look at the dimensions of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'M':1,'B':0}\n",
    "dataset = dataset.replace(d) #replace M/B with 0 or 1 for the neural net classification\n",
    "dataset = dataset.drop(['Unnamed: 32'],axis=1) #remove column 32 - unknown purpose\n",
    "dataset = dataset.drop(['id'],axis=1) #remove the id - not needed for the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_temp = dataset.drop(['diagnosis'],axis=1) #make a temporary dataset with only our feature vectors\n",
    "X = np.array(dataset_temp).T #create our Numpy array of feature vectors to be used in our neural net\n",
    "Y = np.array(dataset['diagnosis']).T #create our Numpy array of diagnosis to be used in our neural net\n",
    "Y = Y.reshape(1,569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's normalized our feature vector.  We will force the mean of each column to 0, and divide by the maximum\n",
    "X_mean = np.mean(X,axis=1,keepdims=True) #Find the mean of each feature\n",
    "X_max = np.max(X,axis=1,keepdims=True) #Find the maximum of each feature\n",
    "X_normalized = (X-X_mean)/(X_max) #Normalizing our dataset by subtracting the mean and dividing by the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's split our dataset into two segments\n",
    "# 1) Training set to train our neural net\n",
    "# 2) A cross validation set to test the accuracy of our neural net\n",
    "\n",
    "#We'll take the first 380 samples for our training set\n",
    "X_train = X_normalized[:,:380]\n",
    "Y_train = Y[:,:380]\n",
    "\n",
    "#We will take the remaining 189 for our cross-validation set\n",
    "X_cv = X_normalized[:,381:]\n",
    "Y_cv = Y[:,381:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now define our sigmoid function to be used in the output layer of our neural network (L3)\n",
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now define our tanh(x) function to be used in hidden layers of our neural network (L1, L2)\n",
    "#Note that the tanh(x) function allows better centering of data than the sigmoid function.  This is why it will be used in our hidden layers.\n",
    "\n",
    "def tanh(z):\n",
    "    s = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's define our forward propogation function.\n",
    "def forward_prop(X,W1,W2,W3,b1,b2,b3):\n",
    "    \n",
    "    #First layer forward propogation\n",
    "    Z1 = np.dot(W1,X)# where W1 represents our matrix of weights in L1, and X represents our feature matrix of measures\n",
    "    A1 = tanh(Z1 + b1) #where b1 represents our intercept term for our first layer\n",
    "    #Second layer forward propogation\n",
    "    Z2 = np.dot(W2,A1) #where W2 represents our matrix of weights in L2\n",
    "    A2 = tanh(Z2 + b2) # where b2 represents our intercept term for our second layer\n",
    "    #Third layer forward propogation\n",
    "    Z3 = np.dot(W3,A2) #where W3 represents our matrix of weights in L3\n",
    "    #where b3 represents our intercept term for our second layer\n",
    "    A3 = sigmoid(Z3 + b3) #A3 will produce our probability vector\n",
    "    \n",
    "    cache = {    \n",
    "                  \"Z1\": Z1,\n",
    "                  \"A1\": A1,\n",
    "                  \"Z2\": Z2,\n",
    "                  \"A2\": A2,\n",
    "                  \"Z3\": Z3,\n",
    "                  \"A3\": A3\n",
    "            }\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will perform gradient descent for our neural network in the following steps:\n",
    "#1) Start by randomly initializing our weight and intercept parameters\n",
    "#2) Run forward propogation through our neural network\n",
    "#3) Calculate the derivatives of our weights and intercept parameters via back propogation\n",
    "#4) Refine our parameters using derivatives from (3)\n",
    "#5) Reiterate 1 - 4 \n",
    "\n",
    "def gradient_descent(iterations,X,Y,alpha):\n",
    "    \n",
    "    #Randomly initialized our parameters before running the algorithm\n",
    "    W1 = np.random.randn(3,30)*0.01\n",
    "    b1 = np.random.rand(3,1)\n",
    "    W2 = np.random.randn(2,3)*0.01\n",
    "    b2 = np.random.rand(2,1)\n",
    "    W3 = np.random.rand(1,2)*0.01\n",
    "    b3 = np.random.rand(1,1)\n",
    "    dummy,m = X.shape\n",
    "    \n",
    "    caches = [] #we will store our cost at each iteration in this array\n",
    "    count_vector = [] #We will store our iteration count in this array\n",
    "    count = 0\n",
    "    \n",
    "    for i in range (1,iterations):\n",
    "        \n",
    "            count = count + 1\n",
    "            \n",
    "            count_vector.append(count)\n",
    "        \n",
    "            params = forward_prop(X,W1,W2,W3,b1,b2,b3) #forward propogation using our parameters\n",
    "            \n",
    "            #Define our values to be used in back propogation using the dictionary of values created from running forward_prop\n",
    "            Z1 = params['Z1']\n",
    "            Z2 = params['Z2']\n",
    "            Z3 = params['Z3']\n",
    "            A1 = params['A1']\n",
    "            A2 = params['A2']\n",
    "            A3 = params['A3']\n",
    "            \n",
    "            #Define our cost function, append the cost of each iteration to caches\n",
    "            cost = -(1 / m)*np.sum(np.multiply(Y,np.log(A3)) + np.multiply((1-Y),np.log(1-A3)))\n",
    "            caches.append(cost)\n",
    "            \n",
    "            #Back propogation for layer 3\n",
    "            dA3 = -Y/A3 + (1-Y)/(1-A3)\n",
    "            dZ3 = dA3 * sigmoid(Z3)*(1-sigmoid(Z3))\n",
    "            dW3 = (1 / m)*np.dot(dZ3,A2.T)\n",
    "            db3 = (1 / m)*np.sum(dZ3,axis=1,keepdims=True)\n",
    "            \n",
    "            #Back propogation for layer 2\n",
    "            dA2 = np.dot(W3.T,dZ3)\n",
    "            dZ2 = dA2*(1-np.power(tanh(Z2),2))\n",
    "            dW2 = (1 / m)*np.dot(dZ2,A1.T)\n",
    "            db2 = (1 / m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "            \n",
    "            #Back propogation for layer 1\n",
    "            dA1 = np.dot(W2.T,dZ2)\n",
    "            dZ1 = dA1*(1-np.power(tanh(Z1),2))\n",
    "            dW1 = (1 / m)*np.dot(dZ1,X.T)\n",
    "            db1 = (1 / m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "            \n",
    "            #Redefine our weight parameters using the derivatives calculated in back propogation\n",
    "            W1 = W1 - alpha*dW1\n",
    "            W2 = W2 - alpha*dW2\n",
    "            W3 = W3 - alpha*dW3\n",
    "            \n",
    "            #Redefine our weight parameters using the derivatives calculated in back propogation\n",
    "            b1 = b1 - alpha*db1\n",
    "            b2 = b2 - alpha*db2\n",
    "            b3 = b3 - alpha*db3\n",
    "        \n",
    "    return W1,W2,W3,b1,b2,b3,count_vector,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5wddX3/8dd7zzl7zW6uS8gNEiCo4RZhwUsRqaICRVCrEqRV64XaX6lavBR+9meV+hOVX5VS8YLUeikCSsVGpUUFLKCC2WhAQkBCALMJJJv7dbO3z++Pmd2cLJtks+zs2bPn/Xw8zmPPzHzPzGcysO/9zpz5jiICMzOrXFWlLsDMzErLQWBmVuEcBGZmFc5BYGZW4RwEZmYVzkFgZlbhHARm44ykV0h6rNR1WPlwENiYIOltklol7ZD0jKT/knT681znU5LOGqkah7jNuZJCUj6d/oakT2W8zZB0TN90RNwbES/Icps2vjgIrOQkXQZcA3wamA4cAXwJuKCUdY0FfYFiliUHgZWUpInAlcBfR8T3I2JnRHRFxA8j4iNpmxpJ10ham76ukVSTLpsm6UeStkjaJOleSVWSvk0SKD9MexkfHWTbKySdVzSdl9Qu6WRJtZL+XdLGdN1LJE0/xH27BLgY+Ghaww/T+TMl/Ue6rSclvb/oM5+QdGu67W3AOyWdJulXaR3PSPqipOq0/T3pRx9Mt3GhpDMltRWt80WSfp5+frmk84uWfUPSdZJ+LGm7pAckHX0o+2njQET45VfJXsDZQDeQP0CbK4H7gcOAZuCXwD+my64CvgIU0tcrAKXLngLOOsB6Pw7cWDT9J8CK9P1fAj8E6oEccArQNIT9mQtE3/4A3wA+VbS8CliabrsaOApYBbwuXf4JoAt4Q9q2Lt32S4F8uv4VwAeL1hnAMUXTZwJt6fsCsBL43+n2XgVsB15QVN9G4LR0/TcCN5f6vwu/RvflHoGV2lRgQ0R0H6DNxcCVEbE+ItqBTwJ/ni7rAmYAR0bSk7g3IoY6gNZ3gPMl1afTbwNuKlrvVJJfsD0RsTQith3Cfu3PqUBzRFwZEZ0RsQr4GrCoqM2vIuIHEdEbEbvTbd8fEd0R8RTwVeCVQ9zeS4EJwGfS7d0F/Ai4qKjNbRHx6/QY3AgsfJ77aGXGQWClthGYdpBz4TOBp4umn07nAVxN8hfvTyStknT5UDccEStJ/rp+fRoG55OEA8C3gTuAm9PTUZ+TVBjqug/gSGBmeppmi6QtJH+tF592Wl38AUnHpqe/nk1PF30amDbE7c0EVkdEb9G8p4FZRdPPFr3fRRIcVkEcBFZqvwL2kJwK2Z+1JL9A+xyRziMitkfEhyLiKJJf5JdJenXabig9g5tI/jq+AHgkDQfS3sUnI2IB8HLgPODtQ9+tfgNrWA08GRGTil6NEXHuAT7zZeBRYH5ENJEEh4a4/bXAHEnF/68fAawZ+i7YeOcgsJKKiK0k58uvk/QGSfWSCpLOkfS5tNlNwN9LapY0LW3/7wCSzpN0jCQBW4EeoO+v33Uk5+AP5GbgtcBfsbc3gKQ/lnSCpBywjeRUUe/gqziggTX8Gtgu6e8k1UnKSTpe0qkHWEdjWsMOSS9Maz3QNoo9QPJX/kfTf9czgdeT7LcZ4CCwMSAi/gm4DPh7oJ3kr+ZLgR+kTT4FtAIPAb8DfpPOA5gP/AzYQdK7+FJE3J0uu4okQLZI+vB+tv1M+rmXA7cULTocuJXkF/AK4H9IThch6SuSvjLE3ftXYEFaww8iooekd7EQeBLYANwATDzAOj5Mcv1iO8n1hFsGLP8E8M10G28dsH+dJL/4z0m39SXg7RHx6BDrtwrQ9+0KMzOrUO4RmJlVOAeBmVmFcxCYmVU4B4GZWYUruwGtpk2bFnPnzi11GWZmZWXp0qUbIqJ5sGVlFwRz586ltbW11GWYmZUVSU/vb5lPDZmZVTgHgZlZhXMQmJlVuLK7RmBmNlxdXV20tbXR0dFR6lIyU1tby+zZsykUhj5YroPAzCpGW1sbjY2NzJ07l2ScwvElIti4cSNtbW3MmzdvyJ/L9NSQpLMlPSZp5WDjxEv6gqRl6ev36djsZmaZ6OjoYOrUqeMyBAAkMXXq1EPu8WTWI0iH770OeA3QBiyRtDgiHulrExF/W9T+b4AXZ1WPmRkwbkOgz3D2L8sewWnAyohYlQ6FezPJwz/25yL2PiZwxC15ahP/9JPH6O4ZzpDyZmbjV5ZBMIt9H7nXxr6Px+sn6UhgHnDXfpZfIqlVUmt7e/uwivntHzbzL3etZE+3g8DMSufZZ59l0aJFHH300Zxyyimce+65/P73vz+kdXz6058e0ZrGytdHFwG3pg/teI6IuD4iWiKipbl50DukD6qQS3a100FgZiUSEbzxjW/kzDPP5IknnmDp0qVcddVVrFu37pDWU05BsAaYUzQ9m/0/J3URGZ4WAqjOp0HgU0NmViJ33303hUKB973vff3zTjrpJE4//XQ+8pGPcPzxx3PCCSdwyy3JQ+ieeeYZzjjjDBYuXMjxxx/Pvffey+WXX87u3btZuHAhF1988YjUleXXR5cA8yXNIwmARSSP29tH+gzWySSPC8yMewRmVuyTP1zOI2u3jeg6F8xs4h9ef9x+lz/88MOccsopz5n//e9/n2XLlvHggw+yYcMGTj31VM444wy+853v8LrXvY6Pfexj9PT0sGvXLl7xilfwxS9+kWXLlo1Y3ZkFQUR0S7oUuAPIAV+PiOWSrgRaI2Jx2nQRcHNk/MzMGvcIzGyMuu+++7jooovI5XJMnz6dV77ylSxZsoRTTz2Vd73rXXR1dfGGN7yBhQsXZrL9TG8oi4jbgdsHzPv4gOlPZFlDn74eQZeDwMzggH+5Z+W4447j1ltvHXL7M844g3vuuYcf//jHvPOd7+Syyy7j7W9/+4jXNVYuFmeu2qeGzKzEXvWqV7Fnzx6uv/76/nkPPfQQkyZN4pZbbqGnp4f29nbuueceTjvtNJ5++mmmT5/Oe9/7Xt7znvfwm9/8BoBCoUBXV9eI1VUxQ0z0XSx2j8DMSkUSt912Gx/84Af57Gc/S21tLXPnzuWaa65hx44dnHTSSUjic5/7HIcffjjf/OY3ufrqqykUCkyYMIFvfetbAFxyySWceOKJnHzyydx4443Pv66MT82PuJaWlhjOg2l+9cRGLvra/XznvS/h5UdPy6AyMxvrVqxYwYte9KJSl5G5wfZT0tKIaBmsfeWcGurvEZRX8JmZZa1ygsDXCMzMBlU5QZB3EJhZcnfveDac/auYICjkkhH5fLHYrHLV1taycePGcRsGfc8jqK2tPaTPVdy3htwjMKtcs2fPpq2tjeEOXlkO+p5QdigqJwhyvrPYrNIVCoVDenJXpaiYU0PuEZiZDa5igsBDTJiZDa5igsA9AjOzwVVMEOSr/K0hM7PBVEwQSKI6X8UeB4GZ2T4qJggAanJVdHWPz+8Pm5kNV0UFQSFfRWfPoI9FNjOrWBUVBNW5Kl8sNjMboKKCoJCXRx81MxugooKgNp9jd6dPDZmZFcs0CCSdLekxSSslXb6fNm+V9Iik5ZK+k2U9E2rz7OzsznITZmZlJ7OxhiTlgOuA1wBtwBJJiyPikaI284ErgD+KiM2SDsuqHoAJNXl27HEQmJkVy7JHcBqwMiJWRUQncDNwwYA27wWui4jNABGxPsN6aKjOs6PDQWBmVizLIJgFrC6abkvnFTsWOFbSLyTdL+nswVYk6RJJrZJan8/wsRNq8+x0j8DMbB+lvlicB+YDZwIXAV+TNGlgo4i4PiJaIqKlubl52BvzqSEzs+fKMgjWAHOKpmen84q1AYsjoisingR+TxIMmWioybFjT/e4fTqRmdlwZBkES4D5kuZJqgYWAYsHtPkBSW8ASdNIThWtyqqghpo8vQEdXb6pzMysT2ZBEBHdwKXAHcAK4LsRsVzSlZLOT5vdAWyU9AhwN/CRiNiYVU2NNcmXpHx6yMxsr0wfVRkRtwO3D5j38aL3AVyWvjLXkAbBzj3dNDfWjMYmzczGvFJfLB5VDe4RmJk9R0UFQd+poe2+l8DMrF9FBcGUCdUAbNy5p8SVmJmNHRUVBIc31QLw7NaOEldiZjZ2VFQQTKwrUJOvYt02B4GZWZ+KCgJJHD6xlme3+dSQmVmfigoCgOlNtazzqSEzs34VFwQzJtayZsvuUpdhZjZmVFwQHDeziTVbdtO+3aeHzMygAoOgZe4UAJY+vanElZiZjQ0VFwTHz5xIU22em3692qOQmpmR8VhDY1F1vopLX3UMn779Uc699j5eMH0CTXUFmmoLTG6oZkpDgUn11TRPqGHetIb+YSnMzMarivwt957Tj6KuOs+PHlzLb1dvYdvuLrbu7qJ3kA7CzIm1vPjIyZwxfxrnnDCDptrC6BdsZpYhldvpkZaWlmhtbR3x9fb2Bts7utm0q5PNuzpZt7WDVRt28tiz27l/1UbWb99DfXWO95w+j//1x8dQW8iNeA1mZlmRtDQiWgZbVpE9gsFUVYmJ9QUm1heYR8M+yyKCh9q28rV7V3HtXSu567H1fPMvTmPqBA9lbWblr+IuFg+HJE6aM4kvvu1krv/zU1i5fgfv/mYrXT1+0pmZlT8HwSF67XGH8/m3LmTZ6i38y10rS12Omdnz5iAYhnNPmMF5J87g+nueYMMO35hmZuXNQTBMHzzrWDq6erllyepSl2Jm9rw4CIbpmMMm8LKjpnLr0jbfmGZmZS3TIJB0tqTHJK2UdPkgy98pqV3SsvT1nizrGWnnnjiDJzfs5In2HaUuxcxs2DILAkk54DrgHGABcJGkBYM0vSUiFqavG7KqJwtnvegwAO5+tL3ElZiZDV+WPYLTgJURsSoiOoGbgQsy3N6omzGxjiOm1NPqAezMrIxlGQSzgOIrqW3pvIH+VNJDkm6VNGewFUm6RFKrpNb29rH113fL3Mm0PrXZ1wnMrGyV+mLxD4G5EXEi8FPgm4M1iojrI6IlIlqam5tHtcCDOWn2JDbu7GSdH39pZmUqyyBYAxT/hT87ndcvIjZGRN9v0BuAUzKsJxMvOLwRgMfWbS9xJWZmw5NlECwB5kuaJ6kaWAQsLm4gaUbR5PnAigzrycQLpqdB8Oy2EldiZjY8mQ06FxHdki4F7gBywNcjYrmkK4HWiFgMvF/S+UA3sAl4Z1b1ZGVyQzXNjTU8vs5fITWz8pTp6KMRcTtw+4B5Hy96fwVwRZY1jIa5U+v5w6ZdpS7DzGxYSn2xeFyYM7mets27S12GmdmwOAhGwOwp9azdupvObg9LbWblx0EwAo6YUk8ErN3iXoGZlR8HwQiYNakOcBCYWXlyEIyAw5qSR1au3+6bysys/DgIRsBhjUkQrNvWUeJKzMwOnYNgBEyoyVNXyLlHYGZlyUEwAiQxvanGQWBmZclBMEIOa6z1qSEzK0sOghHS3FjjB9mbWVlyEIyQSfUFtuzqKnUZZmaHzEEwQibXV7NlVye9vX5AjZmVFwfBCJlUX6A3YHtHd6lLMTM7JA6CETKpvhqAzbs6S1yJmdmhcRCMkMn1BcBBYGblx0EwQvp6BFt2+4KxmZUXB8EI6esRbHGPwMzKjINghPRfI9jpHoGZlRcHwQiZWFdAco/AzMpPpkEg6WxJj0laKenyA7T7U0khqSXLerKUqxJNtQU2+6YyMyszmQWBpBxwHXAOsAC4SNKCQdo1Ah8AHsiqltEyub7gi8VmVnay7BGcBqyMiFUR0QncDFwwSLt/BD4LlP2IbZPSu4vNzMpJlkEwC1hdNN2Wzusn6WRgTkT8+EArknSJpFZJre3t7SNf6QhpqiuwzT0CMyszJbtYLKkK+DzwoYO1jYjrI6IlIlqam5uzL26YGmvybN/jISbMrLxkGQRrgDlF07PTeX0ageOBn0t6CngpsLicLxg31uY91pCZlZ0sg2AJMF/SPEnVwCJgcd/CiNgaEdMiYm5EzAXuB86PiNYMa8pUEgQ+NWRm5SWzIIiIbuBS4A5gBfDdiFgu6UpJ52e13VJqrC3Q0dVLV09vqUsxMxuyfJYrj4jbgdsHzPv4ftqemWUto2FCTfLPuaOjm8kN1SWuxsxsaIbUI5D07aHMq3SNtUkQ+DqBmZWToZ4aOq54Ir1Z7JSRL6e8NdYmA89t3+PrBGZWPg4YBJKukLQdOFHStvS1HVgP/OeoVFhG3CMws3J0wCCIiKsiohG4OiKa0ldjREyNiCtGqcay4SAws3I01FNDP5LUACDpzyR9XtKRGdZVlvpODe3wqSEzKyNDDYIvA7sknURyJ/ATwLcyq6pM9X1ryD0CMysnQw2C7ogIkkHjvhgR15HcGWxFfGrIzMrRUO8j2C7pCuDPgVek4wQVsiurPNUWclTnqhwEZlZWhtojuBDYA7wrIp4lGTfo6syqKmMeZsLMys2QgiD95X8jMFHSeUBHRPgawSAmeOA5MyszQ72z+K3Ar4G3AG8FHpD05iwLK1eNtXl2eChqMysjQ71G8DHg1IhYDyCpGfgZcGtWhZWrxpqCTw2ZWVkZ6jWCqr4QSG08hM9WFJ8aMrNyM9QewX9LugO4KZ2+kAGjilrCD6cxs3JzwCCQdAwwPSI+IulNwOnpol+RXDy2AZpqfWrIzMrLwU7vXANsA4iI70fEZRFxGXBbuswGmFCTXCxO7r8zMxv7DhYE0yPidwNnpvPmZlJRmWuszdMbsKuzp9SlmJkNycGCYNIBltWNZCHjRf8zCXydwMzKxMGCoFXSewfOlPQeYGk2JZW3veMN+TqBmZWHg31r6IPAbZIuZu8v/hagGnhjloWVqwl9QeCbysysTBzswTTrIuLlwCeBp9LXJyPiZemwEwck6WxJj0laKenyQZa/T9LvJC2TdJ+kBcPbjbGjKQ2CbbvdIzCz8jCk+wgi4m7g7kNZcfpc4+uA1wBtwBJJiyPikaJm34mIr6Ttzwc+D5x9KNsZa5p8jcDMykyWdwefBqyMiFUR0QncTPI8g34Rsa1osgEo++9cNtUlQbDN1wjMrEwM9c7i4ZgFrC6abgNeMrCRpL8GLiO57vCqwVYk6RLgEoAjjjhixAsdSX09gm273SMws/JQ8vGCIuK6iDga+Dvg7/fT5vqIaImIlubm5tEt8BDVFqoo5OQegZmVjSyDYA0wp2h6djpvf24G3pBhPaNCEk21BV8sNrOykWUQLAHmS5onqRpYBCwubiBpftHknwCPZ1jPqGmqK7DNF4vNrExkdo0gIrolXQrcAeSAr0fEcklXAq0RsRi4VNJZQBewGXhHVvWMpiY/rtLMykiWF4uJiNsZMFx1RHy86P0Hstx+qTTV+dSQmZWPkl8sHo+aan1qyMzKh4MgA011efcIzKxsOAgykPQIHARmVh4cBBlorM3T0dXLnm4/k8DMxj4HQQb6hpnweENmVg4cBBnYO8yETw+Z2djnIMjAxPokCDbvchCY2djnIMjAlPpqADbv7CxxJWZmB+cgyMCUhiQINjkIzKwMOAgyMHVCGgS7HARmNvY5CDJQV8hRk69yj8DMyoKDIAOSmNpQzcYdDgIzG/scBBmZMqGazT41ZGZlwEGQkcn11Wz0qSEzKwMOgoxMbahm0849pS7DzOygHAQZmdJQw+advqHMzMY+B0FGpjQU2LGn2wPPmdmY5yDIyNQJNQBs8DeHzGyMcxBk5PCJtQA8u7WjxJWYmR1YpkEg6WxJj0laKenyQZZfJukRSQ9JulPSkVnWM5pmpEHwzNbdJa7EzOzAMgsCSTngOuAcYAFwkaQFA5r9FmiJiBOBW4HPZVXPaJsxsQ5wj8DMxr4sewSnASsjYlVEdAI3AxcUN4iIuyNiVzp5PzA7w3pGVVNtnvrqHGu3OAjMbGzLMghmAauLptvSefvzbuC/Blsg6RJJrZJa29vbR7DE7EhixsRant3mU0NmNraNiYvFkv4MaAGuHmx5RFwfES0R0dLc3Dy6xT0PMybWuUdgZmNelkGwBphTND07nbcPSWcBHwPOj4hxdSvujIm1vkZgZmNelkGwBJgvaZ6kamARsLi4gaQXA18lCYH1GdZSEjMm1bF+ewed3b2lLsXMbL8yC4KI6AYuBe4AVgDfjYjlkq6UdH7a7GpgAvA9ScskLd7P6srS3Kn19Ab8YdOugzc2MyuRfJYrj4jbgdsHzPt40fuzstx+qR3VPAGAVe07OOawCSWuxsxscGPiYvF4dVRzAwCrNuwscSVmZvvnIMhQU22B5sYanli/o9SlmJntl4MgY0c3N7hHYGZjmoMgY0c3T+DxdduJiFKXYmY2KAdBxo6bOZFtHd3+5pCZjVkOgoydNGciAA+2bS1xJWZmg3MQZOzY6Y3U5Kt4aPWWUpdiZjYoB0HGCrkqjpvZxINtDgIzG5scBKOgZe4UHly9ld2dfn6xmY09DoJRcPox0+js6eWBJzeWuhQzs+dwEIyC0+ZNoTpfxX2Pbyh1KWZmz+EgGAW1hRwvmTeFOx9d7/sJzGzMcRCMknNPmMGTG3ayfO22UpdiZrYPB8EoOef4w8lXicUPri11KWZm+3AQjJJJ9dWccWwzP3xwLT29Pj1kZmOHg2AUveWU2TyztYOfrVhX6lLMzPo5CEbRaxZMZ9akOv7tF0+WuhQzs34OglGUz1Xx9pcdyf2rNvHwGo89ZGZjg4NglF30kiOYWFfgCz/9falLMTMDHASjrqm2wF++8ijufHQ9v/nD5lKXY2aWbRBIOlvSY5JWSrp8kOVnSPqNpG5Jb86ylrHkHS+by9SGaj77X4/6BjMzK7nMgkBSDrgOOAdYAFwkacGAZn8A3gl8J6s6xqKGmjyXvfZYHnhyEz986JlSl2NmFS7LHsFpwMqIWBURncDNwAXFDSLiqYh4COjNsI4xadGpR3D8rCb+748fYeee7lKXY2YVLMsgmAWsLppuS+cdMkmXSGqV1Nre3j4ixZVarkp88vzjWbdtD9fe9XipyzGzClYWF4sj4vqIaImIlubm5lKXM2JOOXIyb22ZzQ33Pumvk5pZyWQZBGuAOUXTs9N5VuRj5y5gakM1H/7eg3R2V9wZMjMbA7IMgiXAfEnzJFUDi4DFGW6vLE2sL3DVm07g0We380WfIjKzEsgsCCKiG7gUuANYAXw3IpZLulLS+QCSTpXUBrwF+Kqk5VnVM5a9+kXTedPJs7ju50+wzA+5N7NRpnL7HntLS0u0traWuowRt3VXF+deey8AP37/6Uyqry5xRWY2nkhaGhEtgy0ri4vFlWBifYHrLj6Z9ds7+NB3H6TXQ1Wb2ShxEIwhC+dM4mPnvog7H13PNXf6eoGZjY58qQuwfb3j5XNZvnYb1975ONObarj4JUeWuiQzG+ccBGOMJK560wls3NnJ//nBw9Tkc7z5lNmlLsvMxjGfGhqD8rkqrnvbybz86Gl8+HsPcsO9qzw4nZllxkEwRtVV5/jXd7bwJyfM4FM/XsGHvvsguzo9JpGZjTwHwRhWk89x7UUv5m/POpbblq3hvGvv45crN5S6LDMbZxwEY1yuSnzgrPnc+O6X0N0bvO2GB/ibm37LqvYdpS7NzMYJ31BWRjq6evjSz5/g+nueoLO7l/NOnMm7T5/HibMnIqnU5ZnZGHagG8ocBGWoffsebrhvFf/+q6fZ2dnDCw9v5MJT53DO8TM4fGJtqcszszHIQTBObevoYvGytXy3dTUPtSXDWJ80eyKvWTCdVx57GC+a0Ug+57N/ZuYgqAiPr9vOTx5Zx08fWdc/cN2EmjynHDmZlxw1hYVzJnHczIlMrCuUuFIzKwUHQYVZv72D+1dt4oFVG3ngyU2sXL/3wvKcKXUcP3Mix81sYv70Ro5ubuCIKQ1U591zMBvPHAQVbuOOPfxuzVaWr93G8rXJz6c37upfnqsScybXcVTzBI6a1sDsyXXMmlzPzEm1zJ5UT1Nd3hejzcrcgYLAQ0xUgKkTajjzBYdx5gsO65+3vaOLVe07WbVhR/KzfSdPtO/gl09soKNr3yelNVTnmDW5jpmT6ji8qZbmxprkNaFm7/vGGuqr/Z+TWTny/7kVqrG2wElzJnHSnEn7zI8INu7sZM3m3azdsps1W3bTVvR++dptbNyxh8FGyW6oztHcWMOUhmom11czqb6aSfUFJtcXit7v+7OukHNvw6zEHAS2D0lMm1DDtAk1zwmJPj29waadnbRv30P7jj3Jz/S1fnsHm3d18szWDlY8s40tu7vY1dmz3+1V56porM0zoTaf/KzJ01hboLEmXzS/kM7vexVoqM5TX52jvjpHXXWO+uo8uSoHitlwOAjskOWq1H86aCg6unrYuruLLbu62Lyrky27OtP3XWzZ3cmOjm527Olme0c3Ozq6Wb1pV//09o6uQXsfg6nOV1FfnaOhOp+GQ466Qi4NjKJ51TnqC0mQ1BaqqMnnqCn6Wds/nczbt00V1bkq92JsXHEQWOZqCzlqCzmmNx36zW4Rwe6unjQUkmDYsScJjF2dPezq6mF3Z/J+d2dPMq+zh91d3f3vN+zoZFfnLnZ39rAzbdfZ03vwje+HxHNDIl/VHybVuSoK+Sqqc6KQq+p/VecHTOdEvn9aVOeripYrWU+6rn2m0+X5XBX5KpGr0t6fg8xzaNnBZBoEks4G/hnIATdExGcGLK8BvgWcAmwELoyIp7KsycqLJOqr89RX55neNHLr7e7pZVdXD3u6etnT3cOe7l46upKfffM6ipbt6VuWvu8YMK/vsx1dPXT3BLt3d9HV05u+gs7u3n2ne3rp7B5+GB2K3IBgKOSq9g2P/p/p/FzabsD0c9pViaoqUaVkG1VKXn3vc1Uk86pETkm7/vcD2/R9bpC2SZil+6Ek2JJ94jnb7K+lfz2iqmpvfSL5b6pKyWc14Gff+32mSdv0bw9E8Wf2rrNcQzezIJCUA64DXgO0AUskLY6IR4qavRvYHBHHSFoEfBa4MKuazPrkc1U05aqghCNyRAQ9vdEfDH1B0V083b3vsq6eXjq7g66eXnp6g+7eoKc3CZfi6e7eoKenbzr52d3Tu890f7u+6bR9d2+67nSde7p7ij6z77oikmtGPRH9+9PTG/QG9Ebf+73zKkFxyDaRxIwAAAd8SURBVOwTOhSFTlXx9P6DaeDPD7x6Pq8/aeaI15xlj+A0YGVErAKQdDNwAVAcBBcAn0jf3wp8UZKi3G5uMBsGKflrO5+DOnKlLmdU9Kah0RtBby/0pCHRHyJRFC5piPSm071RFCrpZ5P1DPLZ/mDa+9lIwylIQrivhiCdH3sDLILnTBf/7A0I0unevdO9UbTuoGg9e6eLt7XPZ3r31jfYNiPIbGSALINgFrC6aLoNeMn+2kREt6StwFRgn0H3JV0CXAJwxBFHZFWvmWWsqkpUUZ6nT8azshhXICKuj4iWiGhpbm4udTlmZuNKlkGwBphTND07nTdoG0l5YCLJRWMzMxslWQbBEmC+pHmSqoFFwOIBbRYD70jfvxm4y9cHzMxGV2bXCNJz/pcCd5B8ffTrEbFc0pVAa0QsBv4V+LaklcAmkrAwM7NRlOl9BBFxO3D7gHkfL3rfAbwlyxrMzOzAyuJisZmZZcdBYGZW4RwEZmYVruyeUCapHXh6mB+fxoCb1SqA97kyeJ8rw/PZ5yMjYtAbscouCJ4PSa37e1TbeOV9rgze58qQ1T771JCZWYVzEJiZVbhKC4LrS11ACXifK4P3uTJkss8VdY3AzMyeq9J6BGZmNoCDwMyswlVEEEg6W9JjklZKurzU9YwUSXMk3S3pEUnLJX0gnT9F0k8lPZ7+nJzOl6Rr03+HhySdXNo9GD5JOUm/lfSjdHqepAfSfbslHfEWSTXp9Mp0+dxS1j1ckiZJulXSo5JWSHrZeD/Okv42/e/6YUk3Saodj8dZ0tclrZf0cNG8Qz62kt6Rtn9c0jsG29b+jPsgKHp28jnAAuAiSQtKW9WI6QY+FBELgJcCf53u2+XAnRExH7gznYbk32B++roE+PLolzxiPgCsKJr+LPCFiDgG2EzyPGwoei428IW0XTn6Z+C/I+KFwEkk+z5uj7OkWcD7gZaIOJ5kBOO+55qPt+P8DeDsAfMO6dhKmgL8A8lTIE8D/qEvPIYk0udnjtcX8DLgjqLpK4ArSl1XRvv6n8BrgMeAGem8GcBj6fuvAhcVte9vV04vkocc3Qm8CvgRIJK7LfMDjznJMOgvS9/n03Yq9T4c4v5OBJ4cWPd4Ps7sfYztlPS4/Qh43Xg9zsBc4OHhHlvgIuCrRfP3aXew17jvETD4s5NnlaiWzKRd4RcDDwDTI+KZdNGzwPT0/Xj5t7gG+CjQm05PBbZERHc6Xbxf+zwXG+h7LnY5mQe0A/+Wng67QVID4/g4R8Qa4P8BfwCeITluSxnfx7nYoR7b53XMKyEIxj1JE4D/AD4YEduKl0Xy58G4+Y6wpPOA9RGxtNS1jKI8cDLw5Yh4MbCTvacKgHF5nCcDF5CE4EyggeeePqkIo3FsKyEIhvLs5LIlqUASAjdGxPfT2eskzUiXzwDWp/PHw7/FHwHnS3oKuJnk9NA/A5PS517Dvvs1Hp6L3Qa0RcQD6fStJMEwno/zWcCTEdEeEV3A90mO/Xg+zsUO9dg+r2NeCUEwlGcnlyVJInnc54qI+HzRouJnQb+D5NpB3/y3p988eCmwtaj7WRYi4oqImB0Rc0mO5V0RcTFwN8lzr+G5+1zWz8WOiGeB1ZJekM56NfAI4/g4k5wSeqmk+vS/8759HrfHeYBDPbZ3AK+VNDntTb02nTc0pb5IMkoXYs4Ffg88AXys1PWM4H6dTtJlfAhYlr7OJTk3eifwOPAzYEraXiTfoHoC+B3JNzJKvh/PY//PBH6Uvj8K+DWwEvgeUJPOr02nV6bLjyp13cPc14VAa3qsfwBMHu/HGfgk8CjwMPBtoGY8HmfgJpLrIF0kvb93D+fYAu9K938l8BeHUoOHmDAzq3CVcGrIzMwOwEFgZlbhHARmZhXOQWBmVuEcBGZmFc5BYBVL0o7051xJbxvhdf/vAdO/HMn1m40kB4FZMuDXIQVB0d2t+7NPEETEyw+xJrNR4yAwg88Ar5C0LB0DPyfpaklL0jHf/xJA0pmS7pW0mOQuVyT9QNLSdNz8S9J5nwHq0vXdmM7r630oXffDkn4n6cKidf9ce585cGN6R61Z5g72V41ZJbgc+HBEnAeQ/kLfGhGnSqoBfiHpJ2nbk4HjI+LJdPpdEbFJUh2wRNJ/RMTlki6NiIWDbOtNJHcJnwRMSz9zT7rsxcBxwFrgFyRj69w38rtrti/3CMye67Uk47ksIxnWeyrJg0AAfl0UAgDvl/QgcD/JoF/zObDTgZsioici1gH/A5xatO62iOglGS5k7ojsjdlBuEdg9lwC/iYi9hm0S9KZJENAF0+fRfJAlF2Sfk4y5s1w7Sl634P//7RR4h6BGWwHGoum7wD+Kh3iG0nHpg+CGWgiyeMRd0l6IcnjQvt09X1+gHuBC9PrEM3AGSSDpJmVjP/iMEtG9OxJT/F8g+T5BnOB36QXbNuBNwzyuf8G3idpBckjA+8vWnY98JCk30QyTHaf20gesfggycixH42IZ9MgMSsJjz5qZlbhfGrIzKzCOQjMzCqcg8DMrMI5CMzMKpyDwMyswjkIzMwqnIPAzKzC/X/VhY+t6M1O+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets see if our algorithm is working.  We should see a declining learning curve with iteration, which eventually flatterns out\n",
    "#This will help us determine the appropriate number of iterations to run to determine the appropriate parameters\n",
    "#Note: we will use a learning rate of 0.5 for now\n",
    "\n",
    "W1,W2,W3,b1,b2,b3,count,caches = gradient_descent(1000,X_cv,Y_cv,0.5)\n",
    "\n",
    "plt.plot(count,caches,label='Cost')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "\n",
    "plt.title(\"Cost vs. Iteration\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6983870729400153,\n",
       " 0.6488987075799445,\n",
       " 0.6148245932379022,\n",
       " 0.5906973367855283,\n",
       " 0.5735725435902469,\n",
       " 0.5615571274547415,\n",
       " 0.5532817599854452,\n",
       " 0.5477017934116957,\n",
       " 0.5440170738239696,\n",
       " 0.5416288183292965,\n",
       " 0.5401039772853271,\n",
       " 0.5391403533462392,\n",
       " 0.5385338868412753,\n",
       " 0.5381506384318255,\n",
       " 0.5379047321860825,\n",
       " 0.5377421330233481,\n",
       " 0.5376293309182597,\n",
       " 0.537545763258035,\n",
       " 0.537478896756451,\n",
       " 0.5374211109971585,\n",
       " 0.5373677593802484,\n",
       " 0.5373159792851057,\n",
       " 0.5372639695558827,\n",
       " 0.5372105551205987,\n",
       " 0.5371549260212268,\n",
       " 0.53709648144976,\n",
       " 0.5370347365551662,\n",
       " 0.5369692665389934,\n",
       " 0.536899672756787,\n",
       " 0.5368255616926811,\n",
       " 0.5367465313553077,\n",
       " 0.536662161832278,\n",
       " 0.5365720080346517,\n",
       " 0.5364755934225423,\n",
       " 0.5363724039444449,\n",
       " 0.5362618816747973,\n",
       " 0.5361434177730757,\n",
       " 0.536016344458382,\n",
       " 0.5358799257223569,\n",
       " 0.5357333465055555,\n",
       " 0.5355757000466634,\n",
       " 0.5354059730845687,\n",
       " 0.5352230285522694,\n",
       " 0.5350255853491286,\n",
       " 0.534812194713194,\n",
       " 0.5345812126363545,\n",
       " 0.5343307676693982,\n",
       " 0.5340587233482573,\n",
       " 0.5337626343327492,\n",
       " 0.5334396951800893,\n",
       " 0.5330866804715199,\n",
       " 0.5326998747647657,\n",
       " 0.5322749905495602,\n",
       " 0.5318070720286152,\n",
       " 0.5312903821206141,\n",
       " 0.5307182695712568,\n",
       " 0.5300830124462739,\n",
       " 0.5293756335462019,\n",
       " 0.5285856824019651,\n",
       " 0.5277009774540669,\n",
       " 0.5267073007548809,\n",
       " 0.5255880360341741,\n",
       " 0.5243237392194757,\n",
       " 0.5228916285372949,\n",
       " 0.5212649792741085,\n",
       " 0.5194124064952403,\n",
       " 0.5172970182515263,\n",
       " 0.514875423488346,\n",
       " 0.512096585566272,\n",
       " 0.508900528206919,\n",
       " 0.505216932146931,\n",
       " 0.5009637165656676,\n",
       " 0.4960457900945301,\n",
       " 0.49035429257904956,\n",
       " 0.4837668372871684,\n",
       " 0.4761494987719646,\n",
       " 0.46736154430138405,\n",
       " 0.45726410100842235,\n",
       " 0.44573393367472997,\n",
       " 0.43268301635213613,\n",
       " 0.41808325538526264,\n",
       " 0.40199328133495543,\n",
       " 0.3845809403398029,\n",
       " 0.36613241871634167,\n",
       " 0.3470394090953682,\n",
       " 0.32776139993293185,\n",
       " 0.30876978064545485,\n",
       " 0.29048882031828727,\n",
       " 0.2732501656675377,\n",
       " 0.2572711940929934,\n",
       " 0.24265757571977012,\n",
       " 0.22942262907167976,\n",
       " 0.21751362191293158,\n",
       " 0.20683719346517704,\n",
       " 0.1972797515527282,\n",
       " 0.18872180354367898,\n",
       " 0.18104693917095688,\n",
       " 0.17414680384677847,\n",
       " 0.16792337393360882,\n",
       " 0.1622895665479135,\n",
       " 0.1571689051432242,\n",
       " 0.1524947059454389,\n",
       " 0.14820906690723076,\n",
       " 0.1442618198925601,\n",
       " 0.140609531384999,\n",
       " 0.13721459202547556,\n",
       " 0.13404440955072547,\n",
       " 0.13107070569619858,\n",
       " 0.12826891054722495,\n",
       " 0.12561764471631665,\n",
       " 0.12309827881970759,\n",
       " 0.12069455997966641,\n",
       " 0.1183922958890886,\n",
       " 0.11617908800229713,\n",
       " 0.114044106476082,\n",
       " 0.11197790048256379,\n",
       " 0.10997223840649957,\n",
       " 0.10801997320894122,\n",
       " 0.10611492888727682,\n",
       " 0.1042518044975924,\n",
       " 0.1024260926423381,\n",
       " 0.10063400968013558,\n",
       " 0.09887243520226537,\n",
       " 0.09713885855991188,\n",
       " 0.09543133043631159,\n",
       " 0.09374841765750064,\n",
       " 0.09208915964283049,\n",
       " 0.09045302512841275,\n",
       " 0.0888398680660698,\n",
       " 0.08724988191419622,\n",
       " 0.08568355189400996,\n",
       " 0.08414160517401068,\n",
       " 0.08262495934583237,\n",
       " 0.08113466993602268,\n",
       " 0.07967187802525566,\n",
       " 0.07823775928389143,\n",
       " 0.07683347585196022,\n",
       " 0.07546013247640294,\n",
       " 0.07411873816907266,\n",
       " 0.0728101743832857,\n",
       " 0.0715351703570249,\n",
       " 0.07029428587886684,\n",
       " 0.06908790134212722,\n",
       " 0.06791621460272808,\n",
       " 0.0667792438761749,\n",
       " 0.06567683571553364,\n",
       " 0.06460867700950747,\n",
       " 0.0635743099210202,\n",
       " 0.062573148737983,\n",
       " 0.06160449771100759,\n",
       " 0.060667569088752676,\n",
       " 0.05976150071293193,\n",
       " 0.05888537268741557,\n",
       " 0.05803822277868078,\n",
       " 0.057219060331176796,\n",
       " 0.05642687858730013,\n",
       " 0.05566066538656719,\n",
       " 0.054919412283038815,\n",
       " 0.05420212216612026,\n",
       " 0.05350781550019841,\n",
       " 0.052835535316070055,\n",
       " 0.05218435109456934,\n",
       " 0.05155336168276142,\n",
       " 0.05094169737772324,\n",
       " 0.050348521304077715,\n",
       " 0.049773030200509674,\n",
       " 0.0492144547185506,\n",
       " 0.04867205932476474,\n",
       " 0.04814514188565465,\n",
       " 0.04763303300348272,\n",
       " 0.047135095160998404,\n",
       " 0.04665072172386788,\n",
       " 0.046179335841456115,\n",
       " 0.045720389279485775,\n",
       " 0.04527336121193457,\n",
       " 0.044837756994256704,\n",
       " 0.044413106935535886,\n",
       " 0.043998965083407934,\n",
       " 0.043594908032443404,\n",
       " 0.043200533764069696,\n",
       " 0.0428154605239646,\n",
       " 0.042439325741099036,\n",
       " 0.04207178499118526,\n",
       " 0.04171251100614605,\n",
       " 0.04136119273031267,\n",
       " 0.04101753442334646,\n",
       " 0.040681254809326585,\n",
       " 0.04035208627102493,\n",
       " 0.040029774088076966,\n",
       " 0.03971407571753078,\n",
       " 0.03940476011510161,\n",
       " 0.03910160709536037,\n",
       " 0.038804406729030294,\n",
       " 0.03851295877554734,\n",
       " 0.038227072149047865,\n",
       " 0.03794656441597718,\n",
       " 0.037671261322556694,\n",
       " 0.037400996350404996,\n",
       " 0.037135610298671386,\n",
       " 0.0368749508911117,\n",
       " 0.0366188724066079,\n",
       " 0.03636723533170891,\n",
       " 0.03611990603384359,\n",
       " 0.035876756453931395,\n",
       " 0.035637663817188615,\n",
       " 0.03540251036099772,\n",
       " 0.03517118307877532,\n",
       " 0.03494357347883875,\n",
       " 0.03471957735733362,\n",
       " 0.03449909458434202,\n",
       " 0.03428202890234838,\n",
       " 0.034068287736290885,\n",
       " 0.033857782014476784,\n",
       " 0.033650425999685644,\n",
       " 0.03344613712982897,\n",
       " 0.033244835867574915,\n",
       " 0.03304644555838525,\n",
       " 0.03285089229644786,\n",
       " 0.03265810479802125,\n",
       " 0.032468014281738615,\n",
       " 0.03228055435544912,\n",
       " 0.03209566090920008,\n",
       " 0.03191327201399026,\n",
       " 0.03173332782594766,\n",
       " 0.03155577049560741,\n",
       " 0.03138054408198622,\n",
       " 0.031207594471168883,\n",
       " 0.031036869299140672,\n",
       " 0.03086831787861572,\n",
       " 0.030701891129627616,\n",
       " 0.03053754151366297,\n",
       " 0.030375222971132192,\n",
       " 0.03021489086198459,\n",
       " 0.030056501909286843,\n",
       " 0.029900014145595236,\n",
       " 0.029745386861961754,\n",
       " 0.029592580559425014,\n",
       " 0.02944155690284492,\n",
       " 0.029292278676949494,\n",
       " 0.029144709744469588,\n",
       " 0.028998815006245095,\n",
       " 0.02885456036319304,\n",
       " 0.028711912680034837,\n",
       " 0.028570839750685664,\n",
       " 0.028431310265215202,\n",
       " 0.02829329377829404,\n",
       " 0.028156760679045354,\n",
       " 0.02802168216222613,\n",
       " 0.027888030200666693,\n",
       " 0.02775577751890166,\n",
       " 0.0276248975679291,\n",
       " 0.027495364501038733,\n",
       " 0.027367153150653325,\n",
       " 0.02724023900613039,\n",
       " 0.027114598192475404,\n",
       " 0.026990207449919208,\n",
       " 0.02686704411431611,\n",
       " 0.026745086098321522,\n",
       " 0.026624311873309773,\n",
       " 0.026504700451995898,\n",
       " 0.02638623137172648,\n",
       " 0.026268884678407496,\n",
       " 0.026152640911037854,\n",
       " 0.02603748108682034,\n",
       " 0.02592338668682227,\n",
       " 0.025810339642160427,\n",
       " 0.025698322320685776,\n",
       " 0.0255873175141453,\n",
       " 0.025477308425799264,\n",
       " 0.025368278658473437,\n",
       " 0.025260212203027148,\n",
       " 0.0251530934272189,\n",
       " 0.025046907064952276,\n",
       " 0.024941638205886064,\n",
       " 0.024837272285392942,\n",
       " 0.024733795074852457,\n",
       " 0.024631192672264175,\n",
       " 0.024529451493168212,\n",
       " 0.024428558261860717,\n",
       " 0.024328500002892293,\n",
       " 0.024229264032838708,\n",
       " 0.024130837952332656,\n",
       " 0.02403320963834727,\n",
       " 0.023936367236721146,\n",
       " 0.023840299154916166,\n",
       " 0.023744994054999444,\n",
       " 0.023650440846841023,\n",
       " 0.023556628681519486,\n",
       " 0.023463546944928186,\n",
       " 0.023371185251574635,\n",
       " 0.023279533438566525,\n",
       " 0.023188581559777658,\n",
       " 0.023098319880187575,\n",
       " 0.02300873887038908,\n",
       " 0.02291982920125779,\n",
       " 0.022831581738778146,\n",
       " 0.022743987539021005,\n",
       " 0.022657037843267402,\n",
       " 0.02257072407327379,\n",
       " 0.0224850378266743,\n",
       " 0.02239997087251522,\n",
       " 0.022315515146917732,\n",
       " 0.022231662748864643,\n",
       " 0.022148405936107143,\n",
       " 0.02206573712118781,\n",
       " 0.021983648867576294,\n",
       " 0.021902133885913756,\n",
       " 0.021821185030363396,\n",
       " 0.021740795295062804,\n",
       " 0.021660957810675924,\n",
       " 0.021581665841040838,\n",
       " 0.021502912779910843,\n",
       " 0.021424692147785813,\n",
       " 0.02134699758883111,\n",
       " 0.021269822867881375,\n",
       " 0.021193161867526636,\n",
       " 0.02111700858527833,\n",
       " 0.021041357130812656,\n",
       " 0.02096620172328909,\n",
       " 0.020891536688741785,\n",
       " 0.020817356457541635,\n",
       " 0.020743655561926932,\n",
       " 0.02067042863360062,\n",
       " 0.020597670401392067,\n",
       " 0.020525375688981565,\n",
       " 0.02045353941268573,\n",
       " 0.020382156579301768,\n",
       " 0.02031122228400929,\n",
       " 0.020240731708327655,\n",
       " 0.020170680118127345,\n",
       " 0.020101062861693792,\n",
       " 0.02003187536784223,\n",
       " 0.019963113144081918,\n",
       " 0.019894771774828415,\n",
       " 0.019826846919662584,\n",
       " 0.019759334311634822,\n",
       " 0.0196922297556134,\n",
       " 0.019625529126675447,\n",
       " 0.0195592283685396,\n",
       " 0.019493323492038948,\n",
       " 0.019427810573633245,\n",
       " 0.019362685753959143,\n",
       " 0.019297945236417587,\n",
       " 0.019233585285797064,\n",
       " 0.019169602226932026,\n",
       " 0.01910599244339498,\n",
       " 0.019042752376222143,\n",
       " 0.01897987852267067,\n",
       " 0.018917367435007555,\n",
       " 0.018855215719328657,\n",
       " 0.018793420034407385,\n",
       " 0.018731977090572064,\n",
       " 0.018670883648611233,\n",
       " 0.018610136518706077,\n",
       " 0.018549732559389326,\n",
       " 0.01848966867652984,\n",
       " 0.018429941822342255,\n",
       " 0.018370548994420723,\n",
       " 0.0183114872347967,\n",
       " 0.01825275362901935,\n",
       " 0.01819434530525866,\n",
       " 0.018136259433430193,\n",
       " 0.018078493224341047,\n",
       " 0.01802104392885662,\n",
       " 0.01796390883708729,\n",
       " 0.01790708527759474,\n",
       " 0.01785057061661734,\n",
       " 0.017794362257314008,\n",
       " 0.01773845763902622,\n",
       " 0.017682854236557376,\n",
       " 0.01762754955946961,\n",
       " 0.017572541151396906,\n",
       " 0.017517826589374653,\n",
       " 0.01746340348318499,\n",
       " 0.017409269474717395,\n",
       " 0.017355422237344383,\n",
       " 0.01730185947531177,\n",
       " 0.01724857892314318,\n",
       " 0.01719557834505827,\n",
       " 0.017142855534404627,\n",
       " 0.017090408313102688,\n",
       " 0.017038234531103487,\n",
       " 0.016986332065858877,\n",
       " 0.016934698821804022,\n",
       " 0.016883332729851402,\n",
       " 0.01683223174689686,\n",
       " 0.016781393855336436,\n",
       " 0.016730817062594393,\n",
       " 0.016680499400661918,\n",
       " 0.01663043892564622,\n",
       " 0.016580633717329672,\n",
       " 0.016531081878739022,\n",
       " 0.016481781535724143,\n",
       " 0.01643273083654619,\n",
       " 0.016383927951474916,\n",
       " 0.016335371072395046,\n",
       " 0.016287058412421125,\n",
       " 0.016238988205521127,\n",
       " 0.016191158706148144,\n",
       " 0.016143568188880283,\n",
       " 0.016096214948068375,\n",
       " 0.016049097297491356,\n",
       " 0.01600221357001924,\n",
       " 0.015955562117283262,\n",
       " 0.01590914130935324,\n",
       " 0.015862949534421907,\n",
       " 0.015816985198495937,\n",
       " 0.015771246725093696,\n",
       " 0.015725732554949304,\n",
       " 0.015680441145723232,\n",
       " 0.015635370971718774,\n",
       " 0.015590520523604712,\n",
       " 0.015545888308143737,\n",
       " 0.015501472847926653,\n",
       " 0.015457272681112027,\n",
       " 0.015413286361171492,\n",
       " 0.015369512456640156,\n",
       " 0.01532594955087222,\n",
       " 0.015282596241801873,\n",
       " 0.015239451141708816,\n",
       " 0.015196512876988852,\n",
       " 0.015153780087928989,\n",
       " 0.015111251428487389,\n",
       " 0.015068925566077515,\n",
       " 0.01502680118135692,\n",
       " 0.01498487696802015,\n",
       " 0.014943151632595868,\n",
       " 0.01490162389424822,\n",
       " 0.014860292484581848,\n",
       " 0.014819156147451187,\n",
       " 0.014778213638773335,\n",
       " 0.014737463726344662,\n",
       " 0.014696905189661207,\n",
       " 0.014656536819742386,\n",
       " 0.014616357418958404,\n",
       " 0.01457636580086099,\n",
       " 0.014536560790017285,\n",
       " 0.014496941221847293,\n",
       " 0.014457505942464196,\n",
       " 0.014418253808517934,\n",
       " 0.014379183687041793,\n",
       " 0.014340294455301961,\n",
       " 0.014301585000649864,\n",
       " 0.014263054220377536,\n",
       " 0.014224701021575628,\n",
       " 0.01418652432099414,\n",
       " 0.014148523044905782,\n",
       " 0.014110696128972079,\n",
       " 0.014073042518111766,\n",
       " 0.014035561166371925,\n",
       " 0.013998251036801327,\n",
       " 0.01396111110132629,\n",
       " 0.013924140340628801,\n",
       " 0.013887337744026982,\n",
       " 0.013850702309357699,\n",
       " 0.013814233042861347,\n",
       " 0.013777928959068897,\n",
       " 0.013741789080690877,\n",
       " 0.01370581243850852,\n",
       " 0.01366999807126675,\n",
       " 0.013634345025569386,\n",
       " 0.013598852355776014,\n",
       " 0.013563519123900854,\n",
       " 0.013528344399513509,\n",
       " 0.013493327259641396,\n",
       " 0.013458466788674078,\n",
       " 0.013423762078269133,\n",
       " 0.013389212227260038,\n",
       " 0.013354816341565297,\n",
       " 0.0133205735340996,\n",
       " 0.01328648292468624,\n",
       " 0.013252543639971344,\n",
       " 0.013218754813339447,\n",
       " 0.013185115584830702,\n",
       " 0.013151625101059463,\n",
       " 0.013118282515134331,\n",
       " 0.013085086986579657,\n",
       " 0.013052037681258354,\n",
       " 0.013019133771296123,\n",
       " 0.012986374435006923,\n",
       " 0.01295375885681989,\n",
       " 0.012921286227207333,\n",
       " 0.012888955742614174,\n",
       " 0.012856766605388467,\n",
       " 0.012824718023713141,\n",
       " 0.012792809211539052,\n",
       " 0.012761039388519003,\n",
       " 0.012729407779942973,\n",
       " 0.012697913616674508,\n",
       " 0.012666556135088132,\n",
       " 0.012635334577007782,\n",
       " 0.012604248189646446,\n",
       " 0.012573296225546654,\n",
       " 0.012542477942522038,\n",
       " 0.01251179260359994,\n",
       " 0.012481239476964832,\n",
       " 0.012450817835902888,\n",
       " 0.012420526958747279,\n",
       " 0.012390366128824519,\n",
       " 0.012360334634401627,\n",
       " 0.012330431768634222,\n",
       " 0.012300656829515382,\n",
       " 0.012271009119825517,\n",
       " 0.012241487947082842,\n",
       " 0.012212092623494777,\n",
       " 0.012182822465910294,\n",
       " 0.012153676795772617,\n",
       " 0.012124654939073177,\n",
       " 0.012095756226306,\n",
       " 0.012066979992422868,\n",
       " 0.012038325576789346,\n",
       " 0.012009792323141323,\n",
       " 0.011981379579542366,\n",
       " 0.011953086698341726,\n",
       " 0.011924913036133009,\n",
       " 0.011896857953713463,\n",
       " 0.011868920816043923,\n",
       " 0.011841100992209443,\n",
       " 0.01181339785538046,\n",
       " 0.011785810782774526,\n",
       " 0.011758339155618865,\n",
       " 0.011730982359113096,\n",
       " 0.011703739782392992,\n",
       " 0.011676610818494457,\n",
       " 0.011649594864318144,\n",
       " 0.011622691320594654,\n",
       " 0.01159589959185031,\n",
       " 0.01156921908637322,\n",
       " 0.011542649216180095,\n",
       " 0.01151618939698339,\n",
       " 0.011489839048159057,\n",
       " 0.011463597592714729,\n",
       " 0.011437464457258214,\n",
       " 0.011411439071966765,\n",
       " 0.01138552087055651,\n",
       " 0.011359709290252493,\n",
       " 0.011334003771758984,\n",
       " 0.011308403759230454,\n",
       " 0.011282908700242705,\n",
       " 0.011257518045764562,\n",
       " 0.011232231250129947,\n",
       " 0.011207047771010283,\n",
       " 0.011181967069387339,\n",
       " 0.011156988609526429,\n",
       " 0.011132111858949908,\n",
       " 0.011107336288411252,\n",
       " 0.011082661371869175,\n",
       " 0.011058086586462366,\n",
       " 0.011033611412484405,\n",
       " 0.011009235333359036,\n",
       " 0.010984957835615903,\n",
       " 0.010960778408866395,\n",
       " 0.010936696545779901,\n",
       " 0.01091271174206046,\n",
       " 0.010888823496423578,\n",
       " 0.010865031310573398,\n",
       " 0.01084133468918017,\n",
       " 0.010817733139857979,\n",
       " 0.010794226173142775,\n",
       " 0.010770813302470683,\n",
       " 0.010747494044156538,\n",
       " 0.010724267917372715,\n",
       " 0.010701134444128256,\n",
       " 0.010678093149248148,\n",
       " 0.010655143560352943,\n",
       " 0.010632285207838614,\n",
       " 0.010609517624856532,\n",
       " 0.010586840347293902,\n",
       " 0.01056425291375416,\n",
       " 0.010541754865537836,\n",
       " 0.01051934574662347,\n",
       " 0.010497025103648853,\n",
       " 0.010474792485892412,\n",
       " 0.010452647445254811,\n",
       " 0.010430589536240797,\n",
       " 0.010408618315941262,\n",
       " 0.01038673334401534,\n",
       " 0.010364934182672978,\n",
       " 0.010343220396657454,\n",
       " 0.010321591553228121,\n",
       " 0.01030004722214352,\n",
       " 0.010278586975644374,\n",
       " 0.010257210388437034,\n",
       " 0.010235917037676928,\n",
       " 0.010214706502952255,\n",
       " 0.010193578366267796,\n",
       " 0.010172532212028929,\n",
       " 0.010151567627025805,\n",
       " 0.010130684200417624,\n",
       " 0.010109881523717184,\n",
       " 0.010089159190775443,\n",
       " 0.010068516797766287,\n",
       " 0.010047953943171525,\n",
       " 0.01002747022776589,\n",
       " 0.010007065254602265,\n",
       " 0.009986738628997013,\n",
       " 0.00996648995851556,\n",
       " 0.009946318852957815,\n",
       " 0.009926224924344145,\n",
       " 0.009906207786901083,\n",
       " 0.009886267057047426,\n",
       " 0.009866402353380311,\n",
       " 0.00984661329666159,\n",
       " 0.009826899509803992,\n",
       " 0.00980726061785784,\n",
       " 0.009787696247997527,\n",
       " 0.009768206029508304,\n",
       " 0.009748789593773078,\n",
       " 0.00972944657425943,\n",
       " 0.0097101766065066,\n",
       " 0.009690979328112706,\n",
       " 0.009671854378722042,\n",
       " 0.009652801400012364,\n",
       " 0.0096338200356825,\n",
       " 0.00961490993143982,\n",
       " 0.009596070734988036,\n",
       " 0.009577302096014869,\n",
       " 0.009558603666179988,\n",
       " 0.009539975099103008,\n",
       " 0.009521416050351518,\n",
       " 0.009502926177429247,\n",
       " 0.009484505139764322,\n",
       " 0.009466152598697633,\n",
       " 0.009447868217471246,\n",
       " 0.00942965166121687,\n",
       " 0.00941150259694456,\n",
       " 0.009393420693531352,\n",
       " 0.009375405621709973,\n",
       " 0.009357457054057858,\n",
       " 0.009339574664985925,\n",
       " 0.009321758130727617,\n",
       " 0.009304007129328119,\n",
       " 0.009286321340633365,\n",
       " 0.009268700446279423,\n",
       " 0.009251144129681716,\n",
       " 0.00923365207602449,\n",
       " 0.00921622397225028,\n",
       " 0.009198859507049425,\n",
       " 0.009181558370849698,\n",
       " 0.009164320255806022,\n",
       " 0.00914714485579023,\n",
       " 0.009130031866380868,\n",
       " 0.009112980984853096,\n",
       " 0.009095991910168667,\n",
       " 0.009079064342966004,\n",
       " 0.009062197985550207,\n",
       " 0.009045392541883307,\n",
       " 0.009028647717574447,\n",
       " 0.0090119632198702,\n",
       " 0.008995338757644912,\n",
       " 0.00897877404139115,\n",
       " 0.008962268783210145,\n",
       " 0.008945822696802394,\n",
       " 0.00892943549745819,\n",
       " 0.00891310690204836,\n",
       " 0.008896836629014943,\n",
       " 0.008880624398362012,\n",
       " 0.008864469931646454,\n",
       " 0.00884837295196895,\n",
       " 0.008832333183964888,\n",
       " 0.00881635035379536,\n",
       " 0.00880042418913825,\n",
       " 0.008784554419179394,\n",
       " 0.008768740774603693,\n",
       " 0.00875298298758639,\n",
       " 0.00873728079178436,\n",
       " 0.008721633922327422,\n",
       " 0.008706042115809758,\n",
       " 0.008690505110281356,\n",
       " 0.008675022645239503,\n",
       " 0.008659594461620318,\n",
       " 0.008644220301790429,\n",
       " 0.008628899909538487,\n",
       " 0.008613633030067014,\n",
       " 0.008598419409984076,\n",
       " 0.00858325879729508,\n",
       " 0.00856815094139467,\n",
       " 0.008553095593058557,\n",
       " 0.00853809250443558,\n",
       " 0.008523141429039555,\n",
       " 0.008508242121741432,\n",
       " 0.00849339433876133,\n",
       " 0.008478597837660706,\n",
       " 0.008463852377334503,\n",
       " 0.008449157718003409,\n",
       " 0.008434513621206093,\n",
       " 0.008419919849791574,\n",
       " 0.008405376167911533,\n",
       " 0.008390882341012802,\n",
       " 0.008376438135829712,\n",
       " 0.00836204332037666,\n",
       " 0.008347697663940676,\n",
       " 0.008333400937073892,\n",
       " 0.008319152911586306,\n",
       " 0.008304953360538369,\n",
       " 0.008290802058233735,\n",
       " 0.008276698780212002,\n",
       " 0.008262643303241491,\n",
       " 0.008248635405312146,\n",
       " 0.008234674865628346,\n",
       " 0.00822076146460185,\n",
       " 0.008206894983844812,\n",
       " 0.008193075206162704,\n",
       " 0.008179301915547381,\n",
       " 0.008165574897170244,\n",
       " 0.008151893937375254,\n",
       " 0.008138258823672154,\n",
       " 0.008124669344729659,\n",
       " 0.008111125290368705,\n",
       " 0.00809762645155575,\n",
       " 0.008084172620396022,\n",
       " 0.008070763590126978,\n",
       " 0.008057399155111616,\n",
       " 0.008044079110831966,\n",
       " 0.008030803253882546,\n",
       " 0.008017571381963849,\n",
       " 0.0080043832938759,\n",
       " 0.007991238789511882,\n",
       " 0.007978137669851714,\n",
       " 0.007965079736955703,\n",
       " 0.00795206479395828,\n",
       " 0.0079390926450617,\n",
       " 0.00792616309552982,\n",
       " 0.00791327595168189,\n",
       " 0.007900431020886422,\n",
       " 0.007887628111555038,\n",
       " 0.007874867033136352,\n",
       " 0.007862147596110008,\n",
       " 0.007849469611980547,\n",
       " 0.007836832893271514,\n",
       " 0.007824237253519464,\n",
       " 0.007811682507268005,\n",
       " 0.007799168470061994,\n",
       " 0.007786694958441663,\n",
       " 0.007774261789936782,\n",
       " 0.007761868783060871,\n",
       " 0.0077495157573055145,\n",
       " 0.007737202533134567,\n",
       " 0.007724928931978509,\n",
       " 0.007712694776228822,\n",
       " 0.0077004998892323115,\n",
       " 0.007688344095285599,\n",
       " 0.007676227219629497,\n",
       " 0.007664149088443567,\n",
       " 0.0076521095288405085,\n",
       " 0.007640108368860878,\n",
       " 0.007628145437467506,\n",
       " 0.007616220564540222,\n",
       " 0.007604333580870385,\n",
       " 0.007592484318155672,\n",
       " 0.007580672608994658,\n",
       " 0.007568898286881647,\n",
       " 0.0075571611862014125,\n",
       " 0.007545461142223912,\n",
       " 0.007533797991099261,\n",
       " 0.00752217156985241,\n",
       " 0.00751058171637819,\n",
       " 0.007499028269436097,\n",
       " 0.007487511068645347,\n",
       " 0.007476029954479765,\n",
       " 0.00746458476826282,\n",
       " 0.0074531753521626725,\n",
       " 0.00744180154918719,\n",
       " 0.0074304632031790795,\n",
       " 0.007419160158810994,\n",
       " 0.0074078922615806704,\n",
       " 0.007396659357806117,\n",
       " 0.00738546129462081,\n",
       " 0.007374297919968919,\n",
       " 0.007363169082600607,\n",
       " 0.0073520746320672355,\n",
       " 0.007341014418716783,\n",
       " 0.007329988293689123,\n",
       " 0.007318996108911399,\n",
       " 0.00730803771709343,\n",
       " 0.007297112971723169,\n",
       " 0.007286221727062091,\n",
       " 0.007275363838140749,\n",
       " 0.007264539160754183,\n",
       " 0.007253747551457559,\n",
       " 0.007242988867561659,\n",
       " 0.007232262967128482,\n",
       " 0.007221569708966846,\n",
       " 0.007210908952628089,\n",
       " 0.007200280558401614,\n",
       " 0.007189684387310673,\n",
       " 0.007179120301108089,\n",
       " 0.007168588162271904,\n",
       " 0.007158087834001217,\n",
       " 0.00714761918021201,\n",
       " 0.007137182065532857,\n",
       " 0.0071267763553008266,\n",
       " 0.007116401915557377,\n",
       " 0.007106058613044187,\n",
       " 0.007095746315199078,\n",
       " 0.007085464890152013,\n",
       " 0.007075214206721003,\n",
       " 0.007064994134408082,\n",
       " 0.007054804543395381,\n",
       " 0.007044645304541111,\n",
       " 0.007034516289375626,\n",
       " 0.007024417370097565,\n",
       " 0.007014348419569848,\n",
       " 0.007004309311315925,\n",
       " 0.006994299919515851,\n",
       " 0.0069843201190024785,\n",
       " 0.006974369785257663,\n",
       " 0.0069644487944085145,\n",
       " 0.0069545570232235525,\n",
       " 0.006944694349109073,\n",
       " 0.006934860650105385,\n",
       " 0.006925055804883155,\n",
       " 0.006915279692739668,\n",
       " 0.0069055321935953045,\n",
       " 0.00689581318798978,\n",
       " 0.006886122557078637,\n",
       " 0.006876460182629688,\n",
       " 0.006866825947019345,\n",
       " 0.006857219733229179,\n",
       " 0.00684764142484239,\n",
       " 0.006838090906040314,\n",
       " 0.0068285680615989015,\n",
       " 0.006819072776885325,\n",
       " 0.006809604937854562,\n",
       " 0.0068001644310458995,\n",
       " 0.006790751143579636,\n",
       " 0.006781364963153662,\n",
       " 0.006772005778040151,\n",
       " 0.006762673477082181,\n",
       " 0.006753367949690458,\n",
       " 0.006744089085840054,\n",
       " 0.006734836776067082,\n",
       " 0.00672561091146551,\n",
       " 0.0067164113836838775,\n",
       " 0.006707238084922107,\n",
       " 0.006698090907928347,\n",
       " 0.006688969745995771,\n",
       " 0.006679874492959426,\n",
       " 0.006670805043193114,\n",
       " 0.006661761291606261,\n",
       " 0.006652743133640876,\n",
       " 0.006643750465268415,\n",
       " 0.006634783182986748,\n",
       " 0.006625841183817147,\n",
       " 0.00661692436530121,\n",
       " 0.006608032625497945,\n",
       " 0.0065991658629807074,\n",
       " 0.006590323976834286,\n",
       " 0.0065815068666519455,\n",
       " 0.006572714432532489,\n",
       " 0.006563946575077363,\n",
       " 0.006555203195387751,\n",
       " 0.0065464841950617185,\n",
       " 0.006537789476191345,\n",
       " 0.006529118941359878,\n",
       " 0.006520472493638933,\n",
       " 0.006511850036585654,\n",
       " 0.006503251474239978,\n",
       " 0.006494676711121816,\n",
       " 0.0064861256522283145,\n",
       " 0.006477598203031136,\n",
       " 0.00646909426947372,\n",
       " 0.006460613757968601,\n",
       " 0.006452156575394667,\n",
       " 0.006443722629094584,\n",
       " 0.0064353118268720595,\n",
       " 0.006426924076989217,\n",
       " 0.006418559288164024,\n",
       " 0.006410217369567628,\n",
       " 0.006401898230821817,\n",
       " 0.0063936017819963935,\n",
       " 0.006385327933606681,\n",
       " 0.006377076596610918,\n",
       " 0.006368847682407749,\n",
       " 0.006360641102833762,\n",
       " 0.006352456770160919,\n",
       " 0.0063442945970941375,\n",
       " 0.006336154496768766,\n",
       " 0.006328036382748202,\n",
       " 0.006319940169021397,\n",
       " 0.006311865770000475,\n",
       " 0.0063038131005183,\n",
       " 0.006295782075826127,\n",
       " 0.006287772611591118,\n",
       " 0.0062797846238941656,\n",
       " 0.006271818029227383,\n",
       " 0.006263872744491809,\n",
       " 0.006255948686995162,\n",
       " 0.006248045774449435,\n",
       " 0.006240163924968686,\n",
       " 0.006232303057066711,\n",
       " 0.006224463089654807,\n",
       " 0.006216643942039505,\n",
       " 0.006208845533920374,\n",
       " 0.006201067785387754,\n",
       " 0.006193310616920569,\n",
       " 0.006185573949384155,\n",
       " 0.00617785770402801,\n",
       " 0.006170161802483725,\n",
       " 0.006162486166762762,\n",
       " 0.006154830719254343,\n",
       " 0.006147195382723286,\n",
       " 0.006139580080307944,\n",
       " 0.006131984735518052,\n",
       " 0.006124409272232708,\n",
       " 0.006116853614698204,\n",
       " 0.006109317687526045,\n",
       " 0.006101801415690844,\n",
       " 0.0060943047245283315,\n",
       " 0.006086827539733265,\n",
       " 0.0060793697873575085,\n",
       " 0.006071931393807958,\n",
       " 0.006064512285844556,\n",
       " 0.006057112390578349,\n",
       " 0.006049731635469524,\n",
       " 0.006042369948325427,\n",
       " 0.0060350272572986495,\n",
       " 0.006027703490885064,\n",
       " 0.006020398577921958,\n",
       " 0.006013112447586099,\n",
       " 0.0060058450293918415,\n",
       " 0.005998596253189254,\n",
       " 0.005991366049162238,\n",
       " 0.0059841543478266965,\n",
       " 0.005976961080028656,\n",
       " 0.005969786176942427,\n",
       " 0.005962629570068823,\n",
       " 0.005955491191233303,\n",
       " 0.005948370972584185,\n",
       " 0.005941268846590869,\n",
       " 0.005934184746042012,\n",
       " 0.0059271186040438335,\n",
       " 0.00592007035401827,\n",
       " 0.005913039929701288,\n",
       " 0.005906027265141134,\n",
       " 0.005899032294696575,\n",
       " 0.0058920549530352495,\n",
       " 0.005885095175131886,\n",
       " 0.005878152896266664,\n",
       " 0.0058712280520234715,\n",
       " 0.005864320578288328,\n",
       " 0.005857430411247602,\n",
       " 0.005850557487386427,\n",
       " 0.005843701743487071,\n",
       " 0.005836863116627225,\n",
       " 0.00583004154417847,\n",
       " 0.005823236963804612,\n",
       " 0.0058164493134600665,\n",
       " 0.005809678531388284,\n",
       " 0.005802924556120195,\n",
       " 0.00579618732647256,\n",
       " 0.005789466781546477,\n",
       " 0.005782762860725772,\n",
       " 0.005776075503675478,\n",
       " 0.005769404650340306,\n",
       " 0.005762750240943114,\n",
       " 0.005756112215983342,\n",
       " 0.005749490516235558,\n",
       " 0.005742885082747925,\n",
       " 0.005736295856840754,\n",
       " 0.005729722780104933,\n",
       " 0.0057231657944005684,\n",
       " 0.005716624841855436,\n",
       " 0.005710099864863546,\n",
       " 0.005703590806083685,\n",
       " 0.0056970976084380264,\n",
       " 0.005690620215110655,\n",
       " 0.005684158569546163,\n",
       " 0.0056777126154482,\n",
       " 0.005671282296778169,\n",
       " 0.005664867557753693,\n",
       " 0.005658468342847356,\n",
       " 0.00565208459678524,\n",
       " 0.005645716264545583,\n",
       " 0.00563936329135743,\n",
       " 0.0056330256226992635,\n",
       " 0.005626703204297658,\n",
       " 0.005620395982125934,\n",
       " 0.0056141039024028856,\n",
       " 0.005607826911591358,\n",
       " 0.0056015649563970335,\n",
       " 0.005595317983767037,\n",
       " 0.0055890859408887494,\n",
       " 0.005582868775188401,\n",
       " 0.005576666434329846,\n",
       " 0.005570478866213314,\n",
       " 0.005564306018974076,\n",
       " 0.005558147840981267,\n",
       " 0.005552004280836536,\n",
       " 0.005545875287372889,\n",
       " 0.005539760809653435,\n",
       " 0.005533660796970106,\n",
       " 0.005527575198842468,\n",
       " 0.005521503965016558,\n",
       " 0.005515447045463561,\n",
       " 0.0055094043903787275,\n",
       " 0.005503375950180128,\n",
       " 0.005497361675507435,\n",
       " 0.0054913615172208115]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,Y,iterations,alpha,X_train,Y_train):\n",
    "\n",
    "    W1,W2,W3,b1,b2,b3,count,caches = gradient_descent(iterations,X_train,Y_train,alpha)\n",
    "    \n",
    "    Z1 = np.dot(W1,X)\n",
    "    A1 = tanh(Z1 + b1)\n",
    "    Z2 = np.dot(W2,A1)\n",
    "    A2 = tanh(Z2 + b2)\n",
    "    Z3 = np.dot(W3,A2)\n",
    "    A3 = sigmoid(Z3 + b3)\n",
    "    \n",
    "    dummy,m = A3.shape\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        Y_prediction[0, i] = 1 if A3[0, i] > 0.5 else 0\n",
    "        \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 98.94736842105263 %\n",
      "Cross validation accuracy: 96.80851063829788 %\n"
     ]
    }
   ],
   "source": [
    "#Lets see how accurate the predictions made by our neural network are compared to the training set and cross validation set\n",
    "print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(predict(X_train,Y_train,1000,0.5,X_train,Y_train) - Y_train)) * 100))\n",
    "print(\"Cross validation accuracy: {} %\".format(100 - np.mean(np.abs(predict(X_cv,Y_cv,1000,0.5,X_train,Y_train) - Y_cv)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 positives predicted on the training set\n",
      "168 true positives are in the training set\n",
      "The accuracy of true positives on the training set is: 97.61904761904762 %\n",
      "----------------------------------------------------------------\n",
      "43 positives predicted on the cross validation set\n",
      "43 true positives are in the cross validation set\n",
      "The accuracy of true positives on the cross validation set is: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "dummy,m1 = X_train.shape\n",
    "dummy,m2 = X_cv.shape\n",
    "\n",
    "train_predict = predict(X_train,Y_train,1000,0.5,X_train,Y_train)\n",
    "CV_predict = predict(X_cv,Y_cv,1000,0.5,X_train,Y_train)\n",
    "count_true_pos = 0\n",
    "count_train_pos = 0\n",
    "\n",
    "count_true_pos_cv = 0\n",
    "count_cv_pos = 0\n",
    "\n",
    "for i in range (1,m1):\n",
    "    if train_predict[0,i] == 1 and Y_train[0,i] == 1:\n",
    "        count_true_pos = count_true_pos + 1\n",
    "    if Y_train[0,i] == 1:\n",
    "        count_train_pos = count_train_pos + 1\n",
    "        \n",
    "for i in range (1,m2):\n",
    "    if CV_predict[0,i] == 1 and Y_cv[0,i] == 1:\n",
    "        count_true_pos_cv = count_true_pos_cv + 1\n",
    "    if Y_cv[0,i] == 1:\n",
    "        count_cv_pos = count_cv_pos + 1\n",
    "        \n",
    "print(str(count_true_pos) + \" positives predicted on the training set\")\n",
    "print(str(count_train_pos) + \" true positives are in the training set\")\n",
    "print(\"The accuracy of true positives on the training set is: {} %\".format(100-np.abs(100*((count_true_pos - count_train_pos)/count_train_pos))))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(str(count_true_pos_cv) + \" positives predicted on the cross validation set\")\n",
    "print(str(count_cv_pos) + \" true positives are in the cross validation set\")\n",
    "print(\"The accuracy of true positives on the cross validation set is: {} %\".format(100-np.abs(100*((count_true_pos_cv - count_cv_pos)/count_true_pos_cv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 negatives predicted on the training set\n",
      "211 true negatives are in the training set\n",
      "The accuracy of true negatives on the training set is: 99.5260663507109 %\n",
      "----------------------------------------------------------------\n",
      "138 negatives predicted on the cross validation set\n",
      "144 true negatives are in the cross validation set\n",
      "The accuracy of true negatives on the cross validation set is: 95.65217391304348 %\n"
     ]
    }
   ],
   "source": [
    "count_true_neg = 0\n",
    "count_train_neg = 0\n",
    "\n",
    "count_true_neg_cv = 0\n",
    "count_cv_neg = 0\n",
    "\n",
    "for i in range (1,m1):\n",
    "    if train_predict[0,i] == 0 and Y_train[0,i] == 0:\n",
    "        count_true_neg = count_true_neg + 1\n",
    "    if Y_train[0,i] == 0:\n",
    "        count_train_neg = count_train_neg + 1\n",
    "        \n",
    "for i in range (1,m2):\n",
    "    if CV_predict[0,i] == 0 and Y_cv[0,i] == 0:\n",
    "        count_true_neg_cv = count_true_neg_cv + 1\n",
    "    if Y_cv[0,i] == 0:\n",
    "        count_cv_neg = count_cv_neg + 1\n",
    "        \n",
    "print(str(count_true_neg) + \" negatives predicted on the training set\")\n",
    "print(str(count_train_neg) + \" true negatives are in the training set\")\n",
    "print(\"The accuracy of true negatives on the training set is: {} %\".format(100-np.abs(100*((count_true_neg - count_train_neg)/count_train_neg))))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(str(count_true_neg_cv) + \" negatives predicted on the cross validation set\")\n",
    "print(str(count_cv_neg) + \" true negatives are in the cross validation set\")\n",
    "print(\"The accuracy of true negatives on the cross validation set is: {} %\".format(100-np.abs(100*((count_true_neg_cv - count_cv_neg)/count_true_neg_cv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('soldgame': conda)",
   "language": "python",
   "name": "python37764bitsoldgameconda7262750cf6184ac8bf201f1ff27a4c4f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
