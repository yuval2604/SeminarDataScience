{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd  # Import Pandas for data manipulation using dataframes\n",
    "import numpy as np  # Import Numpy for data statistical analysis\n",
    "import matplotlib.pyplot as plt  # Import matplotlib for data visualisation\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "import os\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "\n",
    "cols = data.columns\n",
    "x_data = data[cols[2:-1]]\n",
    "print(len(cols))\n",
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1]), array(['B', 'M'], dtype=object))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = data[cols[1]] #diagnosis\n",
    "le = LabelEncoder()\n",
    "y_data = np.array(le.fit_transform(y_data))\n",
    "y_data[:5], le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.view(-1,1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569,), (569, 30))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape, x_data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = Variable(torch.from_numpy(x_data.values))\n",
    "y_data = Variable(torch.from_numpy(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6449],\n",
       "        [0.6506],\n",
       "        [0.6502],\n",
       "        [0.6496],\n",
       "        [0.6534],\n",
       "        [0.6492],\n",
       "        [0.6507],\n",
       "        [0.6491],\n",
       "        [0.6507],\n",
       "        [0.6508],\n",
       "        [0.6508],\n",
       "        [0.6492],\n",
       "        [0.6537],\n",
       "        [0.6545],\n",
       "        [0.6545],\n",
       "        [0.6509],\n",
       "        [0.6502],\n",
       "        [0.6500],\n",
       "        [0.6462],\n",
       "        [0.6537],\n",
       "        [0.6539],\n",
       "        [0.6530],\n",
       "        [0.6505],\n",
       "        [0.6494],\n",
       "        [0.6458],\n",
       "        [0.6450],\n",
       "        [0.6518],\n",
       "        [0.6539],\n",
       "        [0.6507],\n",
       "        [0.6536],\n",
       "        [0.6503],\n",
       "        [0.6461],\n",
       "        [0.6505],\n",
       "        [0.6508],\n",
       "        [0.6505],\n",
       "        [0.6508],\n",
       "        [0.6545],\n",
       "        [0.6543],\n",
       "        [0.6535],\n",
       "        [0.6543],\n",
       "        [0.6514],\n",
       "        [0.6516],\n",
       "        [0.6504],\n",
       "        [0.6499],\n",
       "        [0.6517],\n",
       "        [0.6501],\n",
       "        [0.6540],\n",
       "        [0.6507],\n",
       "        [0.6537],\n",
       "        [0.6545],\n",
       "        [0.6543],\n",
       "        [0.6542],\n",
       "        [0.6541],\n",
       "        [0.6534],\n",
       "        [0.6508],\n",
       "        [0.6540],\n",
       "        [0.6457],\n",
       "        [0.6505],\n",
       "        [0.6540],\n",
       "        [0.6529],\n",
       "        [0.6517],\n",
       "        [0.6539],\n",
       "        [0.6496],\n",
       "        [0.6524],\n",
       "        [0.6495],\n",
       "        [0.6518],\n",
       "        [0.6544],\n",
       "        [0.6542],\n",
       "        [0.6530],\n",
       "        [0.6539],\n",
       "        [0.6494],\n",
       "        [0.6514],\n",
       "        [0.6485],\n",
       "        [0.6515],\n",
       "        [0.6540],\n",
       "        [0.6492],\n",
       "        [0.6531],\n",
       "        [0.6449],\n",
       "        [0.6537],\n",
       "        [0.6541],\n",
       "        [0.6541],\n",
       "        [0.6540],\n",
       "        [0.6546],\n",
       "        [0.6546],\n",
       "        [0.6541],\n",
       "        [0.6503],\n",
       "        [0.6543],\n",
       "        [0.6508],\n",
       "        [0.6545],\n",
       "        [0.6534],\n",
       "        [0.6545],\n",
       "        [0.6544],\n",
       "        [0.6494],\n",
       "        [0.6542],\n",
       "        [0.6502],\n",
       "        [0.6526],\n",
       "        [0.6539],\n",
       "        [0.6541],\n",
       "        [0.6535],\n",
       "        [0.6544],\n",
       "        [0.6507],\n",
       "        [0.6518],\n",
       "        [0.6545],\n",
       "        [0.6543],\n",
       "        [0.6540],\n",
       "        [0.6485],\n",
       "        [0.6542],\n",
       "        [0.6544],\n",
       "        [0.6461],\n",
       "        [0.6542],\n",
       "        [0.6532],\n",
       "        [0.6543],\n",
       "        [0.6540],\n",
       "        [0.6541],\n",
       "        [0.6537],\n",
       "        [0.6537],\n",
       "        [0.6532],\n",
       "        [0.6497],\n",
       "        [0.6501],\n",
       "        [0.6544],\n",
       "        [0.6534],\n",
       "        [0.6499],\n",
       "        [0.6530],\n",
       "        [0.6534],\n",
       "        [0.6541],\n",
       "        [0.6542],\n",
       "        [0.6509],\n",
       "        [0.6541],\n",
       "        [0.6535],\n",
       "        [0.6546],\n",
       "        [0.6537],\n",
       "        [0.6502],\n",
       "        [0.6507],\n",
       "        [0.6538],\n",
       "        [0.6507],\n",
       "        [0.6545],\n",
       "        [0.6536],\n",
       "        [0.6540],\n",
       "        [0.6451],\n",
       "        [0.6531],\n",
       "        [0.6531],\n",
       "        [0.6490],\n",
       "        [0.6540],\n",
       "        [0.6540],\n",
       "        [0.6537],\n",
       "        [0.6533],\n",
       "        [0.6520],\n",
       "        [0.6540],\n",
       "        [0.6540],\n",
       "        [0.6541],\n",
       "        [0.6540],\n",
       "        [0.6544],\n",
       "        [0.6491],\n",
       "        [0.6535],\n",
       "        [0.6538],\n",
       "        [0.6542],\n",
       "        [0.6532],\n",
       "        [0.6544],\n",
       "        [0.6537],\n",
       "        [0.6533],\n",
       "        [0.6536],\n",
       "        [0.6531],\n",
       "        [0.6459],\n",
       "        [0.6544],\n",
       "        [0.6509],\n",
       "        [0.6545],\n",
       "        [0.6531],\n",
       "        [0.6502],\n",
       "        [0.6458],\n",
       "        [0.6541],\n",
       "        [0.6535],\n",
       "        [0.6482],\n",
       "        [0.6490],\n",
       "        [0.6536],\n",
       "        [0.6536],\n",
       "        [0.6535],\n",
       "        [0.6529],\n",
       "        [0.6544],\n",
       "        [0.6545],\n",
       "        [0.6535],\n",
       "        [0.6509],\n",
       "        [0.6508],\n",
       "        [0.6506],\n",
       "        [0.6535],\n",
       "        [0.6537],\n",
       "        [0.6501],\n",
       "        [0.6546],\n",
       "        [0.6540],\n",
       "        [0.6542],\n",
       "        [0.6539],\n",
       "        [0.6545],\n",
       "        [0.6537],\n",
       "        [0.6539],\n",
       "        [0.6508],\n",
       "        [0.6545],\n",
       "        [0.6541],\n",
       "        [0.6518],\n",
       "        [0.6541],\n",
       "        [0.6508],\n",
       "        [0.6507],\n",
       "        [0.6515],\n",
       "        [0.6544],\n",
       "        [0.6547],\n",
       "        [0.6473],\n",
       "        [0.6505],\n",
       "        [0.6527],\n",
       "        [0.6541],\n",
       "        [0.6540],\n",
       "        [0.6545],\n",
       "        [0.6538],\n",
       "        [0.6540],\n",
       "        [0.6542],\n",
       "        [0.6552],\n",
       "        [0.6545],\n",
       "        [0.6544],\n",
       "        [0.6541],\n",
       "        [0.6542],\n",
       "        [0.6538],\n",
       "        [0.6477],\n",
       "        [0.6466],\n",
       "        [0.6537],\n",
       "        [0.6536],\n",
       "        [0.6540],\n",
       "        [0.6507],\n",
       "        [0.6541],\n",
       "        [0.6522],\n",
       "        [0.6538],\n",
       "        [0.6541],\n",
       "        [0.6546],\n",
       "        [0.6510],\n",
       "        [0.6544],\n",
       "        [0.6547],\n",
       "        [0.6547],\n",
       "        [0.6509],\n",
       "        [0.6538],\n",
       "        [0.6545],\n",
       "        [0.6479],\n",
       "        [0.6545],\n",
       "        [0.6547],\n",
       "        [0.6509],\n",
       "        [0.6537],\n",
       "        [0.6540],\n",
       "        [0.6543],\n",
       "        [0.6543],\n",
       "        [0.6545],\n",
       "        [0.6543],\n",
       "        [0.6543],\n",
       "        [0.6539],\n",
       "        [0.6539],\n",
       "        [0.6538],\n",
       "        [0.6499],\n",
       "        [0.6539],\n",
       "        [0.6500],\n",
       "        [0.6544],\n",
       "        [0.6504],\n",
       "        [0.6523],\n",
       "        [0.6507],\n",
       "        [0.6533],\n",
       "        [0.6450],\n",
       "        [0.6515],\n",
       "        [0.6509],\n",
       "        [0.6546],\n",
       "        [0.6524],\n",
       "        [0.6546],\n",
       "        [0.6508],\n",
       "        [0.6460],\n",
       "        [0.6538],\n",
       "        [0.6544],\n",
       "        [0.6541],\n",
       "        [0.6542],\n",
       "        [0.6543],\n",
       "        [0.6535],\n",
       "        [0.6475],\n",
       "        [0.6537],\n",
       "        [0.6546],\n",
       "        [0.6530],\n",
       "        [0.6537],\n",
       "        [0.6546],\n",
       "        [0.6543],\n",
       "        [0.6540],\n",
       "        [0.6509],\n",
       "        [0.6527],\n",
       "        [0.6507],\n",
       "        [0.6543],\n",
       "        [0.6538],\n",
       "        [0.6541],\n",
       "        [0.6544],\n",
       "        [0.6537],\n",
       "        [0.6538],\n",
       "        [0.6543],\n",
       "        [0.6530],\n",
       "        [0.6543],\n",
       "        [0.6539],\n",
       "        [0.6543],\n",
       "        [0.6537],\n",
       "        [0.6537],\n",
       "        [0.6535],\n",
       "        [0.6526],\n",
       "        [0.6543],\n",
       "        [0.6543],\n",
       "        [0.6454],\n",
       "        [0.6540],\n",
       "        [0.6532],\n",
       "        [0.6543],\n",
       "        [0.6540],\n",
       "        [0.6546],\n",
       "        [0.6540],\n",
       "        [0.6538],\n",
       "        [0.6536],\n",
       "        [0.6534],\n",
       "        [0.6544],\n",
       "        [0.6540],\n",
       "        [0.6535],\n",
       "        [0.6534],\n",
       "        [0.6540],\n",
       "        [0.6541],\n",
       "        [0.6537],\n",
       "        [0.6506],\n",
       "        [0.6531],\n",
       "        [0.6537],\n",
       "        [0.6535],\n",
       "        [0.6543],\n",
       "        [0.6538],\n",
       "        [0.6508],\n",
       "        [0.6539],\n",
       "        [0.6540],\n",
       "        [0.6536],\n",
       "        [0.6541],\n",
       "        [0.6528],\n",
       "        [0.6540],\n",
       "        [0.6539],\n",
       "        [0.6542],\n",
       "        [0.6542],\n",
       "        [0.6539],\n",
       "        [0.6544],\n",
       "        [0.6502],\n",
       "        [0.6537],\n",
       "        [0.6495],\n",
       "        [0.6541],\n",
       "        [0.6502],\n",
       "        [0.6537],\n",
       "        [0.6539],\n",
       "        [0.6539],\n",
       "        [0.6546],\n",
       "        [0.6535],\n",
       "        [0.6535],\n",
       "        [0.6543],\n",
       "        [0.6538],\n",
       "        [0.6540],\n",
       "        [0.6534],\n",
       "        [0.6537],\n",
       "        [0.6537],\n",
       "        [0.6505],\n",
       "        [0.6506],\n",
       "        [0.6530],\n",
       "        [0.6540],\n",
       "        [0.6541],\n",
       "        [0.6542],\n",
       "        [0.6505],\n",
       "        [0.6481],\n",
       "        [0.6539],\n",
       "        [0.6545],\n",
       "        [0.6543],\n",
       "        [0.6543],\n",
       "        [0.6542],\n",
       "        [0.6539],\n",
       "        [0.6543],\n",
       "        [0.6529],\n",
       "        [0.6449],\n",
       "        [0.6503],\n",
       "        [0.6519],\n",
       "        [0.6538],\n",
       "        [0.6545],\n",
       "        [0.6506],\n",
       "        [0.6541],\n",
       "        [0.6541],\n",
       "        [0.6543],\n",
       "        [0.6547],\n",
       "        [0.6541],\n",
       "        [0.6512],\n",
       "        [0.6536],\n",
       "        [0.6539],\n",
       "        [0.6546],\n",
       "        [0.6540],\n",
       "        [0.6538],\n",
       "        [0.6545],\n",
       "        [0.6537],\n",
       "        [0.6540],\n",
       "        [0.6536],\n",
       "        [0.6545],\n",
       "        [0.6533],\n",
       "        [0.6494],\n",
       "        [0.6460],\n",
       "        [0.6507],\n",
       "        [0.6541],\n",
       "        [0.6543],\n",
       "        [0.6543],\n",
       "        [0.6538],\n",
       "        [0.6537],\n",
       "        [0.6539],\n",
       "        [0.6544],\n",
       "        [0.6533],\n",
       "        [0.6542],\n",
       "        [0.6541],\n",
       "        [0.6535],\n",
       "        [0.6539],\n",
       "        [0.6541],\n",
       "        [0.6541],\n",
       "        [0.6542],\n",
       "        [0.6541],\n",
       "        [0.6544],\n",
       "        [0.6542],\n",
       "        [0.6546],\n",
       "        [0.6545],\n",
       "        [0.6546],\n",
       "        [0.6544],\n",
       "        [0.6525],\n",
       "        [0.6449],\n",
       "        [0.6536],\n",
       "        [0.6544],\n",
       "        [0.6542],\n",
       "        [0.6532],\n",
       "        [0.6538],\n",
       "        [0.6543],\n",
       "        [0.6494],\n",
       "        [0.6545],\n",
       "        [0.6528],\n",
       "        [0.6523],\n",
       "        [0.6541],\n",
       "        [0.6540],\n",
       "        [0.6545],\n",
       "        [0.6541],\n",
       "        [0.6545],\n",
       "        [0.6501],\n",
       "        [0.6540],\n",
       "        [0.6507],\n",
       "        [0.6542],\n",
       "        [0.6538],\n",
       "        [0.6543],\n",
       "        [0.6540],\n",
       "        [0.6541],\n",
       "        [0.6523],\n",
       "        [0.6537],\n",
       "        [0.6542],\n",
       "        [0.6543],\n",
       "        [0.6545],\n",
       "        [0.6509],\n",
       "        [0.6542],\n",
       "        [0.6544],\n",
       "        [0.6508],\n",
       "        [0.6544],\n",
       "        [0.6546],\n",
       "        [0.6547],\n",
       "        [0.6536],\n",
       "        [0.6542],\n",
       "        [0.6547],\n",
       "        [0.6546],\n",
       "        [0.6546],\n",
       "        [0.6546],\n",
       "        [0.6547],\n",
       "        [0.6460],\n",
       "        [0.6449],\n",
       "        [0.6546],\n",
       "        [0.6543],\n",
       "        [0.6543],\n",
       "        [0.6534],\n",
       "        [0.6543],\n",
       "        [0.6526],\n",
       "        [0.6495],\n",
       "        [0.6537],\n",
       "        [0.6524],\n",
       "        [0.6544],\n",
       "        [0.6539],\n",
       "        [0.6546],\n",
       "        [0.6539],\n",
       "        [0.6540],\n",
       "        [0.6541],\n",
       "        [0.6542],\n",
       "        [0.6539],\n",
       "        [0.6542],\n",
       "        [0.6543],\n",
       "        [0.6526],\n",
       "        [0.6538],\n",
       "        [0.6541],\n",
       "        [0.6537],\n",
       "        [0.6537],\n",
       "        [0.6543],\n",
       "        [0.6506],\n",
       "        [0.6534],\n",
       "        [0.6546],\n",
       "        [0.6541],\n",
       "        [0.6537],\n",
       "        [0.6500],\n",
       "        [0.6534],\n",
       "        [0.6544],\n",
       "        [0.6544],\n",
       "        [0.6540],\n",
       "        [0.6543],\n",
       "        [0.6494],\n",
       "        [0.6545],\n",
       "        [0.6538],\n",
       "        [0.6543],\n",
       "        [0.6539],\n",
       "        [0.6459],\n",
       "        [0.6513],\n",
       "        [0.6524],\n",
       "        [0.6543],\n",
       "        [0.6540],\n",
       "        [0.6541],\n",
       "        [0.6546],\n",
       "        [0.6537],\n",
       "        [0.6539],\n",
       "        [0.6504],\n",
       "        [0.6535],\n",
       "        [0.6527],\n",
       "        [0.6541],\n",
       "        [0.6505],\n",
       "        [0.6545],\n",
       "        [0.6517],\n",
       "        [0.6537],\n",
       "        [0.6520],\n",
       "        [0.6506],\n",
       "        [0.6544],\n",
       "        [0.6542],\n",
       "        [0.6536],\n",
       "        [0.6534],\n",
       "        [0.6543],\n",
       "        [0.6538],\n",
       "        [0.6532],\n",
       "        [0.6535],\n",
       "        [0.6538],\n",
       "        [0.6541],\n",
       "        [0.6539],\n",
       "        [0.6541],\n",
       "        [0.6543],\n",
       "        [0.6527],\n",
       "        [0.6546],\n",
       "        [0.6545],\n",
       "        [0.6514],\n",
       "        [0.6530],\n",
       "        [0.6536],\n",
       "        [0.6546],\n",
       "        [0.6546],\n",
       "        [0.6547],\n",
       "        [0.6543],\n",
       "        [0.6544],\n",
       "        [0.6540],\n",
       "        [0.6541],\n",
       "        [0.6539],\n",
       "        [0.6508],\n",
       "        [0.6543],\n",
       "        [0.6544],\n",
       "        [0.6547],\n",
       "        [0.6543],\n",
       "        [0.6547],\n",
       "        [0.6546],\n",
       "        [0.6541],\n",
       "        [0.6543],\n",
       "        [0.6545],\n",
       "        [0.6546],\n",
       "        [0.6546],\n",
       "        [0.6547],\n",
       "        [0.6547],\n",
       "        [0.6541],\n",
       "        [0.6527],\n",
       "        [0.6535],\n",
       "        [0.6546],\n",
       "        [0.6509],\n",
       "        [0.6512]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(30,16)\n",
    "        self.l2 = torch.nn.Linear(16,4)\n",
    "        self.l3 = torch.nn.Linear(4,1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x)) \n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "y_pred = model(x_data.float())\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([569, 30]), torch.Size([569]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.float().size(), y_data.float().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 469.53912353515625 - Pred: tensor([0.6449])\n",
      "Epoch 1 Loss: 468.5921630859375 - Pred: tensor([0.6441])\n",
      "Epoch 2 Loss: 467.653076171875 - Pred: tensor([0.6432])\n",
      "Epoch 3 Loss: 466.8077392578125 - Pred: tensor([0.6424])\n",
      "Epoch 4 Loss: 466.04510498046875 - Pred: tensor([0.6416])\n",
      "Epoch 5 Loss: 465.31781005859375 - Pred: tensor([0.6408])\n",
      "Epoch 6 Loss: 464.6075744628906 - Pred: tensor([0.6400])\n",
      "Epoch 7 Loss: 463.90972900390625 - Pred: tensor([0.6392])\n",
      "Epoch 8 Loss: 463.22308349609375 - Pred: tensor([0.6384])\n",
      "Epoch 9 Loss: 462.5433654785156 - Pred: tensor([0.6375])\n",
      "Epoch 10 Loss: 461.8692932128906 - Pred: tensor([0.6367])\n",
      "Epoch 11 Loss: 461.19512939453125 - Pred: tensor([0.6359])\n",
      "Epoch 12 Loss: 460.5147705078125 - Pred: tensor([0.6351])\n",
      "Epoch 13 Loss: 459.8261413574219 - Pred: tensor([0.6343])\n",
      "Epoch 14 Loss: 459.0963439941406 - Pred: tensor([0.6335])\n",
      "Epoch 15 Loss: 458.30181884765625 - Pred: tensor([0.6327])\n",
      "Epoch 16 Loss: 457.4169616699219 - Pred: tensor([0.6318])\n",
      "Epoch 17 Loss: 456.4685974121094 - Pred: tensor([0.6310])\n",
      "Epoch 18 Loss: 455.4846496582031 - Pred: tensor([0.6302])\n",
      "Epoch 19 Loss: 454.5423278808594 - Pred: tensor([0.6294])\n",
      "Epoch 20 Loss: 453.744873046875 - Pred: tensor([0.6286])\n",
      "Epoch 21 Loss: 453.0741271972656 - Pred: tensor([0.6278])\n",
      "Epoch 22 Loss: 452.4742126464844 - Pred: tensor([0.6270])\n",
      "Epoch 23 Loss: 451.91033935546875 - Pred: tensor([0.6261])\n",
      "Epoch 24 Loss: 451.3644104003906 - Pred: tensor([0.6253])\n",
      "Epoch 25 Loss: 450.82843017578125 - Pred: tensor([0.6245])\n",
      "Epoch 26 Loss: 450.2964782714844 - Pred: tensor([0.6237])\n",
      "Epoch 27 Loss: 449.7654724121094 - Pred: tensor([0.6229])\n",
      "Epoch 28 Loss: 449.2352600097656 - Pred: tensor([0.6220])\n",
      "Epoch 29 Loss: 448.71026611328125 - Pred: tensor([0.6199])\n",
      "Epoch 30 Loss: 448.19183349609375 - Pred: tensor([0.6184])\n",
      "Epoch 31 Loss: 447.6668395996094 - Pred: tensor([0.6177])\n",
      "Epoch 32 Loss: 447.1370544433594 - Pred: tensor([0.6172])\n",
      "Epoch 33 Loss: 446.606689453125 - Pred: tensor([0.6168])\n",
      "Epoch 34 Loss: 446.07086181640625 - Pred: tensor([0.6165])\n",
      "Epoch 35 Loss: 445.5173645019531 - Pred: tensor([0.6160])\n",
      "Epoch 36 Loss: 444.9471740722656 - Pred: tensor([0.6154])\n",
      "Epoch 37 Loss: 444.36083984375 - Pred: tensor([0.6146])\n",
      "Epoch 38 Loss: 443.7637939453125 - Pred: tensor([0.6138])\n",
      "Epoch 39 Loss: 443.1353454589844 - Pred: tensor([0.6130])\n",
      "Epoch 40 Loss: 442.5074157714844 - Pred: tensor([0.6122])\n",
      "Epoch 41 Loss: 441.9244384765625 - Pred: tensor([0.6115])\n",
      "Epoch 42 Loss: 441.3786315917969 - Pred: tensor([0.6107])\n",
      "Epoch 43 Loss: 440.89111328125 - Pred: tensor([0.6099])\n",
      "Epoch 44 Loss: 440.4320983886719 - Pred: tensor([0.6091])\n",
      "Epoch 45 Loss: 439.9853515625 - Pred: tensor([0.6083])\n",
      "Epoch 46 Loss: 439.54736328125 - Pred: tensor([0.6075])\n",
      "Epoch 47 Loss: 439.11297607421875 - Pred: tensor([0.6068])\n",
      "Epoch 48 Loss: 438.6800537109375 - Pred: tensor([0.6060])\n",
      "Epoch 49 Loss: 438.2500915527344 - Pred: tensor([0.6052])\n",
      "Epoch 50 Loss: 437.8226013183594 - Pred: tensor([0.6044])\n",
      "Epoch 51 Loss: 437.3976745605469 - Pred: tensor([0.6037])\n",
      "Epoch 52 Loss: 436.9756164550781 - Pred: tensor([0.6029])\n",
      "Epoch 53 Loss: 436.55584716796875 - Pred: tensor([0.6021])\n",
      "Epoch 54 Loss: 436.1390380859375 - Pred: tensor([0.6014])\n",
      "Epoch 55 Loss: 435.7244873046875 - Pred: tensor([0.6006])\n",
      "Epoch 56 Loss: 435.3123474121094 - Pred: tensor([0.5998])\n",
      "Epoch 57 Loss: 434.9031066894531 - Pred: tensor([0.5991])\n",
      "Epoch 58 Loss: 434.4961853027344 - Pred: tensor([0.5983])\n",
      "Epoch 59 Loss: 434.0919494628906 - Pred: tensor([0.5976])\n",
      "Epoch 60 Loss: 433.69024658203125 - Pred: tensor([0.5968])\n",
      "Epoch 61 Loss: 433.2913818359375 - Pred: tensor([0.5961])\n",
      "Epoch 62 Loss: 432.89459228515625 - Pred: tensor([0.5953])\n",
      "Epoch 63 Loss: 432.500244140625 - Pred: tensor([0.5946])\n",
      "Epoch 64 Loss: 432.1089172363281 - Pred: tensor([0.5938])\n",
      "Epoch 65 Loss: 431.7197265625 - Pred: tensor([0.5931])\n",
      "Epoch 66 Loss: 431.3328857421875 - Pred: tensor([0.5923])\n",
      "Epoch 67 Loss: 430.9482727050781 - Pred: tensor([0.5916])\n",
      "Epoch 68 Loss: 430.5665588378906 - Pred: tensor([0.5908])\n",
      "Epoch 69 Loss: 430.18670654296875 - Pred: tensor([0.5901])\n",
      "Epoch 70 Loss: 429.80963134765625 - Pred: tensor([0.5894])\n",
      "Epoch 71 Loss: 429.4349365234375 - Pred: tensor([0.5886])\n",
      "Epoch 72 Loss: 429.0623474121094 - Pred: tensor([0.5879])\n",
      "Epoch 73 Loss: 428.6922302246094 - Pred: tensor([0.5872])\n",
      "Epoch 74 Loss: 428.32452392578125 - Pred: tensor([0.5864])\n",
      "Epoch 75 Loss: 427.95892333984375 - Pred: tensor([0.5857])\n",
      "Epoch 76 Loss: 427.5957336425781 - Pred: tensor([0.5850])\n",
      "Epoch 77 Loss: 427.2350769042969 - Pred: tensor([0.5842])\n",
      "Epoch 78 Loss: 426.8764953613281 - Pred: tensor([0.5835])\n",
      "Epoch 79 Loss: 426.5197448730469 - Pred: tensor([0.5828])\n",
      "Epoch 80 Loss: 426.16546630859375 - Pred: tensor([0.5821])\n",
      "Epoch 81 Loss: 425.8134460449219 - Pred: tensor([0.5814])\n",
      "Epoch 82 Loss: 425.4638977050781 - Pred: tensor([0.5806])\n",
      "Epoch 83 Loss: 425.11614990234375 - Pred: tensor([0.5799])\n",
      "Epoch 84 Loss: 424.7706604003906 - Pred: tensor([0.5792])\n",
      "Epoch 85 Loss: 424.4275817871094 - Pred: tensor([0.5785])\n",
      "Epoch 86 Loss: 424.0865173339844 - Pred: tensor([0.5778])\n",
      "Epoch 87 Loss: 423.7474060058594 - Pred: tensor([0.5771])\n",
      "Epoch 88 Loss: 423.4106140136719 - Pred: tensor([0.5764])\n",
      "Epoch 89 Loss: 423.0756530761719 - Pred: tensor([0.5757])\n",
      "Epoch 90 Loss: 422.7430419921875 - Pred: tensor([0.5750])\n",
      "Epoch 91 Loss: 422.4125061035156 - Pred: tensor([0.5743])\n",
      "Epoch 92 Loss: 422.0838623046875 - Pred: tensor([0.5736])\n",
      "Epoch 93 Loss: 421.7574157714844 - Pred: tensor([0.5729])\n",
      "Epoch 94 Loss: 421.43310546875 - Pred: tensor([0.5722])\n",
      "Epoch 95 Loss: 421.11083984375 - Pred: tensor([0.5715])\n",
      "Epoch 96 Loss: 420.7904052734375 - Pred: tensor([0.5708])\n",
      "Epoch 97 Loss: 420.47198486328125 - Pred: tensor([0.5701])\n",
      "Epoch 98 Loss: 420.1553649902344 - Pred: tensor([0.5694])\n",
      "Epoch 99 Loss: 419.8412170410156 - Pred: tensor([0.5687])\n",
      "Epoch 100 Loss: 419.52874755859375 - Pred: tensor([0.5680])\n",
      "Epoch 101 Loss: 419.2183532714844 - Pred: tensor([0.5673])\n",
      "Epoch 102 Loss: 418.9095764160156 - Pred: tensor([0.5666])\n",
      "Epoch 103 Loss: 418.6029052734375 - Pred: tensor([0.5660])\n",
      "Epoch 104 Loss: 418.2982177734375 - Pred: tensor([0.5653])\n",
      "Epoch 105 Loss: 417.9956970214844 - Pred: tensor([0.5646])\n",
      "Epoch 106 Loss: 417.6946716308594 - Pred: tensor([0.5639])\n",
      "Epoch 107 Loss: 417.39544677734375 - Pred: tensor([0.5632])\n",
      "Epoch 108 Loss: 417.09881591796875 - Pred: tensor([0.5626])\n",
      "Epoch 109 Loss: 416.8031311035156 - Pred: tensor([0.5619])\n",
      "Epoch 110 Loss: 416.5097351074219 - Pred: tensor([0.5612])\n",
      "Epoch 111 Loss: 416.2180480957031 - Pred: tensor([0.5606])\n",
      "Epoch 112 Loss: 415.92822265625 - Pred: tensor([0.5599])\n",
      "Epoch 113 Loss: 415.64056396484375 - Pred: tensor([0.5592])\n",
      "Epoch 114 Loss: 415.3539733886719 - Pred: tensor([0.5586])\n",
      "Epoch 115 Loss: 415.0697326660156 - Pred: tensor([0.5579])\n",
      "Epoch 116 Loss: 414.78717041015625 - Pred: tensor([0.5572])\n",
      "Epoch 117 Loss: 414.5063171386719 - Pred: tensor([0.5566])\n",
      "Epoch 118 Loss: 414.2275695800781 - Pred: tensor([0.5559])\n",
      "Epoch 119 Loss: 413.9498596191406 - Pred: tensor([0.5553])\n",
      "Epoch 120 Loss: 413.6744384765625 - Pred: tensor([0.5546])\n",
      "Epoch 121 Loss: 413.4007263183594 - Pred: tensor([0.5539])\n",
      "Epoch 122 Loss: 413.12835693359375 - Pred: tensor([0.5533])\n",
      "Epoch 123 Loss: 412.8578796386719 - Pred: tensor([0.5526])\n",
      "Epoch 124 Loss: 412.5895690917969 - Pred: tensor([0.5520])\n",
      "Epoch 125 Loss: 412.322265625 - Pred: tensor([0.5513])\n",
      "Epoch 126 Loss: 412.0567321777344 - Pred: tensor([0.5507])\n",
      "Epoch 127 Loss: 411.7931823730469 - Pred: tensor([0.5501])\n",
      "Epoch 128 Loss: 411.5315246582031 - Pred: tensor([0.5494])\n",
      "Epoch 129 Loss: 411.2707214355469 - Pred: tensor([0.5488])\n",
      "Epoch 130 Loss: 411.01239013671875 - Pred: tensor([0.5481])\n",
      "Epoch 131 Loss: 410.7548522949219 - Pred: tensor([0.5475])\n",
      "Epoch 132 Loss: 410.49932861328125 - Pred: tensor([0.5469])\n",
      "Epoch 133 Loss: 410.2454833984375 - Pred: tensor([0.5462])\n",
      "Epoch 134 Loss: 409.9934997558594 - Pred: tensor([0.5456])\n",
      "Epoch 135 Loss: 409.7425537109375 - Pred: tensor([0.5450])\n",
      "Epoch 136 Loss: 409.49359130859375 - Pred: tensor([0.5443])\n",
      "Epoch 137 Loss: 409.2461853027344 - Pred: tensor([0.5437])\n",
      "Epoch 138 Loss: 409.00018310546875 - Pred: tensor([0.5431])\n",
      "Epoch 139 Loss: 408.75555419921875 - Pred: tensor([0.5424])\n",
      "Epoch 140 Loss: 408.5124816894531 - Pred: tensor([0.5418])\n",
      "Epoch 141 Loss: 408.2714538574219 - Pred: tensor([0.5412])\n",
      "Epoch 142 Loss: 408.03118896484375 - Pred: tensor([0.5406])\n",
      "Epoch 143 Loss: 407.79302978515625 - Pred: tensor([0.5400])\n",
      "Epoch 144 Loss: 407.5563049316406 - Pred: tensor([0.5393])\n",
      "Epoch 145 Loss: 407.3208312988281 - Pred: tensor([0.5387])\n",
      "Epoch 146 Loss: 407.0873107910156 - Pred: tensor([0.5381])\n",
      "Epoch 147 Loss: 406.8546447753906 - Pred: tensor([0.5375])\n",
      "Epoch 148 Loss: 406.6239929199219 - Pred: tensor([0.5369])\n",
      "Epoch 149 Loss: 406.39422607421875 - Pred: tensor([0.5363])\n",
      "Epoch 150 Loss: 406.1663818359375 - Pred: tensor([0.5357])\n",
      "Epoch 151 Loss: 405.9401550292969 - Pred: tensor([0.5351])\n",
      "Epoch 152 Loss: 405.71490478515625 - Pred: tensor([0.5344])\n",
      "Epoch 153 Loss: 405.49102783203125 - Pred: tensor([0.5338])\n",
      "Epoch 154 Loss: 405.2689514160156 - Pred: tensor([0.5332])\n",
      "Epoch 155 Loss: 405.0482482910156 - Pred: tensor([0.5326])\n",
      "Epoch 156 Loss: 404.82891845703125 - Pred: tensor([0.5320])\n",
      "Epoch 157 Loss: 404.6108703613281 - Pred: tensor([0.5314])\n",
      "Epoch 158 Loss: 404.3941955566406 - Pred: tensor([0.5308])\n",
      "Epoch 159 Loss: 404.1792297363281 - Pred: tensor([0.5302])\n",
      "Epoch 160 Loss: 403.96502685546875 - Pred: tensor([0.5296])\n",
      "Epoch 161 Loss: 403.75262451171875 - Pred: tensor([0.5291])\n",
      "Epoch 162 Loss: 403.5417175292969 - Pred: tensor([0.5285])\n",
      "Epoch 163 Loss: 403.3317565917969 - Pred: tensor([0.5279])\n",
      "Epoch 164 Loss: 403.1232604980469 - Pred: tensor([0.5273])\n",
      "Epoch 165 Loss: 402.9158935546875 - Pred: tensor([0.5267])\n",
      "Epoch 166 Loss: 402.71051025390625 - Pred: tensor([0.5261])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167 Loss: 402.5061950683594 - Pred: tensor([0.5255])\n",
      "Epoch 168 Loss: 402.3025817871094 - Pred: tensor([0.5249])\n",
      "Epoch 169 Loss: 402.1008605957031 - Pred: tensor([0.5244])\n",
      "Epoch 170 Loss: 401.90020751953125 - Pred: tensor([0.5238])\n",
      "Epoch 171 Loss: 401.70135498046875 - Pred: tensor([0.5232])\n",
      "Epoch 172 Loss: 401.5031433105469 - Pred: tensor([0.5226])\n",
      "Epoch 173 Loss: 401.3067626953125 - Pred: tensor([0.5221])\n",
      "Epoch 174 Loss: 401.1109313964844 - Pred: tensor([0.5215])\n",
      "Epoch 175 Loss: 400.9168395996094 - Pred: tensor([0.5209])\n",
      "Epoch 176 Loss: 400.72393798828125 - Pred: tensor([0.5203])\n",
      "Epoch 177 Loss: 400.5323486328125 - Pred: tensor([0.5198])\n",
      "Epoch 178 Loss: 400.341796875 - Pred: tensor([0.5192])\n",
      "Epoch 179 Loss: 400.1529235839844 - Pred: tensor([0.5186])\n",
      "Epoch 180 Loss: 399.964599609375 - Pred: tensor([0.5181])\n",
      "Epoch 181 Loss: 399.7779541015625 - Pred: tensor([0.5175])\n",
      "Epoch 182 Loss: 399.5926513671875 - Pred: tensor([0.5169])\n",
      "Epoch 183 Loss: 399.408203125 - Pred: tensor([0.5164])\n",
      "Epoch 184 Loss: 399.2251281738281 - Pred: tensor([0.5158])\n",
      "Epoch 185 Loss: 399.0429382324219 - Pred: tensor([0.5153])\n",
      "Epoch 186 Loss: 398.8626708984375 - Pred: tensor([0.5147])\n",
      "Epoch 187 Loss: 398.6826477050781 - Pred: tensor([0.5142])\n",
      "Epoch 188 Loss: 398.50433349609375 - Pred: tensor([0.5136])\n",
      "Epoch 189 Loss: 398.3274230957031 - Pred: tensor([0.5131])\n",
      "Epoch 190 Loss: 398.1513366699219 - Pred: tensor([0.5125])\n",
      "Epoch 191 Loss: 397.976318359375 - Pred: tensor([0.5120])\n",
      "Epoch 192 Loss: 397.8023986816406 - Pred: tensor([0.5114])\n",
      "Epoch 193 Loss: 397.6300964355469 - Pred: tensor([0.5109])\n",
      "Epoch 194 Loss: 397.4583740234375 - Pred: tensor([0.5103])\n",
      "Epoch 195 Loss: 397.2882385253906 - Pred: tensor([0.5098])\n",
      "Epoch 196 Loss: 397.1191101074219 - Pred: tensor([0.5092])\n",
      "Epoch 197 Loss: 396.9505615234375 - Pred: tensor([0.5087])\n",
      "Epoch 198 Loss: 396.7838439941406 - Pred: tensor([0.5081])\n",
      "Epoch 199 Loss: 396.61785888671875 - Pred: tensor([0.5076])\n",
      "Epoch 200 Loss: 396.452880859375 - Pred: tensor([0.5071])\n",
      "Epoch 201 Loss: 396.2890319824219 - Pred: tensor([0.5065])\n",
      "Epoch 202 Loss: 396.1264343261719 - Pred: tensor([0.5060])\n",
      "Epoch 203 Loss: 395.96490478515625 - Pred: tensor([0.5055])\n",
      "Epoch 204 Loss: 395.8045959472656 - Pred: tensor([0.5049])\n",
      "Epoch 205 Loss: 395.6451721191406 - Pred: tensor([0.5044])\n",
      "Epoch 206 Loss: 395.4866027832031 - Pred: tensor([0.5039])\n",
      "Epoch 207 Loss: 395.3293762207031 - Pred: tensor([0.5034])\n",
      "Epoch 208 Loss: 395.1727600097656 - Pred: tensor([0.5028])\n",
      "Epoch 209 Loss: 395.0177917480469 - Pred: tensor([0.5023])\n",
      "Epoch 210 Loss: 394.8632507324219 - Pred: tensor([0.5018])\n",
      "Epoch 211 Loss: 394.71026611328125 - Pred: tensor([0.5013])\n",
      "Epoch 212 Loss: 394.5581359863281 - Pred: tensor([0.5007])\n",
      "Epoch 213 Loss: 394.406982421875 - Pred: tensor([0.5002])\n",
      "Epoch 214 Loss: 394.2568054199219 - Pred: tensor([0.4997])\n",
      "Epoch 215 Loss: 394.107666015625 - Pred: tensor([0.4992])\n",
      "Epoch 216 Loss: 393.9597473144531 - Pred: tensor([0.4987])\n",
      "Epoch 217 Loss: 393.81243896484375 - Pred: tensor([0.4982])\n",
      "Epoch 218 Loss: 393.6665344238281 - Pred: tensor([0.4977])\n",
      "Epoch 219 Loss: 393.5214538574219 - Pred: tensor([0.4971])\n",
      "Epoch 220 Loss: 393.37744140625 - Pred: tensor([0.4966])\n",
      "Epoch 221 Loss: 393.23419189453125 - Pred: tensor([0.4961])\n",
      "Epoch 222 Loss: 393.092041015625 - Pred: tensor([0.4956])\n",
      "Epoch 223 Loss: 392.9505615234375 - Pred: tensor([0.4951])\n",
      "Epoch 224 Loss: 392.8100891113281 - Pred: tensor([0.4946])\n",
      "Epoch 225 Loss: 392.6710510253906 - Pred: tensor([0.4941])\n",
      "Epoch 226 Loss: 392.5322570800781 - Pred: tensor([0.4936])\n",
      "Epoch 227 Loss: 392.3948669433594 - Pred: tensor([0.4931])\n",
      "Epoch 228 Loss: 392.2585144042969 - Pred: tensor([0.4926])\n",
      "Epoch 229 Loss: 392.1228332519531 - Pred: tensor([0.4921])\n",
      "Epoch 230 Loss: 391.9881591796875 - Pred: tensor([0.4916])\n",
      "Epoch 231 Loss: 391.85443115234375 - Pred: tensor([0.4911])\n",
      "Epoch 232 Loss: 391.7218933105469 - Pred: tensor([0.4906])\n",
      "Epoch 233 Loss: 391.5896911621094 - Pred: tensor([0.4902])\n",
      "Epoch 234 Loss: 391.4590148925781 - Pred: tensor([0.4897])\n",
      "Epoch 235 Loss: 391.3284606933594 - Pred: tensor([0.4892])\n",
      "Epoch 236 Loss: 391.19940185546875 - Pred: tensor([0.4887])\n",
      "Epoch 237 Loss: 391.0709228515625 - Pred: tensor([0.4882])\n",
      "Epoch 238 Loss: 390.943603515625 - Pred: tensor([0.4877])\n",
      "Epoch 239 Loss: 390.8167724609375 - Pred: tensor([0.4872])\n",
      "Epoch 240 Loss: 390.69110107421875 - Pred: tensor([0.4868])\n",
      "Epoch 241 Loss: 390.56640625 - Pred: tensor([0.4863])\n",
      "Epoch 242 Loss: 390.44207763671875 - Pred: tensor([0.4858])\n",
      "Epoch 243 Loss: 390.3191223144531 - Pred: tensor([0.4853])\n",
      "Epoch 244 Loss: 390.1968078613281 - Pred: tensor([0.4848])\n",
      "Epoch 245 Loss: 390.0745544433594 - Pred: tensor([0.4844])\n",
      "Epoch 246 Loss: 389.9535217285156 - Pred: tensor([0.4839])\n",
      "Epoch 247 Loss: 389.8330993652344 - Pred: tensor([0.4834])\n",
      "Epoch 248 Loss: 389.71343994140625 - Pred: tensor([0.4830])\n",
      "Epoch 249 Loss: 389.5949401855469 - Pred: tensor([0.4825])\n",
      "Epoch 250 Loss: 389.4769287109375 - Pred: tensor([0.4820])\n",
      "Epoch 251 Loss: 389.3605651855469 - Pred: tensor([0.4815])\n",
      "Epoch 252 Loss: 389.2445983886719 - Pred: tensor([0.4811])\n",
      "Epoch 253 Loss: 389.1297607421875 - Pred: tensor([0.4806])\n",
      "Epoch 254 Loss: 389.0158386230469 - Pred: tensor([0.4802])\n",
      "Epoch 255 Loss: 388.9025573730469 - Pred: tensor([0.4797])\n",
      "Epoch 256 Loss: 388.790283203125 - Pred: tensor([0.4792])\n",
      "Epoch 257 Loss: 388.6784362792969 - Pred: tensor([0.4788])\n",
      "Epoch 258 Loss: 388.5675964355469 - Pred: tensor([0.4783])\n",
      "Epoch 259 Loss: 388.4577941894531 - Pred: tensor([0.4779])\n",
      "Epoch 260 Loss: 388.34820556640625 - Pred: tensor([0.4774])\n",
      "Epoch 261 Loss: 388.23980712890625 - Pred: tensor([0.4770])\n",
      "Epoch 262 Loss: 388.13165283203125 - Pred: tensor([0.4765])\n",
      "Epoch 263 Loss: 388.0247497558594 - Pred: tensor([0.4761])\n",
      "Epoch 264 Loss: 387.91851806640625 - Pred: tensor([0.4756])\n",
      "Epoch 265 Loss: 387.8128662109375 - Pred: tensor([0.4752])\n",
      "Epoch 266 Loss: 387.707763671875 - Pred: tensor([0.4747])\n",
      "Epoch 267 Loss: 387.6031494140625 - Pred: tensor([0.4743])\n",
      "Epoch 268 Loss: 387.4985046386719 - Pred: tensor([0.4739])\n",
      "Epoch 269 Loss: 387.39044189453125 - Pred: tensor([0.4737])\n",
      "Epoch 270 Loss: 387.2810363769531 - Pred: tensor([0.4743])\n",
      "Epoch 271 Loss: 387.1746826171875 - Pred: tensor([0.4751])\n",
      "Epoch 272 Loss: 387.0725402832031 - Pred: tensor([0.4749])\n",
      "Epoch 273 Loss: 386.9713134765625 - Pred: tensor([0.4745])\n",
      "Epoch 274 Loss: 386.8700256347656 - Pred: tensor([0.4741])\n",
      "Epoch 275 Loss: 386.76800537109375 - Pred: tensor([0.4737])\n",
      "Epoch 276 Loss: 386.6654968261719 - Pred: tensor([0.4733])\n",
      "Epoch 277 Loss: 386.564697265625 - Pred: tensor([0.4729])\n",
      "Epoch 278 Loss: 386.4580383300781 - Pred: tensor([0.4725])\n",
      "Epoch 279 Loss: 386.3494873046875 - Pred: tensor([0.4721])\n",
      "Epoch 280 Loss: 386.2413635253906 - Pred: tensor([0.4717])\n",
      "Epoch 281 Loss: 386.1313171386719 - Pred: tensor([0.4713])\n",
      "Epoch 282 Loss: 386.02423095703125 - Pred: tensor([0.4709])\n",
      "Epoch 283 Loss: 385.907470703125 - Pred: tensor([0.4705])\n",
      "Epoch 284 Loss: 385.7737731933594 - Pred: tensor([0.4702])\n",
      "Epoch 285 Loss: 385.61480712890625 - Pred: tensor([0.4698])\n",
      "Epoch 286 Loss: 385.4413757324219 - Pred: tensor([0.4694])\n",
      "Epoch 287 Loss: 385.24078369140625 - Pred: tensor([0.4690])\n",
      "Epoch 288 Loss: 384.9793701171875 - Pred: tensor([0.4687])\n",
      "Epoch 289 Loss: 384.7127685546875 - Pred: tensor([0.4683])\n",
      "Epoch 290 Loss: 384.61309814453125 - Pred: tensor([0.4680])\n",
      "Epoch 291 Loss: 384.62030029296875 - Pred: tensor([0.4676])\n",
      "Epoch 292 Loss: 384.57318115234375 - Pred: tensor([0.4673])\n",
      "Epoch 293 Loss: 384.44384765625 - Pred: tensor([0.4669])\n",
      "Epoch 294 Loss: 384.2646789550781 - Pred: tensor([0.4666])\n",
      "Epoch 295 Loss: 384.0668029785156 - Pred: tensor([0.4663])\n",
      "Epoch 296 Loss: 383.897216796875 - Pred: tensor([0.4659])\n",
      "Epoch 297 Loss: 383.80535888671875 - Pred: tensor([0.4656])\n",
      "Epoch 298 Loss: 383.74395751953125 - Pred: tensor([0.4653])\n",
      "Epoch 299 Loss: 383.65679931640625 - Pred: tensor([0.4650])\n",
      "Epoch 300 Loss: 383.5511779785156 - Pred: tensor([0.4646])\n",
      "Epoch 301 Loss: 383.42498779296875 - Pred: tensor([0.4643])\n",
      "Epoch 302 Loss: 383.2591857910156 - Pred: tensor([0.4640])\n",
      "Epoch 303 Loss: 383.09149169921875 - Pred: tensor([0.4637])\n",
      "Epoch 304 Loss: 382.9803466796875 - Pred: tensor([0.4633])\n",
      "Epoch 305 Loss: 382.8901062011719 - Pred: tensor([0.4630])\n",
      "Epoch 306 Loss: 382.79608154296875 - Pred: tensor([0.4627])\n",
      "Epoch 307 Loss: 382.68603515625 - Pred: tensor([0.4624])\n",
      "Epoch 308 Loss: 382.56378173828125 - Pred: tensor([0.4621])\n",
      "Epoch 309 Loss: 382.4349060058594 - Pred: tensor([0.4618])\n",
      "Epoch 310 Loss: 382.3060607910156 - Pred: tensor([0.4615])\n",
      "Epoch 311 Loss: 382.1820983886719 - Pred: tensor([0.4612])\n",
      "Epoch 312 Loss: 382.06622314453125 - Pred: tensor([0.4609])\n",
      "Epoch 313 Loss: 381.9588928222656 - Pred: tensor([0.4605])\n",
      "Epoch 314 Loss: 381.8563537597656 - Pred: tensor([0.4602])\n",
      "Epoch 315 Loss: 381.7497863769531 - Pred: tensor([0.4599])\n",
      "Epoch 316 Loss: 381.6360778808594 - Pred: tensor([0.4596])\n",
      "Epoch 317 Loss: 381.5199279785156 - Pred: tensor([0.4593])\n",
      "Epoch 318 Loss: 381.4056701660156 - Pred: tensor([0.4590])\n",
      "Epoch 319 Loss: 381.2873840332031 - Pred: tensor([0.4587])\n",
      "Epoch 320 Loss: 381.16021728515625 - Pred: tensor([0.4584])\n",
      "Epoch 321 Loss: 381.0408630371094 - Pred: tensor([0.4581])\n",
      "Epoch 322 Loss: 380.9489440917969 - Pred: tensor([0.4578])\n",
      "Epoch 323 Loss: 380.8399963378906 - Pred: tensor([0.4575])\n",
      "Epoch 324 Loss: 380.7074279785156 - Pred: tensor([0.4572])\n",
      "Epoch 325 Loss: 380.5934753417969 - Pred: tensor([0.4570])\n",
      "Epoch 326 Loss: 380.49578857421875 - Pred: tensor([0.4567])\n",
      "Epoch 327 Loss: 380.38934326171875 - Pred: tensor([0.4564])\n",
      "Epoch 328 Loss: 380.2694091796875 - Pred: tensor([0.4561])\n",
      "Epoch 329 Loss: 380.1526794433594 - Pred: tensor([0.4558])\n",
      "Epoch 330 Loss: 380.0545654296875 - Pred: tensor([0.4555])\n",
      "Epoch 331 Loss: 379.9518127441406 - Pred: tensor([0.4552])\n",
      "Epoch 332 Loss: 379.8361511230469 - Pred: tensor([0.4549])\n",
      "Epoch 333 Loss: 379.7291259765625 - Pred: tensor([0.4546])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334 Loss: 379.63275146484375 - Pred: tensor([0.4544])\n",
      "Epoch 335 Loss: 379.5294189453125 - Pred: tensor([0.4541])\n",
      "Epoch 336 Loss: 379.4208984375 - Pred: tensor([0.4538])\n",
      "Epoch 337 Loss: 379.320068359375 - Pred: tensor([0.4535])\n",
      "Epoch 338 Loss: 379.22406005859375 - Pred: tensor([0.4532])\n",
      "Epoch 339 Loss: 379.12445068359375 - Pred: tensor([0.4530])\n",
      "Epoch 340 Loss: 379.0223388671875 - Pred: tensor([0.4527])\n",
      "Epoch 341 Loss: 378.9244384765625 - Pred: tensor([0.4524])\n",
      "Epoch 342 Loss: 378.82952880859375 - Pred: tensor([0.4521])\n",
      "Epoch 343 Loss: 378.7342529296875 - Pred: tensor([0.4519])\n",
      "Epoch 344 Loss: 378.63714599609375 - Pred: tensor([0.4516])\n",
      "Epoch 345 Loss: 378.5412902832031 - Pred: tensor([0.4513])\n",
      "Epoch 346 Loss: 378.44769287109375 - Pred: tensor([0.4510])\n",
      "Epoch 347 Loss: 378.355224609375 - Pred: tensor([0.4508])\n",
      "Epoch 348 Loss: 378.26226806640625 - Pred: tensor([0.4505])\n",
      "Epoch 349 Loss: 378.1691589355469 - Pred: tensor([0.4502])\n",
      "Epoch 350 Loss: 378.0765075683594 - Pred: tensor([0.4500])\n",
      "Epoch 351 Loss: 377.9855651855469 - Pred: tensor([0.4497])\n",
      "Epoch 352 Loss: 377.89501953125 - Pred: tensor([0.4495])\n",
      "Epoch 353 Loss: 377.80517578125 - Pred: tensor([0.4492])\n",
      "Epoch 354 Loss: 377.7149658203125 - Pred: tensor([0.4489])\n",
      "Epoch 355 Loss: 377.6253356933594 - Pred: tensor([0.4487])\n",
      "Epoch 356 Loss: 377.5362548828125 - Pred: tensor([0.4484])\n",
      "Epoch 357 Loss: 377.4477844238281 - Pred: tensor([0.4482])\n",
      "Epoch 358 Loss: 377.35931396484375 - Pred: tensor([0.4479])\n",
      "Epoch 359 Loss: 377.2716979980469 - Pred: tensor([0.4477])\n",
      "Epoch 360 Loss: 377.18450927734375 - Pred: tensor([0.4474])\n",
      "Epoch 361 Loss: 377.0976867675781 - Pred: tensor([0.4471])\n",
      "Epoch 362 Loss: 377.0107421875 - Pred: tensor([0.4469])\n",
      "Epoch 363 Loss: 376.92437744140625 - Pred: tensor([0.4466])\n",
      "Epoch 364 Loss: 376.8382263183594 - Pred: tensor([0.4464])\n",
      "Epoch 365 Loss: 376.7528381347656 - Pred: tensor([0.4461])\n",
      "Epoch 366 Loss: 376.6673278808594 - Pred: tensor([0.4459])\n",
      "Epoch 367 Loss: 376.58245849609375 - Pred: tensor([0.4457])\n",
      "Epoch 368 Loss: 376.49774169921875 - Pred: tensor([0.4454])\n",
      "Epoch 369 Loss: 376.412841796875 - Pred: tensor([0.4452])\n",
      "Epoch 370 Loss: 376.3283386230469 - Pred: tensor([0.4449])\n",
      "Epoch 371 Loss: 376.2435607910156 - Pred: tensor([0.4447])\n",
      "Epoch 372 Loss: 376.1582946777344 - Pred: tensor([0.4445])\n",
      "Epoch 373 Loss: 376.0715637207031 - Pred: tensor([0.4442])\n",
      "Epoch 374 Loss: 375.9820251464844 - Pred: tensor([0.4440])\n",
      "Epoch 375 Loss: 375.89569091796875 - Pred: tensor([0.4437])\n",
      "Epoch 376 Loss: 375.815185546875 - Pred: tensor([0.4435])\n",
      "Epoch 377 Loss: 375.7272033691406 - Pred: tensor([0.4433])\n",
      "Epoch 378 Loss: 375.63714599609375 - Pred: tensor([0.4430])\n",
      "Epoch 379 Loss: 375.55413818359375 - Pred: tensor([0.4428])\n",
      "Epoch 380 Loss: 375.4712219238281 - Pred: tensor([0.4426])\n",
      "Epoch 381 Loss: 375.38348388671875 - Pred: tensor([0.4424])\n",
      "Epoch 382 Loss: 375.2978515625 - Pred: tensor([0.4421])\n",
      "Epoch 383 Loss: 375.2154846191406 - Pred: tensor([0.4419])\n",
      "Epoch 384 Loss: 375.1327209472656 - Pred: tensor([0.4417])\n",
      "Epoch 385 Loss: 375.0483093261719 - Pred: tensor([0.4415])\n",
      "Epoch 386 Loss: 374.9647521972656 - Pred: tensor([0.4412])\n",
      "Epoch 387 Loss: 374.88311767578125 - Pred: tensor([0.4410])\n",
      "Epoch 388 Loss: 374.80194091796875 - Pred: tensor([0.4408])\n",
      "Epoch 389 Loss: 374.7198181152344 - Pred: tensor([0.4406])\n",
      "Epoch 390 Loss: 374.6373291015625 - Pred: tensor([0.4404])\n",
      "Epoch 391 Loss: 374.5555725097656 - Pred: tensor([0.4401])\n",
      "Epoch 392 Loss: 374.474853515625 - Pred: tensor([0.4399])\n",
      "Epoch 393 Loss: 374.3943786621094 - Pred: tensor([0.4397])\n",
      "Epoch 394 Loss: 374.3132019042969 - Pred: tensor([0.4395])\n",
      "Epoch 395 Loss: 374.2325134277344 - Pred: tensor([0.4393])\n",
      "Epoch 396 Loss: 374.1516418457031 - Pred: tensor([0.4391])\n",
      "Epoch 397 Loss: 374.07196044921875 - Pred: tensor([0.4389])\n",
      "Epoch 398 Loss: 373.99163818359375 - Pred: tensor([0.4387])\n",
      "Epoch 399 Loss: 373.9114074707031 - Pred: tensor([0.4385])\n",
      "Epoch 400 Loss: 373.8310852050781 - Pred: tensor([0.4383])\n",
      "Epoch 401 Loss: 373.7511901855469 - Pred: tensor([0.4381])\n",
      "Epoch 402 Loss: 373.6711120605469 - Pred: tensor([0.4379])\n",
      "Epoch 403 Loss: 373.5908203125 - Pred: tensor([0.4377])\n",
      "Epoch 404 Loss: 373.51007080078125 - Pred: tensor([0.4375])\n",
      "Epoch 405 Loss: 373.4288330078125 - Pred: tensor([0.4373])\n",
      "Epoch 406 Loss: 373.34710693359375 - Pred: tensor([0.4371])\n",
      "Epoch 407 Loss: 373.2652587890625 - Pred: tensor([0.4369])\n",
      "Epoch 408 Loss: 373.1833801269531 - Pred: tensor([0.4367])\n",
      "Epoch 409 Loss: 373.1017150878906 - Pred: tensor([0.4365])\n",
      "Epoch 410 Loss: 373.0191650390625 - Pred: tensor([0.4363])\n",
      "Epoch 411 Loss: 372.9370422363281 - Pred: tensor([0.4361])\n",
      "Epoch 412 Loss: 372.8545227050781 - Pred: tensor([0.4359])\n",
      "Epoch 413 Loss: 372.77239990234375 - Pred: tensor([0.4358])\n",
      "Epoch 414 Loss: 372.6894836425781 - Pred: tensor([0.4356])\n",
      "Epoch 415 Loss: 372.6065979003906 - Pred: tensor([0.4354])\n",
      "Epoch 416 Loss: 372.523681640625 - Pred: tensor([0.4352])\n",
      "Epoch 417 Loss: 372.4403991699219 - Pred: tensor([0.4350])\n",
      "Epoch 418 Loss: 372.3572692871094 - Pred: tensor([0.4349])\n",
      "Epoch 419 Loss: 372.2737121582031 - Pred: tensor([0.4347])\n",
      "Epoch 420 Loss: 372.19049072265625 - Pred: tensor([0.4345])\n",
      "Epoch 421 Loss: 372.1072998046875 - Pred: tensor([0.4343])\n",
      "Epoch 422 Loss: 372.02410888671875 - Pred: tensor([0.4342])\n",
      "Epoch 423 Loss: 371.9407653808594 - Pred: tensor([0.4340])\n",
      "Epoch 424 Loss: 371.8582458496094 - Pred: tensor([0.4338])\n",
      "Epoch 425 Loss: 371.7752380371094 - Pred: tensor([0.4337])\n",
      "Epoch 426 Loss: 371.6922607421875 - Pred: tensor([0.4335])\n",
      "Epoch 427 Loss: 371.6094970703125 - Pred: tensor([0.4333])\n",
      "Epoch 428 Loss: 371.52716064453125 - Pred: tensor([0.4332])\n",
      "Epoch 429 Loss: 371.4435729980469 - Pred: tensor([0.4330])\n",
      "Epoch 430 Loss: 371.3579406738281 - Pred: tensor([0.4328])\n",
      "Epoch 431 Loss: 371.26116943359375 - Pred: tensor([0.4327])\n",
      "Epoch 432 Loss: 371.1746826171875 - Pred: tensor([0.4325])\n",
      "Epoch 433 Loss: 371.07830810546875 - Pred: tensor([0.4324])\n",
      "Epoch 434 Loss: 370.989501953125 - Pred: tensor([0.4322])\n",
      "Epoch 435 Loss: 370.9114074707031 - Pred: tensor([0.4320])\n",
      "Epoch 436 Loss: 370.8218688964844 - Pred: tensor([0.4319])\n",
      "Epoch 437 Loss: 370.7415771484375 - Pred: tensor([0.4317])\n",
      "Epoch 438 Loss: 370.66302490234375 - Pred: tensor([0.4316])\n",
      "Epoch 439 Loss: 370.5780029296875 - Pred: tensor([0.4314])\n",
      "Epoch 440 Loss: 370.4950256347656 - Pred: tensor([0.4313])\n",
      "Epoch 441 Loss: 370.41204833984375 - Pred: tensor([0.4312])\n",
      "Epoch 442 Loss: 370.3197326660156 - Pred: tensor([0.4310])\n",
      "Epoch 443 Loss: 370.2198486328125 - Pred: tensor([0.4309])\n",
      "Epoch 444 Loss: 370.10009765625 - Pred: tensor([0.4307])\n",
      "Epoch 445 Loss: 369.9346923828125 - Pred: tensor([0.4306])\n",
      "Epoch 446 Loss: 369.69830322265625 - Pred: tensor([0.4305])\n",
      "Epoch 447 Loss: 369.3417053222656 - Pred: tensor([0.4303])\n",
      "Epoch 448 Loss: 368.7984619140625 - Pred: tensor([0.4302])\n",
      "Epoch 449 Loss: 368.30377197265625 - Pred: tensor([0.4301])\n",
      "Epoch 450 Loss: 368.0752868652344 - Pred: tensor([0.4299])\n",
      "Epoch 451 Loss: 368.1876525878906 - Pred: tensor([0.4298])\n",
      "Epoch 452 Loss: 368.24639892578125 - Pred: tensor([0.4297])\n",
      "Epoch 453 Loss: 368.1144714355469 - Pred: tensor([0.4295])\n",
      "Epoch 454 Loss: 367.8182067871094 - Pred: tensor([0.4294])\n",
      "Epoch 455 Loss: 367.43170166015625 - Pred: tensor([0.4293])\n",
      "Epoch 456 Loss: 367.1355285644531 - Pred: tensor([0.4292])\n",
      "Epoch 457 Loss: 367.12591552734375 - Pred: tensor([0.4291])\n",
      "Epoch 458 Loss: 367.0271301269531 - Pred: tensor([0.4290])\n",
      "Epoch 459 Loss: 366.8895263671875 - Pred: tensor([0.4289])\n",
      "Epoch 460 Loss: 366.76483154296875 - Pred: tensor([0.4288])\n",
      "Epoch 461 Loss: 366.5809326171875 - Pred: tensor([0.4287])\n",
      "Epoch 462 Loss: 366.3766174316406 - Pred: tensor([0.4285])\n",
      "Epoch 463 Loss: 366.1351318359375 - Pred: tensor([0.4284])\n",
      "Epoch 464 Loss: 365.9139709472656 - Pred: tensor([0.4283])\n",
      "Epoch 465 Loss: 365.7817687988281 - Pred: tensor([0.4283])\n",
      "Epoch 466 Loss: 365.6539001464844 - Pred: tensor([0.4282])\n",
      "Epoch 467 Loss: 365.5218505859375 - Pred: tensor([0.4281])\n",
      "Epoch 468 Loss: 365.3324279785156 - Pred: tensor([0.4280])\n",
      "Epoch 469 Loss: 365.0914611816406 - Pred: tensor([0.4279])\n",
      "Epoch 470 Loss: 364.8788146972656 - Pred: tensor([0.4278])\n",
      "Epoch 471 Loss: 364.74566650390625 - Pred: tensor([0.4277])\n",
      "Epoch 472 Loss: 364.60101318359375 - Pred: tensor([0.4276])\n",
      "Epoch 473 Loss: 364.4425964355469 - Pred: tensor([0.4276])\n",
      "Epoch 474 Loss: 364.26556396484375 - Pred: tensor([0.4275])\n",
      "Epoch 475 Loss: 364.0423889160156 - Pred: tensor([0.4274])\n",
      "Epoch 476 Loss: 363.8504943847656 - Pred: tensor([0.4273])\n",
      "Epoch 477 Loss: 363.70404052734375 - Pred: tensor([0.4273])\n",
      "Epoch 478 Loss: 363.5479431152344 - Pred: tensor([0.4272])\n",
      "Epoch 479 Loss: 363.3671875 - Pred: tensor([0.4271])\n",
      "Epoch 480 Loss: 363.1572570800781 - Pred: tensor([0.4271])\n",
      "Epoch 481 Loss: 362.98211669921875 - Pred: tensor([0.4270])\n",
      "Epoch 482 Loss: 362.8361511230469 - Pred: tensor([0.4270])\n",
      "Epoch 483 Loss: 362.6595458984375 - Pred: tensor([0.4269])\n",
      "Epoch 484 Loss: 362.466552734375 - Pred: tensor([0.4269])\n",
      "Epoch 485 Loss: 362.276611328125 - Pred: tensor([0.4268])\n",
      "Epoch 486 Loss: 362.1065673828125 - Pred: tensor([0.4268])\n",
      "Epoch 487 Loss: 361.94598388671875 - Pred: tensor([0.4267])\n",
      "Epoch 488 Loss: 361.7537841796875 - Pred: tensor([0.4267])\n",
      "Epoch 489 Loss: 361.5643615722656 - Pred: tensor([0.4266])\n",
      "Epoch 490 Loss: 361.3990478515625 - Pred: tensor([0.4266])\n",
      "Epoch 491 Loss: 361.2252502441406 - Pred: tensor([0.4266])\n",
      "Epoch 492 Loss: 361.0392150878906 - Pred: tensor([0.4265])\n",
      "Epoch 493 Loss: 360.8536071777344 - Pred: tensor([0.4265])\n",
      "Epoch 494 Loss: 360.6760559082031 - Pred: tensor([0.4265])\n",
      "Epoch 495 Loss: 360.50213623046875 - Pred: tensor([0.4264])\n",
      "Epoch 496 Loss: 360.31671142578125 - Pred: tensor([0.4264])\n",
      "Epoch 497 Loss: 360.12774658203125 - Pred: tensor([0.4264])\n",
      "Epoch 498 Loss: 359.9423522949219 - Pred: tensor([0.4264])\n",
      "Epoch 499 Loss: 359.7439880371094 - Pred: tensor([0.4263])\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epochs in range(500):\n",
    "    y_pred = model(x_data.float())\n",
    "    loss = criterion(y_pred, y_data.view(-1,1).float())\n",
    "    print('Epoch',epochs,'Loss:',loss.item(), '- Pred:', y_pred.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#The error function is the function representing the difference between the values \n",
    "#computed by your model and the real values. In the optimization field often they\n",
    "#speak about two phases: a training phase in which the model is set,\n",
    "#and a test phase in which the model tests its behaviour against the real values of output.\n",
    "#In the training phase the error is necessary to improve the model, \n",
    "#while in the test phase the error is useful to check if the model works properly.\n",
    "\n",
    "#The objective function is the function you want to maximize or minimize. \n",
    "#When they call it \"cost function\" (again, it's the objective function)\n",
    "#it's because they want to only minimize it. \n",
    "#I see the cost function and the objective function as the same thing seen from slightly \n",
    "#different perspectives.\n",
    "\n",
    "#The \"criterion\" is usually the rule for stopping the algorithm you're using.\n",
    "#Suppose you want that your model find the minimum of an objective function,\n",
    "#in real experiences it is often hard to find the exact minimum and the algorithm \n",
    "#could continuing to work for a very long time. In that case you could accept to \n",
    "#stop it \"near\" to the optimum with a particular stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True, False,  True,  True, False, False, False])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.data[0]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False]), tensor(1))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.double().forward(x_data.data[25]) > 0.5, y_data[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4263], dtype=torch.float64, grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.double().forward(x_data.data[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.data[25]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False]), tensor(0))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.double().forward(x_data.data[55]) > 0.5, y_data[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.double().forward(x_data) > 0.5\n",
    "pred.numpy()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569,), (569,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pred.numpy()\n",
    "b = y_data.numpy()\n",
    "pred.numpy().reshape(-1).shape, y_data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "c = confusion_matrix(a,b)\n",
    "sns.heatmap(c, annot=True, xticklabels=le.classes_, yticklabels=le.classes_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.92      0.95       380\n",
      "        True       0.85      0.96      0.90       189\n",
      "\n",
      "    accuracy                           0.93       569\n",
      "   macro avg       0.92      0.94      0.92       569\n",
      "weighted avg       0.94      0.93      0.93       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(a,b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('soldgame': conda)",
   "language": "python",
   "name": "python37764bitsoldgameconda7262750cf6184ac8bf201f1ff27a4c4f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
