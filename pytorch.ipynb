{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd  # Import Pandas for data manipulation using dataframes\n",
    "import numpy as np  # Import Numpy for data statistical analysis\n",
    "import matplotlib.pyplot as plt  # Import matplotlib for data visualisation\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "import os\n",
    "\n",
    "data = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "\n",
    "cols = data.columns\n",
    "x_data = data[cols[2:-1]]\n",
    "print(len(cols))\n",
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1]), array(['B', 'M'], dtype=object))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = data[cols[1]] #diagnosis\n",
    "le = LabelEncoder()\n",
    "y_data = np.array(le.fit_transform(y_data))\n",
    "y_data[:5], le.classes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle \\left( \\left( 569\\right), \\  \\left( 569, \\  30\\right)\\right)$"
      ],
      "text/plain": [
       "((569,), (569, 30))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape, x_data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = Variable(torch.from_numpy(x_data.values))\n",
    "y_data = Variable(torch.from_numpy(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4059],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4080],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4064],\n",
       "        [0.4075],\n",
       "        [0.4080],\n",
       "        [0.4075],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4059],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4059],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4075],\n",
       "        [0.4090],\n",
       "        [0.4133],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4074],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4078],\n",
       "        [0.4074],\n",
       "        [0.4082],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4070],\n",
       "        [0.4071],\n",
       "        [0.4077],\n",
       "        [0.4073],\n",
       "        [0.4072],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4078],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4074],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4083],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4077],\n",
       "        [0.4071],\n",
       "        [0.4081],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4075],\n",
       "        [0.4072],\n",
       "        [0.4080],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4074],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4083],\n",
       "        [0.4072],\n",
       "        [0.4074],\n",
       "        [0.4073],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4080],\n",
       "        [0.4074],\n",
       "        [0.4073],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4083],\n",
       "        [0.4082],\n",
       "        [0.4079],\n",
       "        [0.4073],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4074],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4080],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4071],\n",
       "        [0.4080],\n",
       "        [0.4071],\n",
       "        [0.4076],\n",
       "        [0.4075],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4076],\n",
       "        [0.4072],\n",
       "        [0.4076],\n",
       "        [0.4071],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4079],\n",
       "        [0.4076],\n",
       "        [0.4073],\n",
       "        [0.4072],\n",
       "        [0.4079],\n",
       "        [0.4074],\n",
       "        [0.4077],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4080],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4080],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4088],\n",
       "        [0.4074],\n",
       "        [0.4077],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4077],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4074],\n",
       "        [0.4075],\n",
       "        [0.4081],\n",
       "        [0.4080],\n",
       "        [0.4072],\n",
       "        [0.4074],\n",
       "        [0.4118],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4074],\n",
       "        [0.4082],\n",
       "        [0.4065],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4081],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4129],\n",
       "        [0.4090],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4073],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4059],\n",
       "        [0.4079],\n",
       "        [0.4079],\n",
       "        [0.4074],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4076],\n",
       "        [0.4082],\n",
       "        [0.4077],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4082],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4075],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4078],\n",
       "        [0.4071],\n",
       "        [0.4080],\n",
       "        [0.4082],\n",
       "        [0.4074],\n",
       "        [0.4076],\n",
       "        [0.4082],\n",
       "        [0.4072],\n",
       "        [0.4082],\n",
       "        [0.4076],\n",
       "        [0.4071],\n",
       "        [0.4075],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4074],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4071],\n",
       "        [0.4076],\n",
       "        [0.4071],\n",
       "        [0.4059],\n",
       "        [0.4071],\n",
       "        [0.4080],\n",
       "        [0.4081],\n",
       "        [0.4074],\n",
       "        [0.4082],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4072],\n",
       "        [0.4113],\n",
       "        [0.4079],\n",
       "        [0.4082],\n",
       "        [0.4073],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4078],\n",
       "        [0.4081],\n",
       "        [0.4081],\n",
       "        [0.4073],\n",
       "        [0.4082],\n",
       "        [0.4080],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4079],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4083],\n",
       "        [0.4071],\n",
       "        [0.4075],\n",
       "        [0.4091],\n",
       "        [0.4071],\n",
       "        [0.4077],\n",
       "        [0.4071],\n",
       "        [0.4081],\n",
       "        [0.4074],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4076],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4075],\n",
       "        [0.4074],\n",
       "        [0.4082],\n",
       "        [0.4097],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4089],\n",
       "        [0.4072],\n",
       "        [0.4072],\n",
       "        [0.4080],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4081],\n",
       "        [0.4080],\n",
       "        [0.4081],\n",
       "        [0.4071],\n",
       "        [0.4080],\n",
       "        [0.4071],\n",
       "        [0.4077],\n",
       "        [0.4080],\n",
       "        [0.4073],\n",
       "        [0.4081],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4079],\n",
       "        [0.4072],\n",
       "        [0.4073],\n",
       "        [0.4078],\n",
       "        [0.4073],\n",
       "        [0.4071],\n",
       "        [0.4080],\n",
       "        [0.4075],\n",
       "        [0.4071],\n",
       "        [0.4074],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4081],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4077],\n",
       "        [0.4082],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4059],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4088],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4077],\n",
       "        [0.4083],\n",
       "        [0.4072],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4080],\n",
       "        [0.4078],\n",
       "        [0.4077],\n",
       "        [0.4082],\n",
       "        [0.4074],\n",
       "        [0.4071],\n",
       "        [0.4070],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4082],\n",
       "        [0.4080],\n",
       "        [0.4078],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4079],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4081],\n",
       "        [0.4078],\n",
       "        [0.4071],\n",
       "        [0.4077],\n",
       "        [0.4071],\n",
       "        [0.4059],\n",
       "        [0.4082],\n",
       "        [0.4075],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4080],\n",
       "        [0.4080],\n",
       "        [0.4071],\n",
       "        [0.4075],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4081],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4075],\n",
       "        [0.4075],\n",
       "        [0.4073],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4079],\n",
       "        [0.4086],\n",
       "        [0.4077],\n",
       "        [0.4081],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4078],\n",
       "        [0.4071],\n",
       "        [0.4079],\n",
       "        [0.4082],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4076],\n",
       "        [0.4074],\n",
       "        [0.4072],\n",
       "        [0.4082],\n",
       "        [0.4080],\n",
       "        [0.4077],\n",
       "        [0.4071],\n",
       "        [0.4059],\n",
       "        [0.4082],\n",
       "        [0.4079],\n",
       "        [0.4076],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4073],\n",
       "        [0.4078],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4080],\n",
       "        [0.4082],\n",
       "        [0.4078],\n",
       "        [0.4071],\n",
       "        [0.4081],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4072],\n",
       "        [0.4080],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4081],\n",
       "        [0.4078],\n",
       "        [0.4082],\n",
       "        [0.4072],\n",
       "        [0.4077],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4076],\n",
       "        [0.4071],\n",
       "        [0.4081],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4072],\n",
       "        [0.4082],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4077],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4075],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4076],\n",
       "        [0.4071],\n",
       "        [0.4073],\n",
       "        [0.4074],\n",
       "        [0.4081],\n",
       "        [0.4082],\n",
       "        [0.4075],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4071],\n",
       "        [0.4081],\n",
       "        [0.4071],\n",
       "        [0.4082],\n",
       "        [0.4075],\n",
       "        [0.4071],\n",
       "        [0.4072],\n",
       "        [0.4078],\n",
       "        [0.4081],\n",
       "        [0.4080],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4072],\n",
       "        [0.4077],\n",
       "        [0.4081],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4079],\n",
       "        [0.4079],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4082],\n",
       "        [0.4084],\n",
       "        [0.4081],\n",
       "        [0.4073],\n",
       "        [0.4082],\n",
       "        [0.4078],\n",
       "        [0.4081],\n",
       "        [0.4081],\n",
       "        [0.4078],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071],\n",
       "        [0.4071]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(30,16)\n",
    "        self.l2 = torch.nn.Linear(16,4)\n",
    "        self.l3 = torch.nn.Linear(4,1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x)) \n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "y_pred = model(x_data.float())\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([569, 30]), torch.Size([569]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.float().size(), y_data.float().size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 377.4519348144531 - Pred: tensor([0.4059])\n",
      "Epoch 1 Loss: 377.2734069824219 - Pred: tensor([0.4050])\n",
      "Epoch 2 Loss: 377.10272216796875 - Pred: tensor([0.4041])\n",
      "Epoch 3 Loss: 376.9659118652344 - Pred: tensor([0.4033])\n",
      "Epoch 4 Loss: 376.8608093261719 - Pred: tensor([0.4024])\n",
      "Epoch 5 Loss: 376.7729187011719 - Pred: tensor([0.4015])\n",
      "Epoch 6 Loss: 376.6984558105469 - Pred: tensor([0.4007])\n",
      "Epoch 7 Loss: 376.630859375 - Pred: tensor([0.3999])\n",
      "Epoch 8 Loss: 376.5677490234375 - Pred: tensor([0.3994])\n",
      "Epoch 9 Loss: 376.5089416503906 - Pred: tensor([0.3986])\n",
      "Epoch 10 Loss: 376.45367431640625 - Pred: tensor([0.3977])\n",
      "Epoch 11 Loss: 376.4006652832031 - Pred: tensor([0.3968])\n",
      "Epoch 12 Loss: 376.3505554199219 - Pred: tensor([0.3960])\n",
      "Epoch 13 Loss: 376.30303955078125 - Pred: tensor([0.3951])\n",
      "Epoch 14 Loss: 376.25689697265625 - Pred: tensor([0.3942])\n",
      "Epoch 15 Loss: 376.212890625 - Pred: tensor([0.3934])\n",
      "Epoch 16 Loss: 376.1711730957031 - Pred: tensor([0.3925])\n",
      "Epoch 17 Loss: 376.1313171386719 - Pred: tensor([0.3917])\n",
      "Epoch 18 Loss: 376.0939636230469 - Pred: tensor([0.3909])\n",
      "Epoch 19 Loss: 376.05902099609375 - Pred: tensor([0.3901])\n",
      "Epoch 20 Loss: 376.0264892578125 - Pred: tensor([0.3893])\n",
      "Epoch 21 Loss: 375.99615478515625 - Pred: tensor([0.3885])\n",
      "Epoch 22 Loss: 375.9674987792969 - Pred: tensor([0.3878])\n",
      "Epoch 23 Loss: 375.94091796875 - Pred: tensor([0.3870])\n",
      "Epoch 24 Loss: 375.9166259765625 - Pred: tensor([0.3863])\n",
      "Epoch 25 Loss: 375.89361572265625 - Pred: tensor([0.3856])\n",
      "Epoch 26 Loss: 375.8722229003906 - Pred: tensor([0.3849])\n",
      "Epoch 27 Loss: 375.8531799316406 - Pred: tensor([0.3842])\n",
      "Epoch 28 Loss: 375.8351745605469 - Pred: tensor([0.3836])\n",
      "Epoch 29 Loss: 375.818603515625 - Pred: tensor([0.3830])\n",
      "Epoch 30 Loss: 375.80322265625 - Pred: tensor([0.3823])\n",
      "Epoch 31 Loss: 375.78997802734375 - Pred: tensor([0.3817])\n",
      "Epoch 32 Loss: 375.7772521972656 - Pred: tensor([0.3812])\n",
      "Epoch 33 Loss: 375.765625 - Pred: tensor([0.3806])\n",
      "Epoch 34 Loss: 375.75555419921875 - Pred: tensor([0.3801])\n",
      "Epoch 35 Loss: 375.7462158203125 - Pred: tensor([0.3795])\n",
      "Epoch 36 Loss: 375.7375183105469 - Pred: tensor([0.3790])\n",
      "Epoch 37 Loss: 375.730224609375 - Pred: tensor([0.3786])\n",
      "Epoch 38 Loss: 375.72314453125 - Pred: tensor([0.3781])\n",
      "Epoch 39 Loss: 375.71746826171875 - Pred: tensor([0.3776])\n",
      "Epoch 40 Loss: 375.7120666503906 - Pred: tensor([0.3772])\n",
      "Epoch 41 Loss: 375.70703125 - Pred: tensor([0.3768])\n",
      "Epoch 42 Loss: 375.7029113769531 - Pred: tensor([0.3764])\n",
      "Epoch 43 Loss: 375.69873046875 - Pred: tensor([0.3761])\n",
      "Epoch 44 Loss: 375.6954650878906 - Pred: tensor([0.3757])\n",
      "Epoch 45 Loss: 375.69195556640625 - Pred: tensor([0.3754])\n",
      "Epoch 46 Loss: 375.68902587890625 - Pred: tensor([0.3751])\n",
      "Epoch 47 Loss: 375.6859130859375 - Pred: tensor([0.3748])\n",
      "Epoch 48 Loss: 375.68310546875 - Pred: tensor([0.3745])\n",
      "Epoch 49 Loss: 375.6798400878906 - Pred: tensor([0.3742])\n",
      "Epoch 50 Loss: 375.6754150390625 - Pred: tensor([0.3740])\n",
      "Epoch 51 Loss: 375.6654968261719 - Pred: tensor([0.3737])\n",
      "Epoch 52 Loss: 375.65008544921875 - Pred: tensor([0.3735])\n",
      "Epoch 53 Loss: 375.6280517578125 - Pred: tensor([0.3733])\n",
      "Epoch 54 Loss: 375.577880859375 - Pred: tensor([0.3732])\n",
      "Epoch 55 Loss: 375.4989929199219 - Pred: tensor([0.3730])\n",
      "Epoch 56 Loss: 375.37811279296875 - Pred: tensor([0.3728])\n",
      "Epoch 57 Loss: 375.19683837890625 - Pred: tensor([0.3727])\n",
      "Epoch 58 Loss: 374.9361267089844 - Pred: tensor([0.3726])\n",
      "Epoch 59 Loss: 374.5662536621094 - Pred: tensor([0.3725])\n",
      "Epoch 60 Loss: 374.10626220703125 - Pred: tensor([0.3725])\n",
      "Epoch 61 Loss: 373.6768798828125 - Pred: tensor([0.3725])\n",
      "Epoch 62 Loss: 373.3349609375 - Pred: tensor([0.3725])\n",
      "Epoch 63 Loss: 373.06414794921875 - Pred: tensor([0.3726])\n",
      "Epoch 64 Loss: 373.0083923339844 - Pred: tensor([0.3727])\n",
      "Epoch 65 Loss: 373.06268310546875 - Pred: tensor([0.3729])\n",
      "Epoch 66 Loss: 373.1525573730469 - Pred: tensor([0.3731])\n",
      "Epoch 67 Loss: 373.23876953125 - Pred: tensor([0.3734])\n",
      "Epoch 68 Loss: 373.30108642578125 - Pred: tensor([0.3741])\n",
      "Epoch 69 Loss: 373.2937927246094 - Pred: tensor([0.3748])\n",
      "Epoch 70 Loss: 373.21563720703125 - Pred: tensor([0.3752])\n",
      "Epoch 71 Loss: 373.0926208496094 - Pred: tensor([0.3755])\n",
      "Epoch 72 Loss: 372.9582214355469 - Pred: tensor([0.3758])\n",
      "Epoch 73 Loss: 372.8270568847656 - Pred: tensor([0.3762])\n",
      "Epoch 74 Loss: 372.68634033203125 - Pred: tensor([0.3765])\n",
      "Epoch 75 Loss: 372.5653076171875 - Pred: tensor([0.3769])\n",
      "Epoch 76 Loss: 372.4817199707031 - Pred: tensor([0.3773])\n",
      "Epoch 77 Loss: 372.3953857421875 - Pred: tensor([0.3777])\n",
      "Epoch 78 Loss: 372.3267517089844 - Pred: tensor([0.3781])\n",
      "Epoch 79 Loss: 372.2336120605469 - Pred: tensor([0.3784])\n",
      "Epoch 80 Loss: 372.00030517578125 - Pred: tensor([0.3788])\n",
      "Epoch 81 Loss: 371.5950622558594 - Pred: tensor([0.3792])\n",
      "Epoch 82 Loss: 371.0766906738281 - Pred: tensor([0.3796])\n",
      "Epoch 83 Loss: 370.54107666015625 - Pred: tensor([0.3800])\n",
      "Epoch 84 Loss: 370.1943664550781 - Pred: tensor([0.3804])\n",
      "Epoch 85 Loss: 370.113525390625 - Pred: tensor([0.3808])\n",
      "Epoch 86 Loss: 370.1687316894531 - Pred: tensor([0.3813])\n",
      "Epoch 87 Loss: 370.2028503417969 - Pred: tensor([0.3817])\n",
      "Epoch 88 Loss: 370.1606750488281 - Pred: tensor([0.3822])\n",
      "Epoch 89 Loss: 370.0457763671875 - Pred: tensor([0.3827])\n",
      "Epoch 90 Loss: 369.839599609375 - Pred: tensor([0.3831])\n",
      "Epoch 91 Loss: 369.5516052246094 - Pred: tensor([0.3836])\n",
      "Epoch 92 Loss: 369.27166748046875 - Pred: tensor([0.3841])\n",
      "Epoch 93 Loss: 369.03692626953125 - Pred: tensor([0.3846])\n",
      "Epoch 94 Loss: 368.8648681640625 - Pred: tensor([0.3851])\n",
      "Epoch 95 Loss: 368.8165588378906 - Pred: tensor([0.3855])\n",
      "Epoch 96 Loss: 368.7530212402344 - Pred: tensor([0.3860])\n",
      "Epoch 97 Loss: 368.609375 - Pred: tensor([0.3865])\n",
      "Epoch 98 Loss: 368.3754577636719 - Pred: tensor([0.3870])\n",
      "Epoch 99 Loss: 368.0941467285156 - Pred: tensor([0.3875])\n",
      "Epoch 100 Loss: 367.8495788574219 - Pred: tensor([0.3880])\n",
      "Epoch 101 Loss: 367.7047424316406 - Pred: tensor([0.3885])\n",
      "Epoch 102 Loss: 367.5946044921875 - Pred: tensor([0.3889])\n",
      "Epoch 103 Loss: 367.4636535644531 - Pred: tensor([0.3894])\n",
      "Epoch 104 Loss: 367.2977600097656 - Pred: tensor([0.3899])\n",
      "Epoch 105 Loss: 367.091552734375 - Pred: tensor([0.3904])\n",
      "Epoch 106 Loss: 366.85198974609375 - Pred: tensor([0.3909])\n",
      "Epoch 107 Loss: 366.6083068847656 - Pred: tensor([0.3914])\n",
      "Epoch 108 Loss: 366.4134521484375 - Pred: tensor([0.3919])\n",
      "Epoch 109 Loss: 366.26116943359375 - Pred: tensor([0.3924])\n",
      "Epoch 110 Loss: 366.09442138671875 - Pred: tensor([0.3929])\n",
      "Epoch 111 Loss: 365.8878479003906 - Pred: tensor([0.3934])\n",
      "Epoch 112 Loss: 365.6487121582031 - Pred: tensor([0.3939])\n",
      "Epoch 113 Loss: 365.4095458984375 - Pred: tensor([0.3944])\n",
      "Epoch 114 Loss: 365.2060241699219 - Pred: tensor([0.3949])\n",
      "Epoch 115 Loss: 365.0223083496094 - Pred: tensor([0.3954])\n",
      "Epoch 116 Loss: 364.8222351074219 - Pred: tensor([0.3959])\n",
      "Epoch 117 Loss: 364.5946960449219 - Pred: tensor([0.3964])\n",
      "Epoch 118 Loss: 364.35894775390625 - Pred: tensor([0.3969])\n",
      "Epoch 119 Loss: 364.137939453125 - Pred: tensor([0.3974])\n",
      "Epoch 120 Loss: 363.9281005859375 - Pred: tensor([0.3979])\n",
      "Epoch 121 Loss: 363.71356201171875 - Pred: tensor([0.3984])\n",
      "Epoch 122 Loss: 363.4888000488281 - Pred: tensor([0.3989])\n",
      "Epoch 123 Loss: 363.2546691894531 - Pred: tensor([0.3994])\n",
      "Epoch 124 Loss: 363.0196838378906 - Pred: tensor([0.4000])\n",
      "Epoch 125 Loss: 362.79364013671875 - Pred: tensor([0.4005])\n",
      "Epoch 126 Loss: 362.57086181640625 - Pred: tensor([0.4010])\n",
      "Epoch 127 Loss: 362.3389892578125 - Pred: tensor([0.4015])\n",
      "Epoch 128 Loss: 362.10040283203125 - Pred: tensor([0.4021])\n",
      "Epoch 129 Loss: 361.8629150390625 - Pred: tensor([0.4026])\n",
      "Epoch 130 Loss: 361.62774658203125 - Pred: tensor([0.4031])\n",
      "Epoch 131 Loss: 361.3935241699219 - Pred: tensor([0.4037])\n",
      "Epoch 132 Loss: 361.1564025878906 - Pred: tensor([0.4042])\n",
      "Epoch 133 Loss: 360.91485595703125 - Pred: tensor([0.4048])\n",
      "Epoch 134 Loss: 360.67132568359375 - Pred: tensor([0.4053])\n",
      "Epoch 135 Loss: 360.4283142089844 - Pred: tensor([0.4058])\n",
      "Epoch 136 Loss: 360.1851806640625 - Pred: tensor([0.4064])\n",
      "Epoch 137 Loss: 359.9414978027344 - Pred: tensor([0.4069])\n",
      "Epoch 138 Loss: 359.69586181640625 - Pred: tensor([0.4075])\n",
      "Epoch 139 Loss: 359.4475402832031 - Pred: tensor([0.4080])\n",
      "Epoch 140 Loss: 359.1973571777344 - Pred: tensor([0.4086])\n",
      "Epoch 141 Loss: 358.94659423828125 - Pred: tensor([0.4092])\n",
      "Epoch 142 Loss: 358.69476318359375 - Pred: tensor([0.4097])\n",
      "Epoch 143 Loss: 358.4416198730469 - Pred: tensor([0.4103])\n",
      "Epoch 144 Loss: 358.1868896484375 - Pred: tensor([0.4108])\n",
      "Epoch 145 Loss: 357.9306640625 - Pred: tensor([0.4114])\n",
      "Epoch 146 Loss: 357.6720275878906 - Pred: tensor([0.4119])\n",
      "Epoch 147 Loss: 357.4111633300781 - Pred: tensor([0.4125])\n",
      "Epoch 148 Loss: 357.14849853515625 - Pred: tensor([0.4131])\n",
      "Epoch 149 Loss: 356.8843078613281 - Pred: tensor([0.4136])\n",
      "Epoch 150 Loss: 356.6182861328125 - Pred: tensor([0.4142])\n",
      "Epoch 151 Loss: 356.35076904296875 - Pred: tensor([0.4147])\n",
      "Epoch 152 Loss: 356.0819091796875 - Pred: tensor([0.4153])\n",
      "Epoch 153 Loss: 355.8116455078125 - Pred: tensor([0.4158])\n",
      "Epoch 154 Loss: 355.54034423828125 - Pred: tensor([0.4164])\n",
      "Epoch 155 Loss: 355.267333984375 - Pred: tensor([0.4169])\n",
      "Epoch 156 Loss: 354.99322509765625 - Pred: tensor([0.4175])\n",
      "Epoch 157 Loss: 354.7178649902344 - Pred: tensor([0.4180])\n",
      "Epoch 158 Loss: 354.4410400390625 - Pred: tensor([0.4186])\n",
      "Epoch 159 Loss: 354.1627197265625 - Pred: tensor([0.4191])\n",
      "Epoch 160 Loss: 353.8831481933594 - Pred: tensor([0.4197])\n",
      "Epoch 161 Loss: 353.6020812988281 - Pred: tensor([0.4202])\n",
      "Epoch 162 Loss: 353.3193664550781 - Pred: tensor([0.4208])\n",
      "Epoch 163 Loss: 353.0351867675781 - Pred: tensor([0.4213])\n",
      "Epoch 164 Loss: 352.74920654296875 - Pred: tensor([0.4219])\n",
      "Epoch 165 Loss: 352.46142578125 - Pred: tensor([0.4224])\n",
      "Epoch 166 Loss: 352.1715087890625 - Pred: tensor([0.4230])\n",
      "Epoch 167 Loss: 351.879150390625 - Pred: tensor([0.4235])\n",
      "Epoch 168 Loss: 351.5838317871094 - Pred: tensor([0.4240])\n",
      "Epoch 169 Loss: 351.28570556640625 - Pred: tensor([0.4246])\n",
      "Epoch 170 Loss: 350.98529052734375 - Pred: tensor([0.4251])\n",
      "Epoch 171 Loss: 350.6832275390625 - Pred: tensor([0.4257])\n",
      "Epoch 172 Loss: 350.3785400390625 - Pred: tensor([0.4262])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173 Loss: 350.0721740722656 - Pred: tensor([0.4267])\n",
      "Epoch 174 Loss: 349.7661437988281 - Pred: tensor([0.4273])\n",
      "Epoch 175 Loss: 349.4598083496094 - Pred: tensor([0.4278])\n",
      "Epoch 176 Loss: 349.1520690917969 - Pred: tensor([0.4283])\n",
      "Epoch 177 Loss: 348.8440856933594 - Pred: tensor([0.4289])\n",
      "Epoch 178 Loss: 348.5359802246094 - Pred: tensor([0.4294])\n",
      "Epoch 179 Loss: 348.2272033691406 - Pred: tensor([0.4299])\n",
      "Epoch 180 Loss: 347.9172058105469 - Pred: tensor([0.4305])\n",
      "Epoch 181 Loss: 347.6064453125 - Pred: tensor([0.4310])\n",
      "Epoch 182 Loss: 347.2953186035156 - Pred: tensor([0.4315])\n",
      "Epoch 183 Loss: 346.9833068847656 - Pred: tensor([0.4321])\n",
      "Epoch 184 Loss: 346.6705322265625 - Pred: tensor([0.4326])\n",
      "Epoch 185 Loss: 346.3565979003906 - Pred: tensor([0.4331])\n",
      "Epoch 186 Loss: 346.0420837402344 - Pred: tensor([0.4337])\n",
      "Epoch 187 Loss: 345.7264709472656 - Pred: tensor([0.4342])\n",
      "Epoch 188 Loss: 345.40972900390625 - Pred: tensor([0.4348])\n",
      "Epoch 189 Loss: 345.0914001464844 - Pred: tensor([0.4353])\n",
      "Epoch 190 Loss: 344.7710266113281 - Pred: tensor([0.4358])\n",
      "Epoch 191 Loss: 344.4481201171875 - Pred: tensor([0.4364])\n",
      "Epoch 192 Loss: 344.1227111816406 - Pred: tensor([0.4369])\n",
      "Epoch 193 Loss: 343.7951965332031 - Pred: tensor([0.4374])\n",
      "Epoch 194 Loss: 343.4662170410156 - Pred: tensor([0.4380])\n",
      "Epoch 195 Loss: 343.1317443847656 - Pred: tensor([0.4385])\n",
      "Epoch 196 Loss: 342.7880554199219 - Pred: tensor([0.4390])\n",
      "Epoch 197 Loss: 342.4354553222656 - Pred: tensor([0.4396])\n",
      "Epoch 198 Loss: 342.0823974609375 - Pred: tensor([0.4401])\n",
      "Epoch 199 Loss: 341.73089599609375 - Pred: tensor([0.4406])\n",
      "Epoch 200 Loss: 341.383544921875 - Pred: tensor([0.4411])\n",
      "Epoch 201 Loss: 341.04217529296875 - Pred: tensor([0.4416])\n",
      "Epoch 202 Loss: 340.7010192871094 - Pred: tensor([0.4422])\n",
      "Epoch 203 Loss: 340.35809326171875 - Pred: tensor([0.4427])\n",
      "Epoch 204 Loss: 340.01739501953125 - Pred: tensor([0.4432])\n",
      "Epoch 205 Loss: 339.67779541015625 - Pred: tensor([0.4438])\n",
      "Epoch 206 Loss: 339.3359680175781 - Pred: tensor([0.4443])\n",
      "Epoch 207 Loss: 338.99407958984375 - Pred: tensor([0.4449])\n",
      "Epoch 208 Loss: 338.65374755859375 - Pred: tensor([0.4454])\n",
      "Epoch 209 Loss: 338.31219482421875 - Pred: tensor([0.4460])\n",
      "Epoch 210 Loss: 337.9694519042969 - Pred: tensor([0.4465])\n",
      "Epoch 211 Loss: 337.62823486328125 - Pred: tensor([0.4471])\n",
      "Epoch 212 Loss: 337.2862548828125 - Pred: tensor([0.4477])\n",
      "Epoch 213 Loss: 336.94293212890625 - Pred: tensor([0.4483])\n",
      "Epoch 214 Loss: 336.600341796875 - Pred: tensor([0.4488])\n",
      "Epoch 215 Loss: 336.257568359375 - Pred: tensor([0.4494])\n",
      "Epoch 216 Loss: 335.913818359375 - Pred: tensor([0.4500])\n",
      "Epoch 217 Loss: 335.569580078125 - Pred: tensor([0.4506])\n",
      "Epoch 218 Loss: 335.2259216308594 - Pred: tensor([0.4512])\n",
      "Epoch 219 Loss: 334.88092041015625 - Pred: tensor([0.4518])\n",
      "Epoch 220 Loss: 334.53558349609375 - Pred: tensor([0.4524])\n",
      "Epoch 221 Loss: 334.19049072265625 - Pred: tensor([0.4530])\n",
      "Epoch 222 Loss: 333.8443908691406 - Pred: tensor([0.4536])\n",
      "Epoch 223 Loss: 333.49774169921875 - Pred: tensor([0.4542])\n",
      "Epoch 224 Loss: 333.151123046875 - Pred: tensor([0.4548])\n",
      "Epoch 225 Loss: 332.8037109375 - Pred: tensor([0.4554])\n",
      "Epoch 226 Loss: 332.4555358886719 - Pred: tensor([0.4560])\n",
      "Epoch 227 Loss: 332.10711669921875 - Pred: tensor([0.4565])\n",
      "Epoch 228 Loss: 331.7579040527344 - Pred: tensor([0.4571])\n",
      "Epoch 229 Loss: 331.4082336425781 - Pred: tensor([0.4577])\n",
      "Epoch 230 Loss: 331.0580749511719 - Pred: tensor([0.4583])\n",
      "Epoch 231 Loss: 330.7071838378906 - Pred: tensor([0.4589])\n",
      "Epoch 232 Loss: 330.35601806640625 - Pred: tensor([0.4595])\n",
      "Epoch 233 Loss: 330.0038146972656 - Pred: tensor([0.4601])\n",
      "Epoch 234 Loss: 329.65118408203125 - Pred: tensor([0.4607])\n",
      "Epoch 235 Loss: 329.2979431152344 - Pred: tensor([0.4613])\n",
      "Epoch 236 Loss: 328.9440002441406 - Pred: tensor([0.4619])\n",
      "Epoch 237 Loss: 328.5892028808594 - Pred: tensor([0.4625])\n",
      "Epoch 238 Loss: 328.2340393066406 - Pred: tensor([0.4631])\n",
      "Epoch 239 Loss: 327.8779602050781 - Pred: tensor([0.4637])\n",
      "Epoch 240 Loss: 327.52142333984375 - Pred: tensor([0.4643])\n",
      "Epoch 241 Loss: 327.1641540527344 - Pred: tensor([0.4649])\n",
      "Epoch 242 Loss: 326.80621337890625 - Pred: tensor([0.4655])\n",
      "Epoch 243 Loss: 326.4478454589844 - Pred: tensor([0.4661])\n",
      "Epoch 244 Loss: 326.08880615234375 - Pred: tensor([0.4667])\n",
      "Epoch 245 Loss: 325.72918701171875 - Pred: tensor([0.4673])\n",
      "Epoch 246 Loss: 325.369140625 - Pred: tensor([0.4679])\n",
      "Epoch 247 Loss: 325.00848388671875 - Pred: tensor([0.4685])\n",
      "Epoch 248 Loss: 324.6475830078125 - Pred: tensor([0.4691])\n",
      "Epoch 249 Loss: 324.28619384765625 - Pred: tensor([0.4697])\n",
      "Epoch 250 Loss: 323.9241638183594 - Pred: tensor([0.4702])\n",
      "Epoch 251 Loss: 323.56201171875 - Pred: tensor([0.4708])\n",
      "Epoch 252 Loss: 323.1994934082031 - Pred: tensor([0.4714])\n",
      "Epoch 253 Loss: 322.8363342285156 - Pred: tensor([0.4720])\n",
      "Epoch 254 Loss: 322.4730529785156 - Pred: tensor([0.4727])\n",
      "Epoch 255 Loss: 322.10943603515625 - Pred: tensor([0.4733])\n",
      "Epoch 256 Loss: 321.74493408203125 - Pred: tensor([0.4739])\n",
      "Epoch 257 Loss: 321.3797912597656 - Pred: tensor([0.4745])\n",
      "Epoch 258 Loss: 321.0132751464844 - Pred: tensor([0.4751])\n",
      "Epoch 259 Loss: 320.6430358886719 - Pred: tensor([0.4757])\n",
      "Epoch 260 Loss: 320.2637939453125 - Pred: tensor([0.4763])\n",
      "Epoch 261 Loss: 319.8720397949219 - Pred: tensor([0.4769])\n",
      "Epoch 262 Loss: 319.5019226074219 - Pred: tensor([0.4775])\n",
      "Epoch 263 Loss: 319.114013671875 - Pred: tensor([0.4781])\n",
      "Epoch 264 Loss: 318.7342529296875 - Pred: tensor([0.4787])\n",
      "Epoch 265 Loss: 318.3720397949219 - Pred: tensor([0.4793])\n",
      "Epoch 266 Loss: 317.99664306640625 - Pred: tensor([0.4799])\n",
      "Epoch 267 Loss: 317.6258239746094 - Pred: tensor([0.4805])\n",
      "Epoch 268 Loss: 317.26177978515625 - Pred: tensor([0.4812])\n",
      "Epoch 269 Loss: 316.8955383300781 - Pred: tensor([0.4818])\n",
      "Epoch 270 Loss: 316.5263366699219 - Pred: tensor([0.4824])\n",
      "Epoch 271 Loss: 316.1589050292969 - Pred: tensor([0.4830])\n",
      "Epoch 272 Loss: 315.7935791015625 - Pred: tensor([0.4836])\n",
      "Epoch 273 Loss: 315.4264221191406 - Pred: tensor([0.4842])\n",
      "Epoch 274 Loss: 315.05767822265625 - Pred: tensor([0.4849])\n",
      "Epoch 275 Loss: 314.69000244140625 - Pred: tensor([0.4855])\n",
      "Epoch 276 Loss: 314.3233337402344 - Pred: tensor([0.4861])\n",
      "Epoch 277 Loss: 313.9556579589844 - Pred: tensor([0.4868])\n",
      "Epoch 278 Loss: 313.58660888671875 - Pred: tensor([0.4874])\n",
      "Epoch 279 Loss: 313.2181091308594 - Pred: tensor([0.4880])\n",
      "Epoch 280 Loss: 312.85052490234375 - Pred: tensor([0.4887])\n",
      "Epoch 281 Loss: 312.48193359375 - Pred: tensor([0.4893])\n",
      "Epoch 282 Loss: 312.11236572265625 - Pred: tensor([0.4899])\n",
      "Epoch 283 Loss: 311.74371337890625 - Pred: tensor([0.4906])\n",
      "Epoch 284 Loss: 311.3753967285156 - Pred: tensor([0.4912])\n",
      "Epoch 285 Loss: 311.0059509277344 - Pred: tensor([0.4919])\n",
      "Epoch 286 Loss: 310.6359558105469 - Pred: tensor([0.4925])\n",
      "Epoch 287 Loss: 310.2666015625 - Pred: tensor([0.4931])\n",
      "Epoch 288 Loss: 309.8973388671875 - Pred: tensor([0.4938])\n",
      "Epoch 289 Loss: 309.5273742675781 - Pred: tensor([0.4944])\n",
      "Epoch 290 Loss: 309.1574401855469 - Pred: tensor([0.4950])\n",
      "Epoch 291 Loss: 308.7874450683594 - Pred: tensor([0.4957])\n",
      "Epoch 292 Loss: 308.41766357421875 - Pred: tensor([0.4963])\n",
      "Epoch 293 Loss: 308.0473327636719 - Pred: tensor([0.4969])\n",
      "Epoch 294 Loss: 307.6771240234375 - Pred: tensor([0.4976])\n",
      "Epoch 295 Loss: 307.3067932128906 - Pred: tensor([0.4982])\n",
      "Epoch 296 Loss: 306.9367370605469 - Pred: tensor([0.4988])\n",
      "Epoch 297 Loss: 306.5661926269531 - Pred: tensor([0.4995])\n",
      "Epoch 298 Loss: 306.1957092285156 - Pred: tensor([0.5001])\n",
      "Epoch 299 Loss: 305.82513427734375 - Pred: tensor([0.5007])\n",
      "Epoch 300 Loss: 305.45452880859375 - Pred: tensor([0.5014])\n",
      "Epoch 301 Loss: 305.083984375 - Pred: tensor([0.5020])\n",
      "Epoch 302 Loss: 304.71307373046875 - Pred: tensor([0.5027])\n",
      "Epoch 303 Loss: 304.3424072265625 - Pred: tensor([0.5033])\n",
      "Epoch 304 Loss: 303.971923828125 - Pred: tensor([0.5039])\n",
      "Epoch 305 Loss: 303.6009216308594 - Pred: tensor([0.5046])\n",
      "Epoch 306 Loss: 303.2301025390625 - Pred: tensor([0.5052])\n",
      "Epoch 307 Loss: 302.85931396484375 - Pred: tensor([0.5059])\n",
      "Epoch 308 Loss: 302.4884338378906 - Pred: tensor([0.5065])\n",
      "Epoch 309 Loss: 302.1174621582031 - Pred: tensor([0.5072])\n",
      "Epoch 310 Loss: 301.7467041015625 - Pred: tensor([0.5078])\n",
      "Epoch 311 Loss: 301.3758544921875 - Pred: tensor([0.5085])\n",
      "Epoch 312 Loss: 301.0048522949219 - Pred: tensor([0.5091])\n",
      "Epoch 313 Loss: 300.6340637207031 - Pred: tensor([0.5097])\n",
      "Epoch 314 Loss: 300.26318359375 - Pred: tensor([0.5104])\n",
      "Epoch 315 Loss: 299.8922119140625 - Pred: tensor([0.5110])\n",
      "Epoch 316 Loss: 299.521484375 - Pred: tensor([0.5117])\n",
      "Epoch 317 Loss: 299.15057373046875 - Pred: tensor([0.5123])\n",
      "Epoch 318 Loss: 298.7796630859375 - Pred: tensor([0.5130])\n",
      "Epoch 319 Loss: 298.4087829589844 - Pred: tensor([0.5136])\n",
      "Epoch 320 Loss: 298.0381164550781 - Pred: tensor([0.5143])\n",
      "Epoch 321 Loss: 297.667236328125 - Pred: tensor([0.5149])\n",
      "Epoch 322 Loss: 297.2963562011719 - Pred: tensor([0.5156])\n",
      "Epoch 323 Loss: 296.9255065917969 - Pred: tensor([0.5163])\n",
      "Epoch 324 Loss: 296.55487060546875 - Pred: tensor([0.5169])\n",
      "Epoch 325 Loss: 296.1842346191406 - Pred: tensor([0.5176])\n",
      "Epoch 326 Loss: 295.8134765625 - Pred: tensor([0.5182])\n",
      "Epoch 327 Loss: 295.442626953125 - Pred: tensor([0.5189])\n",
      "Epoch 328 Loss: 295.07196044921875 - Pred: tensor([0.5195])\n",
      "Epoch 329 Loss: 294.7015075683594 - Pred: tensor([0.5202])\n",
      "Epoch 330 Loss: 294.3309326171875 - Pred: tensor([0.5208])\n",
      "Epoch 331 Loss: 293.9601135253906 - Pred: tensor([0.5215])\n",
      "Epoch 332 Loss: 293.5894470214844 - Pred: tensor([0.5221])\n",
      "Epoch 333 Loss: 293.218505859375 - Pred: tensor([0.5228])\n",
      "Epoch 334 Loss: 292.8465270996094 - Pred: tensor([0.5235])\n",
      "Epoch 335 Loss: 292.4686279296875 - Pred: tensor([0.5241])\n",
      "Epoch 336 Loss: 292.080322265625 - Pred: tensor([0.5248])\n",
      "Epoch 337 Loss: 291.6374206542969 - Pred: tensor([0.5254])\n",
      "Epoch 338 Loss: 291.0791015625 - Pred: tensor([0.5261])\n",
      "Epoch 339 Loss: 290.3796691894531 - Pred: tensor([0.5268])\n",
      "Epoch 340 Loss: 289.8096618652344 - Pred: tensor([0.5276])\n",
      "Epoch 341 Loss: 289.5958557128906 - Pred: tensor([0.5284])\n",
      "Epoch 342 Loss: 289.0545349121094 - Pred: tensor([0.5293])\n",
      "Epoch 343 Loss: 288.3592224121094 - Pred: tensor([0.5302])\n",
      "Epoch 344 Loss: 287.8909912109375 - Pred: tensor([0.5311])\n",
      "Epoch 345 Loss: 287.5244140625 - Pred: tensor([0.5320])\n",
      "Epoch 346 Loss: 287.01025390625 - Pred: tensor([0.5330])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 347 Loss: 286.3771667480469 - Pred: tensor([0.5339])\n",
      "Epoch 348 Loss: 285.8356628417969 - Pred: tensor([0.5348])\n",
      "Epoch 349 Loss: 285.4412841796875 - Pred: tensor([0.5358])\n",
      "Epoch 350 Loss: 284.9178771972656 - Pred: tensor([0.5368])\n",
      "Epoch 351 Loss: 284.3111267089844 - Pred: tensor([0.5377])\n",
      "Epoch 352 Loss: 283.80487060546875 - Pred: tensor([0.5387])\n",
      "Epoch 353 Loss: 283.3535461425781 - Pred: tensor([0.5397])\n",
      "Epoch 354 Loss: 282.85040283203125 - Pred: tensor([0.5407])\n",
      "Epoch 355 Loss: 282.2647399902344 - Pred: tensor([0.5416])\n",
      "Epoch 356 Loss: 281.779541015625 - Pred: tensor([0.5425])\n",
      "Epoch 357 Loss: 281.31072998046875 - Pred: tensor([0.5435])\n",
      "Epoch 358 Loss: 280.7940368652344 - Pred: tensor([0.5444])\n",
      "Epoch 359 Loss: 280.2484130859375 - Pred: tensor([0.5454])\n",
      "Epoch 360 Loss: 279.7467041015625 - Pred: tensor([0.5464])\n",
      "Epoch 361 Loss: 279.2632751464844 - Pred: tensor([0.5473])\n",
      "Epoch 362 Loss: 278.7408752441406 - Pred: tensor([0.5483])\n",
      "Epoch 363 Loss: 278.1864013671875 - Pred: tensor([0.5493])\n",
      "Epoch 364 Loss: 277.6719970703125 - Pred: tensor([0.5502])\n",
      "Epoch 365 Loss: 277.1739501953125 - Pred: tensor([0.5512])\n",
      "Epoch 366 Loss: 276.67169189453125 - Pred: tensor([0.5521])\n",
      "Epoch 367 Loss: 276.14251708984375 - Pred: tensor([0.5530])\n",
      "Epoch 368 Loss: 275.638671875 - Pred: tensor([0.5540])\n",
      "Epoch 369 Loss: 275.13330078125 - Pred: tensor([0.5549])\n",
      "Epoch 370 Loss: 274.6292419433594 - Pred: tensor([0.5558])\n",
      "Epoch 371 Loss: 274.10064697265625 - Pred: tensor([0.5567])\n",
      "Epoch 372 Loss: 273.6165466308594 - Pred: tensor([0.5576])\n",
      "Epoch 373 Loss: 273.1007080078125 - Pred: tensor([0.5585])\n",
      "Epoch 374 Loss: 272.6015625 - Pred: tensor([0.5594])\n",
      "Epoch 375 Loss: 272.0906677246094 - Pred: tensor([0.5603])\n",
      "Epoch 376 Loss: 271.6096496582031 - Pred: tensor([0.5612])\n",
      "Epoch 377 Loss: 271.1011657714844 - Pred: tensor([0.5621])\n",
      "Epoch 378 Loss: 270.61016845703125 - Pred: tensor([0.5630])\n",
      "Epoch 379 Loss: 270.109619140625 - Pred: tensor([0.5639])\n",
      "Epoch 380 Loss: 269.6292419433594 - Pred: tensor([0.5649])\n",
      "Epoch 381 Loss: 269.1319580078125 - Pred: tensor([0.5658])\n",
      "Epoch 382 Loss: 268.6429138183594 - Pred: tensor([0.5667])\n",
      "Epoch 383 Loss: 268.1524963378906 - Pred: tensor([0.5677])\n",
      "Epoch 384 Loss: 267.66925048828125 - Pred: tensor([0.5686])\n",
      "Epoch 385 Loss: 267.1845703125 - Pred: tensor([0.5696])\n",
      "Epoch 386 Loss: 266.69696044921875 - Pred: tensor([0.5705])\n",
      "Epoch 387 Loss: 266.21624755859375 - Pred: tensor([0.5714])\n",
      "Epoch 388 Loss: 265.7331848144531 - Pred: tensor([0.5724])\n",
      "Epoch 389 Loss: 265.2564392089844 - Pred: tensor([0.5733])\n",
      "Epoch 390 Loss: 264.7739562988281 - Pred: tensor([0.5742])\n",
      "Epoch 391 Loss: 264.2967529296875 - Pred: tensor([0.5751])\n",
      "Epoch 392 Loss: 263.81591796875 - Pred: tensor([0.5760])\n",
      "Epoch 393 Loss: 263.3350830078125 - Pred: tensor([0.5769])\n",
      "Epoch 394 Loss: 262.85418701171875 - Pred: tensor([0.5778])\n",
      "Epoch 395 Loss: 262.3760986328125 - Pred: tensor([0.5787])\n",
      "Epoch 396 Loss: 261.8951721191406 - Pred: tensor([0.5795])\n",
      "Epoch 397 Loss: 261.4203796386719 - Pred: tensor([0.5804])\n",
      "Epoch 398 Loss: 260.9495544433594 - Pred: tensor([0.5812])\n",
      "Epoch 399 Loss: 260.4720764160156 - Pred: tensor([0.5820])\n",
      "Epoch 400 Loss: 259.9939880371094 - Pred: tensor([0.5829])\n",
      "Epoch 401 Loss: 259.5164489746094 - Pred: tensor([0.5837])\n",
      "Epoch 402 Loss: 259.0388488769531 - Pred: tensor([0.5845])\n",
      "Epoch 403 Loss: 258.559814453125 - Pred: tensor([0.5853])\n",
      "Epoch 404 Loss: 258.0814514160156 - Pred: tensor([0.5861])\n",
      "Epoch 405 Loss: 257.608154296875 - Pred: tensor([0.5869])\n",
      "Epoch 406 Loss: 257.1356201171875 - Pred: tensor([0.5877])\n",
      "Epoch 407 Loss: 256.6636962890625 - Pred: tensor([0.5885])\n",
      "Epoch 408 Loss: 256.1964416503906 - Pred: tensor([0.5893])\n",
      "Epoch 409 Loss: 255.7315216064453 - Pred: tensor([0.5902])\n",
      "Epoch 410 Loss: 255.26681518554688 - Pred: tensor([0.5910])\n",
      "Epoch 411 Loss: 254.80435180664062 - Pred: tensor([0.5918])\n",
      "Epoch 412 Loss: 254.34507751464844 - Pred: tensor([0.5927])\n",
      "Epoch 413 Loss: 253.88548278808594 - Pred: tensor([0.5935])\n",
      "Epoch 414 Loss: 253.42697143554688 - Pred: tensor([0.5944])\n",
      "Epoch 415 Loss: 252.97048950195312 - Pred: tensor([0.5953])\n",
      "Epoch 416 Loss: 252.51461791992188 - Pred: tensor([0.5961])\n",
      "Epoch 417 Loss: 252.0586395263672 - Pred: tensor([0.5970])\n",
      "Epoch 418 Loss: 251.60446166992188 - Pred: tensor([0.5979])\n",
      "Epoch 419 Loss: 251.15069580078125 - Pred: tensor([0.5988])\n",
      "Epoch 420 Loss: 250.69705200195312 - Pred: tensor([0.5996])\n",
      "Epoch 421 Loss: 250.24447631835938 - Pred: tensor([0.6005])\n",
      "Epoch 422 Loss: 249.7929229736328 - Pred: tensor([0.6014])\n",
      "Epoch 423 Loss: 249.3413543701172 - Pred: tensor([0.6023])\n",
      "Epoch 424 Loss: 248.8910675048828 - Pred: tensor([0.6031])\n",
      "Epoch 425 Loss: 248.44171142578125 - Pred: tensor([0.6040])\n",
      "Epoch 426 Loss: 247.99282836914062 - Pred: tensor([0.6048])\n",
      "Epoch 427 Loss: 247.54522705078125 - Pred: tensor([0.6056])\n",
      "Epoch 428 Loss: 247.09873962402344 - Pred: tensor([0.6065])\n",
      "Epoch 429 Loss: 246.65286254882812 - Pred: tensor([0.6073])\n",
      "Epoch 430 Loss: 246.2084503173828 - Pred: tensor([0.6081])\n",
      "Epoch 431 Loss: 245.76507568359375 - Pred: tensor([0.6089])\n",
      "Epoch 432 Loss: 245.32260131835938 - Pred: tensor([0.6097])\n",
      "Epoch 433 Loss: 244.88160705566406 - Pred: tensor([0.6105])\n",
      "Epoch 434 Loss: 244.44149780273438 - Pred: tensor([0.6113])\n",
      "Epoch 435 Loss: 244.00262451171875 - Pred: tensor([0.6121])\n",
      "Epoch 436 Loss: 243.56488037109375 - Pred: tensor([0.6129])\n",
      "Epoch 437 Loss: 243.12815856933594 - Pred: tensor([0.6137])\n",
      "Epoch 438 Loss: 242.6925811767578 - Pred: tensor([0.6145])\n",
      "Epoch 439 Loss: 242.2581024169922 - Pred: tensor([0.6153])\n",
      "Epoch 440 Loss: 241.82464599609375 - Pred: tensor([0.6161])\n",
      "Epoch 441 Loss: 241.39227294921875 - Pred: tensor([0.6169])\n",
      "Epoch 442 Loss: 240.9608612060547 - Pred: tensor([0.6177])\n",
      "Epoch 443 Loss: 240.5305938720703 - Pred: tensor([0.6185])\n",
      "Epoch 444 Loss: 240.10122680664062 - Pred: tensor([0.6193])\n",
      "Epoch 445 Loss: 239.67291259765625 - Pred: tensor([0.6201])\n",
      "Epoch 446 Loss: 239.2456512451172 - Pred: tensor([0.6209])\n",
      "Epoch 447 Loss: 238.81930541992188 - Pred: tensor([0.6217])\n",
      "Epoch 448 Loss: 238.3940887451172 - Pred: tensor([0.6225])\n",
      "Epoch 449 Loss: 237.9698028564453 - Pred: tensor([0.6233])\n",
      "Epoch 450 Loss: 237.5465545654297 - Pred: tensor([0.6241])\n",
      "Epoch 451 Loss: 237.1243133544922 - Pred: tensor([0.6249])\n",
      "Epoch 452 Loss: 236.70306396484375 - Pred: tensor([0.6257])\n",
      "Epoch 453 Loss: 236.28289794921875 - Pred: tensor([0.6265])\n",
      "Epoch 454 Loss: 235.8637237548828 - Pred: tensor([0.6273])\n",
      "Epoch 455 Loss: 235.44552612304688 - Pred: tensor([0.6282])\n",
      "Epoch 456 Loss: 235.02842712402344 - Pred: tensor([0.6290])\n",
      "Epoch 457 Loss: 234.6124267578125 - Pred: tensor([0.6298])\n",
      "Epoch 458 Loss: 234.19737243652344 - Pred: tensor([0.6306])\n",
      "Epoch 459 Loss: 233.78343200683594 - Pred: tensor([0.6314])\n",
      "Epoch 460 Loss: 233.37054443359375 - Pred: tensor([0.6322])\n",
      "Epoch 461 Loss: 232.95877075195312 - Pred: tensor([0.6330])\n",
      "Epoch 462 Loss: 232.54812622070312 - Pred: tensor([0.6338])\n",
      "Epoch 463 Loss: 232.1384735107422 - Pred: tensor([0.6346])\n",
      "Epoch 464 Loss: 231.73004150390625 - Pred: tensor([0.6354])\n",
      "Epoch 465 Loss: 231.32257080078125 - Pred: tensor([0.6362])\n",
      "Epoch 466 Loss: 230.91632080078125 - Pred: tensor([0.6370])\n",
      "Epoch 467 Loss: 230.5112762451172 - Pred: tensor([0.6378])\n",
      "Epoch 468 Loss: 230.10733032226562 - Pred: tensor([0.6386])\n",
      "Epoch 469 Loss: 229.70452880859375 - Pred: tensor([0.6394])\n",
      "Epoch 470 Loss: 229.30332946777344 - Pred: tensor([0.6402])\n",
      "Epoch 471 Loss: 228.9046173095703 - Pred: tensor([0.6410])\n",
      "Epoch 472 Loss: 228.51303100585938 - Pred: tensor([0.6417])\n",
      "Epoch 473 Loss: 228.14625549316406 - Pred: tensor([0.6425])\n",
      "Epoch 474 Loss: 227.8338165283203 - Pred: tensor([0.6433])\n",
      "Epoch 475 Loss: 227.63043212890625 - Pred: tensor([0.6441])\n",
      "Epoch 476 Loss: 227.08099365234375 - Pred: tensor([0.6448])\n",
      "Epoch 477 Loss: 226.52886962890625 - Pred: tensor([0.6456])\n",
      "Epoch 478 Loss: 226.23780822753906 - Pred: tensor([0.6464])\n",
      "Epoch 479 Loss: 225.8654022216797 - Pred: tensor([0.6471])\n",
      "Epoch 480 Loss: 225.37640380859375 - Pred: tensor([0.6479])\n",
      "Epoch 481 Loss: 225.0281219482422 - Pred: tensor([0.6487])\n",
      "Epoch 482 Loss: 224.6665802001953 - Pred: tensor([0.6494])\n",
      "Epoch 483 Loss: 224.2247772216797 - Pred: tensor([0.6502])\n",
      "Epoch 484 Loss: 223.8685302734375 - Pred: tensor([0.6510])\n",
      "Epoch 485 Loss: 223.4994659423828 - Pred: tensor([0.6517])\n",
      "Epoch 486 Loss: 223.08303833007812 - Pred: tensor([0.6525])\n",
      "Epoch 487 Loss: 222.72549438476562 - Pred: tensor([0.6533])\n",
      "Epoch 488 Loss: 222.3524932861328 - Pred: tensor([0.6540])\n",
      "Epoch 489 Loss: 221.95318603515625 - Pred: tensor([0.6548])\n",
      "Epoch 490 Loss: 221.5937042236328 - Pred: tensor([0.6556])\n",
      "Epoch 491 Loss: 221.22247314453125 - Pred: tensor([0.6563])\n",
      "Epoch 492 Loss: 220.8334197998047 - Pred: tensor([0.6571])\n",
      "Epoch 493 Loss: 220.4727783203125 - Pred: tensor([0.6578])\n",
      "Epoch 494 Loss: 220.10537719726562 - Pred: tensor([0.6585])\n",
      "Epoch 495 Loss: 219.72381591796875 - Pred: tensor([0.6593])\n",
      "Epoch 496 Loss: 219.3627166748047 - Pred: tensor([0.6600])\n",
      "Epoch 497 Loss: 218.99887084960938 - Pred: tensor([0.6608])\n",
      "Epoch 498 Loss: 218.62445068359375 - Pred: tensor([0.6615])\n",
      "Epoch 499 Loss: 218.26206970214844 - Pred: tensor([0.6622])\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epochs in range(500):\n",
    "    y_pred = model(x_data.float())\n",
    "    loss = criterion(y_pred, y_data.view(-1,1).float())\n",
    "    print('Epoch',epochs,'Loss:',loss.item(), '- Pred:', y_pred.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#The error function is the function representing the difference between the values \n",
    "#computed by your model and the real values. In the optimization field often they\n",
    "#speak about two phases: a training phase in which the model is set,\n",
    "#and a test phase in which the model tests its behaviour against the real values of output.\n",
    "#In the training phase the error is necessary to improve the model, \n",
    "#while in the test phase the error is useful to check if the model works properly.\n",
    "\n",
    "#The objective function is the function you want to maximize or minimize. \n",
    "#When they call it \"cost function\" (again, it's the objective function)\n",
    "#it's because they want to only minimize it. \n",
    "#I see the cost function and the objective function as the same thing seen from slightly \n",
    "#different perspectives.\n",
    "\n",
    "#The \"criterion\" is usually the rule for stopping the algorithm you're using.\n",
    "#Suppose you want that your model find the minimum of an objective function,\n",
    "#in real experiences it is often hard to find the exact minimum and the algorithm \n",
    "#could continuing to work for a very long time. In that case you could accept to \n",
    "#stop it \"near\" to the optimum with a particular stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True, False,  True,  True, False, False, False])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.data[0]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([True]), tensor(1))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.double().forward(x_data.data[25]) > 0.5, y_data[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6630], dtype=torch.float64, grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.double().forward(x_data.data[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.data[25]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False]), tensor(0))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.double().forward(x_data.data[55]) > 0.5, y_data[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.double().forward(x_data) > 0.5\n",
    "pred.numpy()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH4AAAAVCAYAAACAEFoRAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAEvElEQVRoBd2Z7VEbMRCGDyYFMKQCnA4MVJCkAyAVAB3A5Bf8y4QOgAoC6SBQQYI7gA4S6IC8z1o662TdyXZ8js+a0Un7IWnvXWkl3RWvr69FmM/OznohHddz8li/DXoZbOC9lsGOSWxI6awXQTo/Pz8R2Q9YlWpKLt6Yvngbyr1K4/kSPWfLfHudojc3fuXdxavQdCfeMmAxhtcaM9cZuKdyV4aeGiN6iJ+Ui/8s1Q3lgWtCnbQt2cuwOnqK93VEWe2beL6tMURfBjqbqh/GfYlmkj6p/B7oLqSqMTuHRYyXOV5MnHWvcjuFXJNcskfXhhX+pIwjvohfcbpo5D+Uj1W/U1movFXRV/nO0djxoHwp3oXjsYrulZlI9F8m0ei+V1kZq1RooaKxarGSbKmxCPF647BhFYarLIasST5Qh/txgwSNk++ka053ckAMnXktelM65nR0VKf/X6oSiY7hBQmbsS3mBypzr3YZixIvv8cfCNyrBohy8oam5jxCIyu3so1ozI/koDF64UTwIraCI0/40tmMbUygRaXOYhHitS7igxBLgW1A5uQTov1Zei/qqzYkS+ad9yfR52940hk7PImN7QfI204avxGrCcf/31gYXoR6wnQYfmP7c3Icwmr0jmO/Zo8OD2w4jJBN+UkZR6J3K57f75kYYhWbPKL01tEpGe2JGk0RK+puZnIVsDC8CPU7yv5QkkIkJ8fhN3LaBVl19sAH1VkdhUo/ISB3RJ8qo8u+jOMJ7z5xMOQQGCcmDCklw/YU3xrM+bEKWBheOB7HpMKrx6xRLsexT794ZdUJJcwqDhJh4vQer8obKVyL7yfHIQ1E26RxdZzu+0/ZCc+3p0mbiXFSNtiYsrsLWBheOJ7w6YG1F4geOXmkbiTO56MB2fcNL05cxwCTlVQ43S1V91U/USZ6sJp/KpNSfdhYQ3Hrz1XAwvBij585yTHcy7l+Je//kuFUEoP5CWCM6FGGavWFXuV6Jp6PHinHz+KMaPh/JzuEheHFigdM76AUAk1yVmqqLZ0XAmPgOmzqA5WUQ11TKxiHbwCpycP4teE37GQO9ab36AoWhhcrHtDKFZcAp0l+JWdU7uauPXu0ndYdTcgmOsSJSMFp3nRV7onmI86Wd7JKbOsr10UVJllu4khleND0/Rpj+scqYGF4seIHyrsNGDTJubb5MGxdiOYbOomrjyXxcCwrlglgSXVmHvdvO9ANuTYBATdM9M9nXuxIJSZEalJVdN14zyo5V8yaVgELw2tNv+xYnTjQvpfHiIifk7Mi/apnNuE4rmxjYVk8HI/DSejyTb/iUKdjCnqgW971PTMspc/1hNN0dtU7XZonfyCF/abqat95LDxe9i9ezn9U7tf9O8/J69q1zZddPWybZhzp7ylvTNMm1GU85VqsQt1F1mVTFotQh1BPYiVWTtLGHT1y8pHmYmtEmnL7mHBofj2PRaMJ26LWZSxKvMzxAoIPK3bvTgGQk6fatM2TTWwx2Bx/FKodWrpsHfbdv1YpI+gqFjFefsXzuhzGKge1CIOcPFJvncTW8gA54Wj8WSt/+U7YJqXWRSyqeIX7kNsDjkJeWM/JQ90267LjCFvaHCPXd5ewSOH1F/gx5NXpMxByAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle \\left( \\left( 569\\right), \\  \\left( 569\\right)\\right)$"
      ],
      "text/plain": [
       "((569,), (569,))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pred.numpy()\n",
    "b = y_data.numpy()\n",
    "pred.numpy().reshape(-1).shape, y_data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "c = confusion_matrix(a,b)\n",
    "sns.heatmap(c, annot=True, xticklabels=le.classes_, yticklabels=le.classes_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.92      0.95       378\n",
      "        True       0.86      0.96      0.91       191\n",
      "\n",
      "    accuracy                           0.93       569\n",
      "   macro avg       0.92      0.94      0.93       569\n",
      "weighted avg       0.94      0.93      0.94       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "diagnosis                    0\n",
       "radius_mean                  0\n",
       "texture_mean                 0\n",
       "perimeter_mean               0\n",
       "area_mean                    0\n",
       "smoothness_mean              0\n",
       "compactness_mean             0\n",
       "concavity_mean               0\n",
       "concave points_mean          0\n",
       "symmetry_mean                0\n",
       "fractal_dimension_mean       0\n",
       "radius_se                    0\n",
       "texture_se                   0\n",
       "perimeter_se                 0\n",
       "area_se                      0\n",
       "smoothness_se                0\n",
       "compactness_se               0\n",
       "concavity_se                 0\n",
       "concave points_se            0\n",
       "symmetry_se                  0\n",
       "fractal_dimension_se         0\n",
       "radius_worst                 0\n",
       "texture_worst                0\n",
       "perimeter_worst              0\n",
       "area_worst                   0\n",
       "smoothness_worst             0\n",
       "compactness_worst            0\n",
       "concavity_worst              0\n",
       "concave points_worst         0\n",
       "symmetry_worst               0\n",
       "fractal_dimension_worst      0\n",
       "Unnamed: 32                569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "0      842302          1        17.99         10.38          122.80   \n",
       "1      842517          1        20.57         17.77          132.90   \n",
       "2    84300903          1        19.69         21.25          130.00   \n",
       "3    84348301          1        11.42         20.38           77.58   \n",
       "4    84358402          1        20.29         14.34          135.10   \n",
       "..        ...        ...          ...           ...             ...   \n",
       "564    926424          1        21.56         22.39          142.00   \n",
       "565    926682          1        20.13         28.25          131.20   \n",
       "566    926954          1        16.60         28.08          108.30   \n",
       "567    927241          1        20.60         29.33          140.10   \n",
       "568     92751          0         7.76         24.54           47.92   \n",
       "\n",
       "     area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "0       1001.0          0.11840           0.27760         0.30010   \n",
       "1       1326.0          0.08474           0.07864         0.08690   \n",
       "2       1203.0          0.10960           0.15990         0.19740   \n",
       "3        386.1          0.14250           0.28390         0.24140   \n",
       "4       1297.0          0.10030           0.13280         0.19800   \n",
       "..         ...              ...               ...             ...   \n",
       "564     1479.0          0.11100           0.11590         0.24390   \n",
       "565     1261.0          0.09780           0.10340         0.14400   \n",
       "566      858.1          0.08455           0.10230         0.09251   \n",
       "567     1265.0          0.11780           0.27700         0.35140   \n",
       "568      181.0          0.05263           0.04362         0.00000   \n",
       "\n",
       "     concave points_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n",
       "0                0.14710  ...          17.33           184.60      2019.0   \n",
       "1                0.07017  ...          23.41           158.80      1956.0   \n",
       "2                0.12790  ...          25.53           152.50      1709.0   \n",
       "3                0.10520  ...          26.50            98.87       567.7   \n",
       "4                0.10430  ...          16.67           152.20      1575.0   \n",
       "..                   ...  ...            ...              ...         ...   \n",
       "564              0.13890  ...          26.40           166.10      2027.0   \n",
       "565              0.09791  ...          38.25           155.00      1731.0   \n",
       "566              0.05302  ...          34.12           126.70      1124.0   \n",
       "567              0.15200  ...          39.42           184.60      1821.0   \n",
       "568              0.00000  ...          30.37            59.16       268.6   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
       "0                  0.2654          0.4601                  0.11890   \n",
       "1                  0.1860          0.2750                  0.08902   \n",
       "2                  0.2430          0.3613                  0.08758   \n",
       "3                  0.2575          0.6638                  0.17300   \n",
       "4                  0.1625          0.2364                  0.07678   \n",
       "..                    ...             ...                      ...   \n",
       "564                0.2216          0.2060                  0.07115   \n",
       "565                0.1628          0.2572                  0.06637   \n",
       "566                0.1418          0.2218                  0.07820   \n",
       "567                0.2650          0.4087                  0.12400   \n",
       "568                0.0000          0.2871                  0.07039   \n",
       "\n",
       "     Unnamed: 32  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "..           ...  \n",
       "564          NaN  \n",
       "565          NaN  \n",
       "566          NaN  \n",
       "567          NaN  \n",
       "568          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['diagnosis']=np.where(data['diagnosis']=='M',1,0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:, 2:-1].values\n",
    "y=data.iloc[:, 1].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([114, 30])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=torch.FloatTensor(X_train)\n",
    "X_test=torch.FloatTensor(X_test)\n",
    "y_train=torch.LongTensor(y_train)\n",
    "y_test=torch.LongTensor(y_test)\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self,input_features=30,hidden1=100,hidden2=100,out_features=2):\n",
    "        super().__init__()\n",
    "        self.f_connected1=nn.Linear(input_features,hidden1)\n",
    "        self.f_connected2=nn.Linear(hidden1,hidden2)\n",
    "        self.out=nn.Linear(hidden2,out_features)\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.f_connected1(x))\n",
    "        x=F.relu(self.f_connected2(x))\n",
    "        x=self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(20)\n",
    "model=ANN_Model()\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.01\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 and the loss : 9.763959884643555\n",
      "Epoch number: 11 and the loss : 0.6903240084648132\n",
      "Epoch number: 21 and the loss : 0.6612697243690491\n",
      "Epoch number: 31 and the loss : 0.6522195339202881\n",
      "Epoch number: 41 and the loss : 0.6221506595611572\n",
      "Epoch number: 51 and the loss : 0.579852819442749\n",
      "Epoch number: 61 and the loss : 0.47451671957969666\n",
      "Epoch number: 71 and the loss : 0.41255196928977966\n",
      "Epoch number: 81 and the loss : 0.3567558228969574\n",
      "Epoch number: 91 and the loss : 0.3340704143047333\n",
      "Epoch number: 101 and the loss : 0.3147895336151123\n",
      "Epoch number: 111 and the loss : 0.29732105135917664\n",
      "Epoch number: 121 and the loss : 0.2826061248779297\n",
      "Epoch number: 131 and the loss : 0.27026310563087463\n",
      "Epoch number: 141 and the loss : 0.2593381702899933\n",
      "Epoch number: 151 and the loss : 0.2495478242635727\n",
      "Epoch number: 161 and the loss : 0.2403946816921234\n",
      "Epoch number: 171 and the loss : 0.23224492371082306\n",
      "Epoch number: 181 and the loss : 0.22493582963943481\n",
      "Epoch number: 191 and the loss : 0.21832628548145294\n",
      "Epoch number: 201 and the loss : 0.21222767233848572\n",
      "Epoch number: 211 and the loss : 0.20657436549663544\n",
      "Epoch number: 221 and the loss : 0.20132066309452057\n",
      "Epoch number: 231 and the loss : 0.196405827999115\n",
      "Epoch number: 241 and the loss : 0.19167384505271912\n",
      "Epoch number: 251 and the loss : 0.1870897114276886\n",
      "Epoch number: 261 and the loss : 0.18273590505123138\n",
      "Epoch number: 271 and the loss : 0.1791592389345169\n",
      "Epoch number: 281 and the loss : 0.17429699003696442\n",
      "Epoch number: 291 and the loss : 0.16904692351818085\n",
      "Epoch number: 301 and the loss : 0.16419152915477753\n",
      "Epoch number: 311 and the loss : 0.15999345481395721\n",
      "Epoch number: 321 and the loss : 0.15601913630962372\n",
      "Epoch number: 331 and the loss : 0.17303626239299774\n",
      "Epoch number: 341 and the loss : 0.18776686489582062\n",
      "Epoch number: 351 and the loss : 0.15690511465072632\n",
      "Epoch number: 361 and the loss : 0.14812417328357697\n",
      "Epoch number: 371 and the loss : 0.1424110233783722\n",
      "Epoch number: 381 and the loss : 0.1387604922056198\n",
      "Epoch number: 391 and the loss : 0.13469715416431427\n",
      "Epoch number: 401 and the loss : 0.13154448568820953\n",
      "Epoch number: 411 and the loss : 0.12841343879699707\n",
      "Epoch number: 421 and the loss : 0.125429168343544\n",
      "Epoch number: 431 and the loss : 0.12249157577753067\n",
      "Epoch number: 441 and the loss : 0.11971300840377808\n",
      "Epoch number: 451 and the loss : 0.11742181330919266\n",
      "Epoch number: 461 and the loss : 0.1205337643623352\n",
      "Epoch number: 471 and the loss : 0.11510064452886581\n",
      "Epoch number: 481 and the loss : 0.10932428389787674\n",
      "Epoch number: 491 and the loss : 0.10616122931241989\n",
      "Epoch number: 501 and the loss : 0.10431990772485733\n",
      "Epoch number: 511 and the loss : 0.26535671949386597\n",
      "Epoch number: 521 and the loss : 0.16025729477405548\n",
      "Epoch number: 531 and the loss : 0.11594235152006149\n",
      "Epoch number: 541 and the loss : 0.1400996297597885\n",
      "Epoch number: 551 and the loss : 0.11117830872535706\n",
      "Epoch number: 561 and the loss : 0.11274304986000061\n",
      "Epoch number: 571 and the loss : 0.10650572925806046\n",
      "Epoch number: 581 and the loss : 0.10270851105451584\n",
      "Epoch number: 591 and the loss : 0.10001125186681747\n",
      "Epoch number: 601 and the loss : 0.09762473404407501\n",
      "Epoch number: 611 and the loss : 0.09554162621498108\n",
      "Epoch number: 621 and the loss : 0.09353682398796082\n",
      "Epoch number: 631 and the loss : 0.0915040522813797\n",
      "Epoch number: 641 and the loss : 0.08899341523647308\n",
      "Epoch number: 651 and the loss : 0.0865648090839386\n",
      "Epoch number: 661 and the loss : 0.08425286412239075\n",
      "Epoch number: 671 and the loss : 0.08211135864257812\n",
      "Epoch number: 681 and the loss : 0.08012256771326065\n",
      "Epoch number: 691 and the loss : 0.07837413996458054\n",
      "Epoch number: 701 and the loss : 0.07673861086368561\n",
      "Epoch number: 711 and the loss : 0.07561375200748444\n",
      "Epoch number: 721 and the loss : 0.08199116587638855\n",
      "Epoch number: 731 and the loss : 0.0784456878900528\n",
      "Epoch number: 741 and the loss : 0.07620983570814133\n",
      "Epoch number: 751 and the loss : 0.11689525842666626\n",
      "Epoch number: 761 and the loss : 0.26259854435920715\n",
      "Epoch number: 771 and the loss : 0.1772165149450302\n",
      "Epoch number: 781 and the loss : 0.09974084049463272\n",
      "Epoch number: 791 and the loss : 0.09941212087869644\n",
      "Epoch number: 801 and the loss : 0.09235527366399765\n",
      "Epoch number: 811 and the loss : 0.08891312032938004\n",
      "Epoch number: 821 and the loss : 0.08792426437139511\n",
      "Epoch number: 831 and the loss : 0.08678698539733887\n",
      "Epoch number: 841 and the loss : 0.085825614631176\n",
      "Epoch number: 851 and the loss : 0.08501296490430832\n",
      "Epoch number: 861 and the loss : 0.08427435159683228\n",
      "Epoch number: 871 and the loss : 0.083651103079319\n",
      "Epoch number: 881 and the loss : 0.08288467675447464\n",
      "Epoch number: 891 and the loss : 0.08227203786373138\n",
      "Epoch number: 901 and the loss : 0.08164268732070923\n",
      "Epoch number: 911 and the loss : 0.08105430006980896\n",
      "Epoch number: 921 and the loss : 0.08040378987789154\n",
      "Epoch number: 931 and the loss : 0.07978249341249466\n",
      "Epoch number: 941 and the loss : 0.07921861857175827\n",
      "Epoch number: 951 and the loss : 0.07866617292165756\n",
      "Epoch number: 961 and the loss : 0.07810644805431366\n",
      "Epoch number: 971 and the loss : 0.07752705365419388\n",
      "Epoch number: 981 and the loss : 0.07693410664796829\n",
      "Epoch number: 991 and the loss : 0.0763494148850441\n"
     ]
    }
   ],
   "source": [
    "epochs=1000\n",
    "final_losses=[]\n",
    "for i in range(epochs):\n",
    "    i=i+1\n",
    "    y_pred=model.forward(X_train)\n",
    "    loss=loss_function(y_pred,y_train)\n",
    "    final_losses.append(loss)\n",
    "    if i%10==1:\n",
    "        print(\"Epoch number: {} and the loss : {}\".format(i,loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 9.763959884643555$"
      ],
      "text/plain": [
       "9.763959884643555"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_losses[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(epochs),final_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_test):\n",
    "        y_pred=model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "        print(y_pred.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63,  4],\n",
       "       [ 1, 46]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAARKElEQVR4nO3de7BddXXA8e+6eUAaniEQAlgDysOI8iiDIJRGEKSgglOHAj6ig72WKkV0VERHhiI1WhXBOuoVxAAaRZQmVcsYA1SwEg0IAoFCjFATCSE8DNAA996z+kdO4UrgnnOT87v73M33w/zmnL3PPr+zYMJisfZv7x2ZiSSpnJ6qA5CkujPRSlJhJlpJKsxEK0mFmWglqbDxpX+gf81ylzVoA9N3O7rqENSF1qy9OzZ1jpHknAlTd9vk32uHFa0kFVa8opWkUdUYrDqCDZhoJdXL4EDVEWzARCupVjIbVYewAROtpHppmGglqSwrWkkqrAtPhrm8S1K9ZKP90UJEbBMRV0bEXRFxZ0QcHBFTImJhRNzTfN221TwmWkm1koMDbY82XABcnZl7AfsAdwJnAosyc3dgUXN7WCZaSfXSaLQ/hhERWwOHARcDZObTmfkocBwwt3nYXOD4ViGZaCXVywhaBxHRGxFLhozeITPtCjwIXBIRv46IiyJiMjAtM+9vHrMKmNYqJE+GSaqXEZwMy8w+oO8FPh4P7A+clpmLI+ICntMmyMyMiJb3VrCilVQvnTsZtgJYkZmLm9tXsj7xPhAR0wGar6tbTWSilVQvgwPtj2Fk5irg9xGxZ3PXEcBSYAEwu7lvNjC/VUi2DiTVS2evDDsN+FZETASWA+9mfYF6RUScAtwHnNBqEhOtpFrJ7NwFC5l5C3DA83x0xEjmMdFKqhcvwZWkwrypjCQVZkUrSYUN9lcdwQZMtJLqxdaBJBVm60CSCrOilaTCTLSSVFZ6MkySCrNHK0mF2TqQpMKsaCWpMCtaSSrMilaSChto6+m2o8pEK6lerGglqTB7tJJUmBWtJBVmRStJhVnRSlJhrjqQpMIyq45gAyZaSfVij1aSCjPRSlJhngyTpMIGB6uOYAMmWkn10sHWQUTcCzwGDAIDmXlAREwBvgvMAO4FTsjMR4abp6djEUlSN2g02h/teV1m7puZBzS3zwQWZebuwKLm9rBMtJLqJRvtj41zHDC3+X4ucHyrL5hoJdVKNrLtERG9EbFkyOh97nTATyLipiGfTcvM+5vvVwHTWsVkj1ZSvYygR5uZfUDfMIccmpkrI2IHYGFE3PWc72dEtLxCwkQrqV46uOogM1c2X1dHxFXAgcADETE9M++PiOnA6lbz2DqQVC8dOhkWEZMjYsv/fw8cBdwOLABmNw+bDcxvFZIVraR66dzyrmnAVREB63PltzPz6oj4FXBFRJwC3Aec0GoiE21Bax97nLPnfJFly++DCM496wyu/69fcc0Nv6Anepiy7dac9/EPscP221UdqirS09PDT//zB6y6/wFOPuG9VYdTDx26qUxmLgf2eZ79DwFHjGQuE21Bc774VQ55zQGcf94n6O/vZ92TT/HyXf+c03rfCcDl35vPVy75Nmd/5LSKI1VV3nvqbO65+7dsueUWVYdSH114r4MR92gjYmo0a2m9sMcef4Kbbr2dv3nTGwCYMGECW225BVtMnvzMMevWPYn/JF+8pu80jSPfMIvL536v6lDqpZHtj1EybEUbEQcBc4CHgXOBy4CpQE9EvDMzry4f4ti08g+r2HabrfnEeV/gv5ctZ+aeu3PmB/6eP5u0ORd87ZssuHoRW06ezDe+NKfqUFWR8+Z8nHM++Vm22GJy64PVvi6810GrivZfgX8G5gHXAO/JzB2Bw4BPv9CXhi4CvujSeR0LdiwZGBzkzruX8bdvOZYrv/llJk3anIsvuwKA09/7LhZddRnHHvU6vv39f684UlXhqKNnsWbNQ9x6yx1Vh1I72Wi0PUZLq0Q7PjN/kpnfA1Zl5o0AmXnXcF/KzL7MPCAzD3jPO0/qVKxjyo47TGXa9lN59Sv3AuCoWYey9O5lf3LMG496HT+97udVhKeKHfiav+Dovz6Cm2+7hr5LzufQww7iK1//l6rDqocubB20SrRDU/6653zWfc+L6CJTt5vCjjtsz+/uWwHAjTfdwstm/Dn3/X7lM8dcc/0v2PWlu1QVoir0qXM+z6tfcRj7v+pwet99Bjf87EZO/bsPVx1WPZS/18GItVp1sE9ErAUCmNR8T3N786KR1cBZZ5zKR8/5LP0D/bxkp+mce9YZnD3nAu79nxVET7DTjjvwyQ+74kDqqFGsVNsVWfhBZv1rlnff37UqN323o6sOQV1ozdq7N3kdzhOfPLHtnDP5n74zKut+XEcrqV58lI0kFdaFrQMTraRaGc1lW+0y0UqqFytaSSrMRCtJhXXhJbgmWkm1kla0klSYiVaSCnPVgSQVZkUrSYWZaCWprBy0dSBJZVnRSlJZLu+SpNJMtJJUWPe1aE20kuolB7ov05poJdVL9+XZlg9nlKQxJRvZ9mhHRIyLiF9HxA+b27tGxOKIWBYR342Iia3mMNFKqpfGCEZ7TgfuHLL9GeD8zHw58AhwSqsJTLSSaqWTFW1E7AIcC1zU3A7gcODK5iFzgeNbzWOilVQvI6hoI6I3IpYMGb3Pme2LwEd4tv7dDng0Mwea2yuAnVuF5MkwSbXyTAps59jMPqDv+T6LiDcCqzPzpoiYtSkxmWgl1UoHnzZ+CPDmiDgG2BzYCrgA2CYixjer2l2Ala0msnUgqV46dDIsMz+Wmbtk5gzgROCazHwbcC3w1uZhs4H5rUIy0UqqlWy0PzbSR4EPRsQy1vdsL271BVsHkmqlg62DZ+fMvA64rvl+OXDgSL5vopVUKzkYVYewAROtpFopUdFuKhOtpFrJhhWtJBVlRStJhWVa0UpSUVa0klRYw1UHklSWJ8MkqTATrSQVlt33EFwTraR6saKVpMJc3iVJhQ266kCSyrKilaTC7NFKUmGuOpCkwqxoJamwwUb3PaHLRCupVmwdSFJhDVcdSFJZLu+SpMJelK2DSTv9Zemf0Bj0x48eUnUIqilbB5JUmKsOJKmwLuwcmGgl1Us3tg66r8aWpE2QGW2P4UTE5hHxy4i4NSLuiIhzmvt3jYjFEbEsIr4bERNbxWSilVQrjRGMFp4CDs/MfYB9gaMj4iDgM8D5mfly4BHglFYTmWgl1UoSbY9h51nv8ebmhOZI4HDgyub+ucDxrWIy0UqqlYGMtkdE9EbEkiGjd+hcETEuIm4BVgMLgd8Cj2bmQPOQFcDOrWLyZJikWmlVqf7JsZl9QN8wnw8C+0bENsBVwF4bE5OJVlKttNF7HbHMfDQirgUOBraJiPHNqnYXYGWr79s6kFQrnerRRsT2zUqWiJgEHAncCVwLvLV52GxgfquYrGgl1UoHK9rpwNyIGMf6ovSKzPxhRCwFvhMRnwJ+DVzcaiITraRaGRxBj3Y4mfkbYL/n2b8cOHAkc5loJdVKFz7JxkQrqV4aHapoO8lEK6lWvKmMJBVWYnnXpjLRSqqVRtg6kKSiBqsO4HmYaCXViqsOJKkwVx1IUmGuOpCkwmwdSFJhLu+SpMIGrWglqSwrWkkqzEQrSYW1eIp4JUy0kmrFilaSCvMSXEkqzHW0klSYrQNJKsxEK0mFea8DSSrMHq0kFeaqA0kqrNGFzQMTraRa8WSYJBXWffUs9FQdgCR1UmMEYzgR8ZKIuDYilkbEHRFxenP/lIhYGBH3NF+3bRWTiVZSrQxEtj1aTQV8KDNnAgcB74uImcCZwKLM3B1Y1NwelolWUq3kCMaw82Ten5k3N98/BtwJ7AwcB8xtHjYXOL5VTCZaSbUyktZBRPRGxJIho/f55oyIGcB+wGJgWmbe3/xoFTCtVUyeDJNUKyNZ3pWZfUDfcMdExBbA94EPZObaiGeviMjMjGjdg7CilVQrnWodAETEBNYn2W9l5g+aux+IiOnNz6cDq1vNY6KVVCsdXHUQwMXAnZn5hSEfLQBmN9/PBua3isnWgaRaGezcStpDgHcAt0XELc19ZwFzgCsi4hTgPuCEVhOZaCXVSqeuDMvMG4AXukXNESOZy0QrqVayC68NM9FKqhXvdfAi9fW+z3PsMa9n9YNr2He/Ef0fh+oogs1PnUOufZinLv8MABNefyLjX3kQZIP+Xy5k4Mb/qDjIsasb797lqoNRcOmlV3DsG99WdRjqEuMPPoZ8cOWz2/vNIrbejnUXnsG6Cz/IwG0/rzC6sa+Ty7s6xUQ7Cq6/YTEPP/Jo1WGoC8RWUxi/x/70L1n0zL7xBx5F/7VXQjb/1X9ibUXR1cMA2fYYLbYOpFE08Zh38fRPLoeJk57Z1zNlGuNf9VrGveJA8om1PP2jS8iHV1UY5dg25k6GRcSC4T7PzDe/wPd6gV6AGLc1PT2TNzpAqS7G7bE/+fgfafzhd/TMmDnkgwlkfz9PfvVjjJt5IJu95VSevPjs6gId48biybCDgd8D81h/M4W2Hns29Prh8RN37r7/vEgV6Hnpnozb6wAm7bEfjJ9IbDaJzd56Grn2IQaXLgZgcOkv2ewt/1BxpGPbmKtogR2BI4GTgJOBHwHzMvOO0oFJddO/cB79C+cB0DNjJhMOfRNPXfklJhx5MuN225uBm6+lZ8ZMGmv+UHGkY1s3VrTDngzLzMHMvDozZ7P+xrfLgOsi4v2jEl1NXH7Zl7nhZwvYc4+Xce/yJbz7XSdWHZK6SP/1/8a4ma9h0vs/x8QjT+bp+V+rOqQxbTCz7TFaWp4Mi4jNgGNZX9XOAC4EriobVr28/R3vqzoEdZnGvUt56t6l6zee/F+eunxOtQHVSDeuo211MuxSYG/gx8A5mXn7qEQlSRtpLPZo3w48AZwO/OOQG94G6+95u1XB2CRpxLqxRztsos1ML2iQNKaMudaBJI01Y7F1IEljymiuJmiXiVZSrdg6kKTCxtzJMEkaa+zRSlJhtg4kqbD0ZJgkldXBx413jIlWUq3YOpCkwmwdSFJhVrSSVFg3Lu/ypjGSaqWTN/6OiG9ExOqIuH3IvikRsTAi7mm+bttqHhOtpFppkG2PNnwTOPo5+84EFmXm7sCi5vawTLSSaqWTiTYzfwY8/JzdxwFzm+/nAse3msdEK6lWMrPtERG9EbFkyOht4yemZeb9zfergGmtvuDJMEm1MpJVB5nZB/Rt7G9lZkZEyx+0opVUKzmCvzbSAxExHaD5urrVF0y0kmplMBttj420AJjdfD8bmN/qC7YOJNVKJ68Mi4h5wCxgakSsAM4G5gBXRMQpwH3ACa3mMdFKqpVOXhmWmSe9wEdHjGQeE62kWunGK8NMtJJqpeFNZSSpLCtaSSpsE1YTFGOilVQrtg4kqTBbB5JUmBWtJBVmRStJhQ3mYNUhbMBEK6lWfDijJBXmwxklqTArWkkqzFUHklSYqw4kqTAvwZWkwuzRSlJh9mglqTArWkkqzHW0klSYFa0kFeaqA0kqzJNhklSYrQNJKswrwySpMCtaSSqsG3u00Y3Zv64iojcz+6qOQ93FPxf111N1AC8yvVUHoK7kn4uaM9FKUmEmWkkqzEQ7uuzD6fn456LmPBkmSYVZ0UpSYSZaSSrMRDsKImIwIm6JiFsj4uaIeG3VMalaEZERcfmQ7fER8WBE/LDKuFSGV4aNjnWZuS9ARLwB+DTwV9WGpIo9AewdEZMycx1wJLCy4phUiBXt6NsKeKTqINQVfgwc23x/EjCvwlhUkIl2dExqtg7uAi4Czq06IHWF7wAnRsTmwKuBxRXHo0JsHYyOoa2Dg4FLI2LvdG3di1pm/iYiZrC+mv1xtdGoJCvaUZaZvwCmAttXHYu6wgLgc9g2qDUr2lEWEXsB44CHqo5FXeEbwKOZeVtEzKo6GJVhoh0dkyLilub7AGZn5mCVAak7ZOYK4MKq41BZXoIrSYXZo5Wkwky0klSYiVaSCjPRSlJhJlpJKsxEK0mFmWglqbD/A0khN9rNqxtZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,predictions)\n",
    "sns.heatmap(cm, annot=True, xticklabels=le.classes_, yticklabels=le.classes_);\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 0.956140350877193$"
      ],
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(y_test,predictions)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('soldgame': conda)",
   "language": "python",
   "name": "python37764bitsoldgameconda7262750cf6184ac8bf201f1ff27a4c4f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
