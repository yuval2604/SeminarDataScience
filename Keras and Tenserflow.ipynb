VISUALISING PCA AND TSNE PLOTSÂ¶.ipynb{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd  # Import Pandas for data manipulation using dataframes\n",
    "import numpy as np  # Import Numpy for data statistical analysis\n",
    "import matplotlib.pyplot as plt  # Import matplotlib for data visualisation\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "\n",
    "data = pd.read_csv(\"BreastCancerDetection.csv\")\n",
    "\n",
    "#remove the last column\n",
    "data =data.iloc[:,1:-1]\n",
    "\n",
    "#looking for exceptions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "z = np.abs(zscore(data.iloc[:,1:]))\n",
    "data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})\n",
    "\n",
    "X = data.drop(['diagnosis'],axis=1)\n",
    "y = data['diagnosis']\n",
    "\n",
    "X_standard = X.apply(zscore)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size = 0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standard = X.apply(zscore)\n",
    "x_standard.describe()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_standard, y, test_size=0.3, random_state=1)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "def baseline_model(units1, units2, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, input_shape=(30,), activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dense(units2, activation='relu', activity_regularizer=l2(0.01), kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the training if arriving to good results\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               3968      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 21,633\n",
      "Trainable params: 21,121\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff8a1962200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               3968      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 21,633\n",
      "Trainable params: 21,121\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff8a1962200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff8a1962200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff8a1962200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      " 1/13 [=>............................] - ETA: 7s - loss: 3.3401 - accuracy: 0.2812WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7ff8a1e1add0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7ff8a1e1add0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7ff8a1e1add0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7ff8a1e1add0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "13/13 [==============================] - 1s 62ms/step - loss: 2.7745 - accuracy: 0.5380 - val_loss: 1.6863 - val_accuracy: 0.9417\n",
      "13/13 [==============================] - 1s 62ms/step - loss: 2.7745 - accuracy: 0.5380 - val_loss: 1.6863 - val_accuracy: 0.9417\n",
      "Epoch 2/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.9429 - accuracy: 0.9688Epoch 2/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9180 - accuracy: 0.9569 - val_loss: 1.5135 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9180 - accuracy: 0.9569 - val_loss: 1.5135 - val_accuracy: 0.9667\n",
      "Epoch 3/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.7122 - accuracy: 0.9375Epoch 3/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.6937 - accuracy: 0.9568 - val_loss: 1.4003 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.6937 - accuracy: 0.9568 - val_loss: 1.4003 - val_accuracy: 0.9667\n",
      "Epoch 4/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.5275 - accuracy: 0.9688Epoch 4/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.4687 - accuracy: 0.9788 - val_loss: 1.3008 - val_accuracy: 0.9500\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.4687 - accuracy: 0.9788 - val_loss: 1.3008 - val_accuracy: 0.9500\n",
      "Epoch 5/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.3560 - accuracy: 0.9688Epoch 5/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.3221 - accuracy: 0.9769 - val_loss: 1.2168 - val_accuracy: 0.9500\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.3221 - accuracy: 0.9769 - val_loss: 1.2168 - val_accuracy: 0.9500\n",
      "Epoch 6/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.2025 - accuracy: 0.9688Epoch 6/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.1973 - accuracy: 0.9609 - val_loss: 1.1442 - val_accuracy: 0.9417\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.1973 - accuracy: 0.9609 - val_loss: 1.1442 - val_accuracy: 0.9417\n",
      "Epoch 7/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.0600 - accuracy: 1.0000Epoch 7/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.0793 - accuracy: 0.9682 - val_loss: 1.0517 - val_accuracy: 0.9333\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.0793 - accuracy: 0.9682 - val_loss: 1.0517 - val_accuracy: 0.9333\n",
      "Epoch 8/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.1452 - accuracy: 0.8438Epoch 8/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.0235 - accuracy: 0.9404 - val_loss: 0.9806 - val_accuracy: 0.9333\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.0235 - accuracy: 0.9404 - val_loss: 0.9806 - val_accuracy: 0.9333\n",
      "Epoch 9/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.9679 - accuracy: 0.9688Epoch 9/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.8960 - accuracy: 0.9795 - val_loss: 0.9283 - val_accuracy: 0.9417\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.8960 - accuracy: 0.9795 - val_loss: 0.9283 - val_accuracy: 0.9417\n",
      "Epoch 10/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7848 - accuracy: 1.0000Epoch 10/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.8069 - accuracy: 0.9769 - val_loss: 0.8883 - val_accuracy: 0.9333\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.8069 - accuracy: 0.9769 - val_loss: 0.8883 - val_accuracy: 0.9333\n",
      "Epoch 11/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7931 - accuracy: 0.9688Epoch 11/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.7627 - accuracy: 0.9633 - val_loss: 0.8343 - val_accuracy: 0.9417\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.7627 - accuracy: 0.9633 - val_loss: 0.8343 - val_accuracy: 0.9417\n",
      "Epoch 12/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6543 - accuracy: 1.0000Epoch 12/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6699 - accuracy: 0.9839 - val_loss: 0.7907 - val_accuracy: 0.9167\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6699 - accuracy: 0.9839 - val_loss: 0.7907 - val_accuracy: 0.9167\n",
      "Epoch 13/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6144 - accuracy: 1.0000Epoch 13/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6176 - accuracy: 0.9852 - val_loss: 0.7687 - val_accuracy: 0.9083\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6176 - accuracy: 0.9852 - val_loss: 0.7687 - val_accuracy: 0.9083\n",
      "Epoch 14/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6068 - accuracy: 0.9688Epoch 14/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.9683 - val_loss: 0.7089 - val_accuracy: 0.9250\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.9683 - val_loss: 0.7089 - val_accuracy: 0.9250\n",
      "Epoch 15/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5337 - accuracy: 0.9688Epoch 15/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.9822 - val_loss: 0.6692 - val_accuracy: 0.9417\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.9822 - val_loss: 0.6692 - val_accuracy: 0.9417\n",
      "Epoch 16/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4766 - accuracy: 1.0000Epoch 16/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.9951 - val_loss: 0.6326 - val_accuracy: 0.9417\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.9951 - val_loss: 0.6326 - val_accuracy: 0.9417\n",
      "Epoch 17/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6186 - accuracy: 0.9062Epoch 17/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.9681 - val_loss: 0.6118 - val_accuracy: 0.9333\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.9681 - val_loss: 0.6118 - val_accuracy: 0.9333\n",
      "Epoch 18/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4955 - accuracy: 0.9688Epoch 18/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.9826 - val_loss: 0.5905 - val_accuracy: 0.9583\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.9826 - val_loss: 0.5905 - val_accuracy: 0.9583\n",
      "Epoch 19/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4565 - accuracy: 0.9688Epoch 19/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.9794 - val_loss: 0.5474 - val_accuracy: 0.9500\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.9794 - val_loss: 0.5474 - val_accuracy: 0.9500\n",
      "Epoch 20/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3808 - accuracy: 1.0000Epoch 20/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.9915 - val_loss: 0.5420 - val_accuracy: 0.9417\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.9915 - val_loss: 0.5420 - val_accuracy: 0.9417\n",
      "Epoch 21/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3265 - accuracy: 1.0000Epoch 21/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.9902 - val_loss: 0.5002 - val_accuracy: 0.9500\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.9902 - val_loss: 0.5002 - val_accuracy: 0.9500\n",
      "Epoch 22/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3250 - accuracy: 1.0000Epoch 22/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.9942 - val_loss: 0.4809 - val_accuracy: 0.9500\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.9942 - val_loss: 0.4809 - val_accuracy: 0.9500\n",
      "Epoch 23/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3176 - accuracy: 0.9688Epoch 23/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.9793 - val_loss: 0.4601 - val_accuracy: 0.9500\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.9793 - val_loss: 0.4601 - val_accuracy: 0.9500\n",
      "Epoch 24/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3062 - accuracy: 1.0000Epoch 24/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2939 - accuracy: 0.9954 - val_loss: 0.4352 - val_accuracy: 0.9500\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2939 - accuracy: 0.9954 - val_loss: 0.4352 - val_accuracy: 0.9500\n",
      "Epoch 25/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2567 - accuracy: 1.0000Epoch 25/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.9861 - val_loss: 0.4292 - val_accuracy: 0.9333\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.9861 - val_loss: 0.4292 - val_accuracy: 0.9333\n",
      "Epoch 26/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2762 - accuracy: 0.9688Epoch 26/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2777 - accuracy: 0.9789 - val_loss: 0.3766 - val_accuracy: 0.9583\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2777 - accuracy: 0.9789 - val_loss: 0.3766 - val_accuracy: 0.9583\n",
      "Epoch 27/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2549 - accuracy: 0.9688Epoch 27/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2465 - accuracy: 0.9875 - val_loss: 0.3835 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2465 - accuracy: 0.9875 - val_loss: 0.3835 - val_accuracy: 0.9667\n",
      "Epoch 28/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2386 - accuracy: 1.0000Epoch 28/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2549 - accuracy: 0.9889 - val_loss: 0.3477 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2549 - accuracy: 0.9889 - val_loss: 0.3477 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50Epoch 29/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2227 - accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.9896 - val_loss: 0.3050 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.9896 - val_loss: 0.3050 - val_accuracy: 0.9750\n",
      "Epoch 30/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2152 - accuracy: 1.0000Epoch 30/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2239 - accuracy: 0.9865 - val_loss: 0.3248 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2239 - accuracy: 0.9865 - val_loss: 0.3248 - val_accuracy: 0.9667\n",
      "Epoch 31/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3041 - accuracy: 0.9688Epoch 31/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9835 - val_loss: 0.2879 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9835 - val_loss: 0.2879 - val_accuracy: 0.9750\n",
      "Epoch 32/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1913 - accuracy: 1.0000Epoch 32/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9856 - val_loss: 0.2823 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9856 - val_loss: 0.2823 - val_accuracy: 0.9667\n",
      "Epoch 33/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1758 - accuracy: 1.0000Epoch 33/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1866 - accuracy: 0.9977 - val_loss: 0.2733 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1866 - accuracy: 0.9977 - val_loss: 0.2733 - val_accuracy: 0.9667\n",
      "Epoch 34/50\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1724 - accuracy: 0.9931 - val_loss: 0.2595 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1724 - accuracy: 0.9931 - val_loss: 0.2595 - val_accuracy: 0.9750\n",
      "Epoch 35/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3236 - accuracy: 0.9375Epoch 35/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2033 - accuracy: 0.9821 - val_loss: 0.2834 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2033 - accuracy: 0.9821 - val_loss: 0.2834 - val_accuracy: 0.9667\n",
      "Epoch 36/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1547 - accuracy: 1.0000Epoch 36/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1965 - accuracy: 0.9765 - val_loss: 0.2482 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1965 - accuracy: 0.9765 - val_loss: 0.2482 - val_accuracy: 0.9750\n",
      "Epoch 37/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1680 - accuracy: 1.0000Epoch 37/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.9866 - val_loss: 0.2259 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.9866 - val_loss: 0.2259 - val_accuracy: 0.9750\n",
      "Epoch 38/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1783 - accuracy: 1.0000Epoch 38/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1687 - accuracy: 0.9901 - val_loss: 0.2247 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1687 - accuracy: 0.9901 - val_loss: 0.2247 - val_accuracy: 0.9750\n",
      "Epoch 39/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1526 - accuracy: 1.0000Epoch 39/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9899 - val_loss: 0.2123 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9899 - val_loss: 0.2123 - val_accuracy: 0.9750\n",
      "Epoch 40/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1354 - accuracy: 1.0000Epoch 40/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1455 - accuracy: 0.9912 - val_loss: 0.2230 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1455 - accuracy: 0.9912 - val_loss: 0.2230 - val_accuracy: 0.9750\n",
      "Epoch 41/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1628 - accuracy: 1.0000Epoch 41/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1530 - accuracy: 0.9934 - val_loss: 0.1990 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1530 - accuracy: 0.9934 - val_loss: 0.1990 - val_accuracy: 0.9750\n",
      "Epoch 42/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1231 - accuracy: 1.0000Epoch 42/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1499 - accuracy: 0.9865 - val_loss: 0.2166 - val_accuracy: 0.9583\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1499 - accuracy: 0.9865 - val_loss: 0.2166 - val_accuracy: 0.9583\n",
      "Epoch 43/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1300 - accuracy: 1.0000Epoch 43/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1404 - accuracy: 0.9854 - val_loss: 0.2011 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1404 - accuracy: 0.9854 - val_loss: 0.2011 - val_accuracy: 0.9667\n",
      "Epoch 44/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1120 - accuracy: 1.0000Epoch 44/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.9937 - val_loss: 0.1957 - val_accuracy: 0.9583\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.9937 - val_loss: 0.1957 - val_accuracy: 0.9583\n",
      "Epoch 45/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1509 - accuracy: 0.9688Epoch 45/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9750 - val_loss: 0.2284 - val_accuracy: 0.9583\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9750 - val_loss: 0.2284 - val_accuracy: 0.9583\n",
      "Epoch 46/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1240 - accuracy: 1.0000Epoch 46/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9894 - val_loss: 0.1830 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9894 - val_loss: 0.1830 - val_accuracy: 0.9667\n",
      "Epoch 47/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2466 - accuracy: 0.9062Epoch 47/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.9722 - val_loss: 0.1686 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.9722 - val_loss: 0.1686 - val_accuracy: 0.9750\n",
      "Epoch 48/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1179 - accuracy: 1.0000Epoch 48/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1148 - accuracy: 0.9995 - val_loss: 0.1756 - val_accuracy: 0.9833\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1148 - accuracy: 0.9995 - val_loss: 0.1756 - val_accuracy: 0.9833\n",
      "Epoch 49/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1135 - accuracy: 1.0000Epoch 49/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1214 - accuracy: 0.9909 - val_loss: 0.1820 - val_accuracy: 0.9833\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1214 - accuracy: 0.9909 - val_loss: 0.1820 - val_accuracy: 0.9833\n",
      "Epoch 50/50\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0983 - accuracy: 1.0000Epoch 50/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 0.9978 - val_loss: 0.1972 - val_accuracy: 0.9667\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 0.9978 - val_loss: 0.1972 - val_accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "units1 = 128\n",
    "units2 = 128\n",
    "dropout = 0.25\n",
    " \n",
    "# Fit the model\n",
    "model = baseline_model(units1, units2, dropout)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=32, \n",
    "                    epochs=50, \n",
    "                    validation_data=(x_val,y_val), \n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1126 - accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1126 - accuracy: 1.0000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1972 - accuracy: 0.9667\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1972 - accuracy: 0.9667\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2225 - accuracy: 0.9412\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2225 - accuracy: 0.9412\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_train, y_train)\n",
    "loss, acc = model.evaluate(x_val, y_val)\n",
    "loss, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7ff8a2abc950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7ff8a2abc950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7ff8a2abc950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7ff8a2abc950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "The accuracy is: 0.94\n",
      "f1 score : 0.94 \n",
      "The accuracy is: 0.94\n",
      "f1 score : 0.94 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.array([0 if n <= .5 else 1 for n in y_pred])\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('The accuracy is: %.2f' % acc)\n",
    "print('f1 score : %.2f '% f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        28\n",
      "           1       0.91      0.91      0.91        23\n",
      "\n",
      "    accuracy                           0.92        51\n",
      "   macro avg       0.92      0.92      0.92        51\n",
      "weighted avg       0.92      0.92      0.92        51\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        28\n",
      "           1       0.91      0.91      0.91        23\n",
      "\n",
      "    accuracy                           0.92        51\n",
      "   macro avg       0.92      0.92      0.92        51\n",
      "weighted avg       0.92      0.92      0.92        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yuval's Tenserflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standard = X.apply(zscore)\n",
    "x_standard.describe()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_standard, y, test_size=0.3, random_state=1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "#units = (30+1)/2\n",
    "# result is binary - 1\n",
    "#X_train shape is (6000,11) \n",
    "classifier.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(x_train, y_train, batch_size = 32, epochs = 100)\n",
    "\n",
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "#y_pred = classifier.predict(X_test)\n",
    "\n",
    "#y_pred = (y_pred > 0.5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop - get best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def baseline_model(units1, units2, dropout):\n",
    "\n",
    "best_i=1\n",
    "best_j=1\n",
    "best_d=1\n",
    "best_loss=1.0\n",
    "best_acc=0.0\n",
    "\n",
    "best_batch_size=1\n",
    "best_epochs=1\n",
    "\n",
    "for i in range(1,128):\n",
    "    for j in range(1,128):\n",
    "        for d in range(1,100,5):\n",
    "            model= baseline_model(i, j, d/100)\n",
    "            history = model.fit(x_train, y_train, \n",
    "                    batch_size=32, \n",
    "                    epochs=50, \n",
    "                    validation_data=(x_val,y_val), \n",
    "                    callbacks=[es])\n",
    "            \n",
    "#            \n",
    "            loss, acc = model.evaluate(x_test, y_test)\n",
    "            print(\"the best yet are:\")\n",
    "            print(\"best i %d the best j is %d the best d is %d \" %(best_i,best_j,best_d) )\n",
    "            print('The best accuracy is: %.2f loss is  %.2f ' %(acc,loss)) \n",
    "            \n",
    "            if(best_loss>loss and best_acc<acc):\n",
    "                best_loss=loss \n",
    "                best_acc=acc\n",
    "                best_i=i\n",
    "                best_j=j\n",
    "                best_d=d\n",
    "                print(\"-----------------------------------------------new score\")\n",
    "                print('The accuracy is: %.2f loss is  %.2f ' %(acc,loss)) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i,j,d)\n",
    "print(\"best results\", \n",
    "best_i,\n",
    "best_j,\n",
    "best_d,\n",
    "best_loss,\n",
    "best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get best batch and epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_standard, y, test_size=0.3, random_state=1)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_i=1\n",
    "best_j=1\n",
    "best_d=1\n",
    "best_loss=1.0\n",
    "best_acc=0.0\n",
    "\n",
    "best_batch_size=1\n",
    "best_epochs=1\n",
    "\n",
    "units1 = 128\n",
    "units2 = 128\n",
    "dropout = 0.25\n",
    " \n",
    "# Fit the model\n",
    "\n",
    "for i in range(1,128):\n",
    "    for j in range(1,128):\n",
    "        \n",
    "        model = baseline_model(units1, units2, dropout)\n",
    "\n",
    "        history = model.fit(x_train, y_train, \n",
    "                batch_size=i, \n",
    "                epochs=j, \n",
    "                validation_data=(x_val,y_val), \n",
    "                callbacks=[es])\n",
    "\n",
    "        print(\"current are i : %d j : %d\" %(i,j))\n",
    "        loss, acc = model.evaluate(x_test, y_test)\n",
    "        print(\"the best yet are:\")\n",
    "        print(\"best i %d the best j is %d  \" %(best_i,best_j) )\n",
    "        print('The best accuracy is: %.2f loss is  %.2f ' %(acc,loss)) \n",
    "\n",
    "        if(best_loss>loss and best_acc<acc):\n",
    "            best_loss=loss \n",
    "            best_acc=acc\n",
    "            best_i=i\n",
    "            best_j=j\n",
    "\n",
    "            print(\"-----------------------------------------------new score\")\n",
    "            print('The accuracy is: %.2f loss is  %.2f ' %(acc,loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i,j,d)\n",
    "print(\"best results\", \n",
    "best_i,\n",
    "best_j,\n",
    "best_d,\n",
    "best_loss,\n",
    "best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing exceptional data and checking again the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutting first column\n",
    "data2=data.iloc[:,1:]\n",
    "chart = sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(data2))\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=45)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z0 = data2.apply(zscore)\n",
    "z = np.abs(zscore(data2))\n",
    "z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0]\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing exeptional points\n",
    "z_in = (np.abs(zscore(data2)) < 3)\n",
    "data_clean = data2[z_in.all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNNClassifier - Doent Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('soldgame': conda)",
   "language": "python",
   "name": "python37764bitsoldgameconda7262750cf6184ac8bf201f1ff27a4c4f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
